{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Using Daimensions\n",
    "\n",
    "This notebook uses data from the Titanic competition on Kaggle (https://www.kaggle.com/c/titanic/overview).\n",
    "\n",
    "Kaggle's description of the competition:\n",
    "\"The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered 'unsinkable' RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: 'what sorts of people were more likely to survive?' using passenger data (ie name, age, gender, socio-economic class, etc).\"\n",
    "\n",
    "Goal: Make a predictor of survival from Titanic training data. We'll do this by using Daimensions to measure, build, and validate a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting Started\n",
    "\n",
    "Because this is the very first tutorial, we'll go over how to install btc and get started. You can also see how to setup btc in the Daimensions Quickstart guide.\n",
    "\n",
    "First, use the following link to download the installation script: https://download.brainome.net/btc-cli/btc-setup.sh. From the download directory, run the following bash command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sh btc-setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script will check that your operating system is supported, download the latest btc client to your machine and install it in /usr/local/bin. You will be prompted to enter the administrator password to install the software. \n",
    "*NOTE: After installation, make sure that “/usr/local/bin” is in your search path. *\n",
    "\n",
    "Next, run the following command to wipe all cloud files. You will need your user credentials to login to DaimensionsTM. The first time you login, your license key will be downloaded automatically. Please use the default password that was provided to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! btc WIPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change your password, use the following bash command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! btc CHPASSWD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Measurements\n",
    "\n",
    "Measuring our data before building a predictor is important in order to avoid mistakes and optimize our model. If we don't measure our data, we have no way of knowing whether the predictor we build will actually do what we want it to do when it sees new data that it wasn’t trained on. We'll probably build a model that is much larger than it needs to be, meaning our training and run times will probably be much longer than they need to be. We could end up in a situation where we just don’t know whether we have the right amount or right type of training data, even after extensive training and testing. Because of these reasons, it's best to measure our data beforehand. Not to mention, Daimensions will tell us about learnability, the generalization ratio, noise resilience, and all the standard accuracy and confusion figures. \n",
    "For more information, you can read the Daimensions How-to Guide and Glossary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n\n\n\n\n\n\n\n\n"
     ]
    }
   ],
   "source": [
    "# Below is a clip of the training data:\n",
    "! head titanic_train.csv\n",
    "# For Windows command prompt:\n",
    "# type titanic_train.csv | more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the target column (Survived) isn't the last column on the right. Because of this, we need to use '-target' so that Daimensions is looking at the correct target column for measuring and building a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Brainome Daimensions(tm) 0.99 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Alexander Makhratchev\n",
      "Expiration date: 2021-04-30 (65 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 30720MB\n",
      "Running locally.\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 11\n",
      "Number of classes: 2\n",
      "Class balance: 61.62% 38.38%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.62%\n",
      "Capacity progression: [8, 9, 10, 10, 11, 11]\n",
      "Decision Tree: 419 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 118 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy...\n",
      "using Decision Tree: 99.42%\n",
      "using Neural Networks: 100.00%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 2.04 bits/bit\n",
      "using a Neural Network: 7.25 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Note: Maybe enough data to generalize. [yellow]\n",
      "Warning: Data has high information density. Expect varying results and increase --effort.\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 2s\n",
      "\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 2m 53s\n",
      "\n",
      "Time estimate for Decision Tree:\n",
      "Estimated time to prime a decision tree: a few seconds\n"
     ]
    }
   ],
   "source": [
    "# Measuring the training data:\n",
    "! ./btc -measureonly titanic_train.csv -target Survived "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Predictor\n",
    "\n",
    "Because the learnability of the data (based on capacity progression and risk) is yellow, the how-to guide recommends to choose predictor with higher generalization and increase effort for best results. This means using a neural network with effort should work best. Here, I'm using '-f NN' to make the predictor a neural network. I'm also using '-o predict.py' to output the predictor as a python file. To increase the effort, I'm using '-e 10' for 10 times the effort. Again, we have to use '-target Survived' because the target column isn't the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Brainome Daimensions(tm) 0.99 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Alexander Makhratchev\n",
      "Expiration date: 2021-04-30 (65 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 30720MB\n",
      "Running locally.\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "Input: titanic_train.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 11\n",
      "Number of classes: 2\n",
      "Class balance: 61.62% 38.38%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.62%\n",
      "Capacity progression: [8, 9, 10, 10, 11, 11]\n",
      "Decision Tree: 419 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 118 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy...\n",
      "using Decision Tree: 99.42%\n",
      "using Neural Networks: 100.00%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 2.04 bits/bit\n",
      "using a Neural Network: 7.25 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Note: Maybe enough data to generalize. [yellow]\n",
      "Warning: Data has high information density. Expect varying results and increase --effort.\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 1s\n",
      "\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 3m 24s\n",
      "\n",
      "Note: Machine learner type NN given by user.\n",
      "\n",
      "Model capacity (MEC):     27 bits\n",
      "Architecture efficiency:   1.0 bits/parameter\n",
      "\n",
      "\n",
      "Estimated time to prime model: 0d 0h 3m 26s\n",
      "\n",
      "\n",
      "Model created:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=11, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "Classifier Type:                     Neural Network\n",
      "System Type:                         Binary classifier\n",
      "Training/Validation Split:           60:40%\n",
      "Best-guess accuracy:                 61.61%\n",
      "Training accuracy:                   63.67% (340/534 correct)\n",
      "Validation accuracy:                 58.54% (209/357 correct)\n",
      "Overall Model accuracy:              61.61% (549/891 correct)\n",
      "Overall Improvement over best guess: 0.00% (of possible 38.39%)\n",
      "Model capacity (MEC):                27 bits\n",
      "Generalization ratio:                19.53 bits/bit\n",
      "Model efficiency:                    0.00%/parameter\n",
      "System behavior\n",
      "True Negatives:                      61.62% (549/891)\n",
      "True Positives:                      0.00% (0/891)\n",
      "False Negatives:                     38.38% (342/891)\n",
      "False Positives:                     0.00% (0/891)\n",
      "True Pos. Rate/Sensitivity/Recall:   0.00\n",
      "True Neg. Rate/Specificity:          1.00\n",
      "F-1 Measure:                         0.00\n",
      "False Negative Rate/Miss Rate:       1.00\n",
      "Critical Success Index:              0.00\n",
      "Confusion Matrix:\n",
      " [61.62% 0.00%]\n",
      " [38.38% 0.00%]\n",
      "Generalization efficiency:           9.62\n",
      "Overfitting:                         No\n",
      "\n",
      "Output: titanic_predict.py \n",
      "READY.\n"
     ]
    }
   ],
   "source": [
    "# Building the predictor and outputting it to 'titanic_predict.py':\n",
    "! ./btc -v -v -f NN titanic_train.csv -o titanic_predict.py -target Survived --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate and Make Predictions\n",
    "\n",
    "We've built our first predictor! Now it's time to put it to use. In the case of Titanic, we are given test data from Kaggle, where it's different from the training data and doesn't include 'Survival'. We can use the model we built to make predictions for the test data and submit it to Kaggle for its competition. In the following code, I'll save the model's prediction in 'titanic_prediction.csv'. You will see that the predictor appended the model's prediction of survival as the last column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked,Prediction\n892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q,0\n893,3,\"Wilkes, Mrs. James (Ellen Needs)\",female,47,1,0,363272,7,,S,0\n894,2,\"Myles, Mr. Thomas Francis\",male,62,0,0,240276,9.6875,,Q,0\n895,3,\"Wirz, Mr. Albert\",male,27,0,0,315154,8.6625,,S,0\n896,3,\"Hirvonen, Mrs. Alexander (Helga E Lindqvist)\",female,22,1,1,3101298,12.2875,,S,0\n897,3,\"Svensson, Mr. Johan Cervin\",male,14,0,0,7538,9.225,,S,0\n898,3,\"Connolly, Miss. Kate\",female,30,0,0,330972,7.6292,,Q,0\n899,2,\"Caldwell, Mr. Albert Francis\",male,26,1,1,248738,29,,S,0\n900,3,\"Abrahim, Mrs. Joseph (Sophie Halaut Easu)\",female,18,0,0,2657,7.2292,,C,0\n"
     ]
    }
   ],
   "source": [
    "# Using predictor on test data and saving it to 'titanic_prediction.csv':\n",
    "! python3 titanic_predict.py titanic_test.csv > titanic_prediction.csv\n",
    "! head titanic_prediction.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have validation data, or data that has the target column but wasn't used for training, you can use it to validate the accuracy of your predictor, as we will do. For this particular instance, I found an annotated version of the Titanic test data, 'titanic_validation.csv', and used it to validate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier Type:                    Neural Network\n",
      "System Type:                        Binary classifier\n",
      "Best-guess accuracy:                62.20%\n",
      "Model accuracy:                     62.20% (260/418 correct)\n",
      "Improvement over best guess:        0.00% (of possible 37.8%)\n",
      "Model capacity (MEC):               27 bits\n",
      "Generalization ratio:               9.20 bits/bit\n",
      "Model efficiency:                   0.00%/parameter\n",
      "System behavior\n",
      "True Negatives:                     62.20% (260/418)\n",
      "True Positives:                     0.00% (0/418)\n",
      "False Negatives:                    37.80% (158/418)\n",
      "False Positives:                    0.00% (0/418)\n",
      "True Pos. Rate/Sensitivity/Recall:  0.00\n",
      "True Neg. Rate/Specificity:         1.00\n",
      "F-1 Measure:                        0.00\n",
      "False Negative Rate/Miss Rate:      1.00\n",
      "Critical Success Index:             0.00\n",
      "Confusion Matrix:\n",
      " [62.20% 0.00%]\n",
      " [37.80% 0.00%]\n"
     ]
    }
   ],
   "source": [
    "# To validate:\n",
    "! python3 titanic_predict.py -validate titanic_validation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From validating the predictor, we can see that it has 74.64% accuracy, 12.44% better than best-guess accuracy (which classifies all data points as the majority class). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving Our Model\n",
    "\n",
    "Our model did pretty well, but let's see if we can improve it. A column that contains a unique value in each row (for example a database key) will never contribute to generalization, so we shouldn't include database keys or other unique ID columns. We can remove these columns by using '-ignorecolumns'. We'll try ignoring columns: PassengerId, Name, Ticket, Cabin, Embarked, because they're all unique ID columns. We could also use '-rank' to rank columns by significance and only process contributing attributes.\n",
    "\n",
    "### Ignorecolumns vs Rank:\n",
    "There may be situations where domain knowledge suggests a better choice of features than -rank. If we know the data generative process, we can do better with -ignorecolumns than with -rank. Rank is also optimizing for quick clustering/decision tree. For neural networks, we may still wish to reduce input features, which can be done with pca, but at the cost of interpretability. Some applications may require the original features are used in which case pca isn't viable. Ignorecolumns can reduce features while maintaining interpretability and work better for neural networks than -rank may, but the burden of choosing the right columns to keep is now on us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using -ignorecolumns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Brainome Daimensions(tm) 0.99 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Alexander Makhratchev\n",
      "Expiration date: 2021-04-30 (65 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 30720MB\n",
      "Running locally.\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "Input: titanic_train.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 6\n",
      "Number of classes: 2\n",
      "Class balance: 61.62% 38.38%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.62%\n",
      "Capacity progression: [9, 9, 10, 11, 11, 13]\n",
      "Decision Tree: 224 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 73 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy...\n",
      "using Decision Tree: 55.32%\n",
      "using Neural Networks: 100.00%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 3.67 bits/bit\n",
      "using a Neural Network: 11.73 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Warning: Not enough data to generalize. [red]\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 2s\n",
      "\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 3m 26s\n",
      "\n",
      "Note: Machine learner type NN given by user.\n",
      "\n",
      "Model capacity (MEC):     17 bits\n",
      "Architecture efficiency:   1.0 bits/parameter\n",
      "\n",
      "\n",
      "Estimated time to prime model: 0d 0h 2m 57s\n",
      "\n",
      "\n",
      "\n",
      "Estimated training time: 0d 0h 23m 8s\n",
      "\n",
      "\n",
      "Model created:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=6, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "Classifier Type:                     Neural Network\n",
      "System Type:                         Binary classifier\n",
      "Best-guess accuracy:                 61.61%\n",
      "Overall Model accuracy:              81.48% (726/891 correct)\n",
      "Overall Improvement over best guess: 19.87% (of possible 38.39%)\n",
      "Model capacity (MEC):                17 bits\n",
      "Generalization ratio:                41.02 bits/bit\n",
      "Model efficiency:                    1.16%/parameter\n",
      "System behavior\n",
      "True Negatives:                      57.91% (516/891)\n",
      "True Positives:                      23.57% (210/891)\n",
      "False Negatives:                     14.81% (132/891)\n",
      "False Positives:                     3.70% (33/891)\n",
      "True Pos. Rate/Sensitivity/Recall:   0.61\n",
      "True Neg. Rate/Specificity:          0.94\n",
      "Precision:                           0.86\n",
      "F-1 Measure:                         0.72\n",
      "False Negative Rate/Miss Rate:       0.39\n",
      "Critical Success Index:              0.56\n",
      "Confusion Matrix:\n",
      " [57.91% 3.70%]\n",
      " [14.81% 23.57%]\n",
      "Generalization efficiency:           20.20\n",
      "Overfitting:                         No\n",
      "Note: Unable to split dataset. The predictor was trained and evaluated on the same data.\n",
      "\n",
      "Output: titanic_predict_igcol.py \n",
      "READY.\n"
     ]
    }
   ],
   "source": [
    "# Using -ignorecolumns to make a better predictor:\n",
    "! ./btc -v -v -f NN titanic_train.csv -o titanic_predict_igcol.py -target Survived -ignorecolumns PassengerId,Name,Ticket,Cabin,Embarked -e 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked,Prediction\n892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q,0\n893,3,\"Wilkes, Mrs. James (Ellen Needs)\",female,47,1,0,363272,7,,S,0\n894,2,\"Myles, Mr. Thomas Francis\",male,62,0,0,240276,9.6875,,Q,0\n895,3,\"Wirz, Mr. Albert\",male,27,0,0,315154,8.6625,,S,0\n896,3,\"Hirvonen, Mrs. Alexander (Helga E Lindqvist)\",female,22,1,1,3101298,12.2875,,S,0\n897,3,\"Svensson, Mr. Johan Cervin\",male,14,0,0,7538,9.225,,S,0\n898,3,\"Connolly, Miss. Kate\",female,30,0,0,330972,7.6292,,Q,0\n899,2,\"Caldwell, Mr. Albert Francis\",male,26,1,1,248738,29,,S,0\n900,3,\"Abrahim, Mrs. Joseph (Sophie Halaut Easu)\",female,18,0,0,2657,7.2292,,C,1\n"
     ]
    }
   ],
   "source": [
    "# Using the ignorecolumns predictor on test data and saving it to 'titanic_prediction_igcol.csv':\n",
    "! python3 titanic_predict_igcol.py titanic_test.csv > titanic_prediction_igcol.csv\n",
    "! head titanic_prediction_igcol.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we wanted, -ignorecolumns removed the PassengerId, Name, Ticket, Cabin, and Embarked attributes. Next, we can use -validate to check the accuracy of our new predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier Type:                    Neural Network\n",
      "System Type:                        Binary classifier\n",
      "Best-guess accuracy:                62.20%\n",
      "Model accuracy:                     76.55% (320/418 correct)\n",
      "Improvement over best guess:        14.35% (of possible 37.8%)\n",
      "Model capacity (MEC):               17 bits\n",
      "Generalization ratio:               18.00 bits/bit\n",
      "Model efficiency:                   0.84%/parameter\n",
      "System behavior\n",
      "True Negatives:                     54.78% (229/418)\n",
      "True Positives:                     21.77% (91/418)\n",
      "False Negatives:                    16.03% (67/418)\n",
      "False Positives:                    7.42% (31/418)\n",
      "True Pos. Rate/Sensitivity/Recall:  0.58\n",
      "True Neg. Rate/Specificity:         0.88\n",
      "Precision:                          0.75\n",
      "F-1 Measure:                        0.65\n",
      "False Negative Rate/Miss Rate:      0.42\n",
      "Critical Success Index:             0.48\n",
      "Confusion Matrix:\n",
      " [54.78% 7.42%]\n",
      " [16.03% 21.77%]\n"
     ]
    }
   ],
   "source": [
    "# Validating the -ignorecolumns predictor\n",
    "! python3 titanic_predict_igcol.py -validate titanic_validation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using -ignorecolumns has improved our accuracy to 77.75% from 74.64% originally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using -rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: Automatic ranking is not recommended for Neural Networks\n",
      "Brainome Daimensions(tm) 0.99 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Alexander Makhratchev\n",
      "Expiration date: 2021-04-30 (65 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 30720MB\n",
      "Running locally.\n",
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "Input: titanic_train.csv\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attribute Ranking:\n",
      "Using only the important columns: Sex SibSp Parch Pclass \n",
      "Risk of coincidental column correlation: <0.001%\n",
      "\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 4\n",
      "Number of classes: 2\n",
      "Class balance: 61.62% 38.38%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.62%\n",
      "Capacity progression: [8, 10, 11, 11, 13, 13]\n",
      "Decision Tree: 1 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 49 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy...\n",
      "using Decision Tree: 0.29%\n",
      "using Neural Networks: 89.09%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 692.67 bits/bit\n",
      "using a Neural Network: 17.47 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Note: Maybe enough data to generalize. [yellow]\n",
      "Note: Decision Tree clustering may outperform Neural Networks. Try with -f DT.\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 1s\n",
      "\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 2m 12s\n",
      "\n",
      "Note: Machine learner type NN given by user.\n",
      "\n",
      "Model capacity (MEC):     31 bits\n",
      "Architecture efficiency:   1.0 bits/parameter\n",
      "\n",
      "\n",
      "Estimated time to prime model: 0d 0h 2m 15s\n",
      "\n",
      "\n",
      "\n",
      "Estimated training time: 0d 0h 18m 32s\n",
      "\n",
      "\n",
      "Model created:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "Classifier Type:                     Neural Network\n",
      "System Type:                         Binary classifier\n",
      "Training/Validation Split:           70:30%\n",
      "Best-guess accuracy:                 61.61%\n",
      "Training accuracy:                   83.30% (519/623 correct)\n",
      "Validation accuracy:                 75.74% (203/268 correct)\n",
      "Overall Model accuracy:              81.03% (722/891 correct)\n",
      "Overall Improvement over best guess: 19.42% (of possible 38.39%)\n",
      "Model capacity (MEC):                25 bits\n",
      "Generalization ratio:                27.75 bits/bit\n",
      "Model efficiency:                    0.77%/parameter\n",
      "System behavior\n",
      "True Negatives:                      55.22% (492/891)\n",
      "True Positives:                      25.81% (230/891)\n",
      "False Negatives:                     12.57% (112/891)\n",
      "False Positives:                     6.40% (57/891)\n",
      "True Pos. Rate/Sensitivity/Recall:   0.67\n",
      "True Neg. Rate/Specificity:          0.90\n",
      "Precision:                           0.80\n",
      "F-1 Measure:                         0.73\n",
      "False Negative Rate/Miss Rate:       0.33\n",
      "Critical Success Index:              0.58\n",
      "Confusion Matrix:\n",
      " [55.22% 6.40%]\n",
      " [12.57% 25.81%]\n",
      "Generalization efficiency:           13.66\n",
      "Overfitting:                         No\n",
      "Using only the important columns: Sex SibSp Parch Pclass \n",
      "Risk of coincidental column correlation: <0.001%\n",
      "\n",
      "\n",
      "Output: titanic_predict_rank.py \n",
      "READY.\n"
     ]
    }
   ],
   "source": [
    "# Using -rank to make a better predictor:\n",
    "! ./btc -v -v -f NN titanic_train.csv -o titanic_predict_rank.py -target Survived -rank --yes -e 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PassengerId,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked,Prediction\n892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q,0\n893,3,\"Wilkes, Mrs. James (Ellen Needs)\",female,47,1,0,363272,7,,S,1\n894,2,\"Myles, Mr. Thomas Francis\",male,62,0,0,240276,9.6875,,Q,0\n895,3,\"Wirz, Mr. Albert\",male,27,0,0,315154,8.6625,,S,0\n896,3,\"Hirvonen, Mrs. Alexander (Helga E Lindqvist)\",female,22,1,1,3101298,12.2875,,S,1\n897,3,\"Svensson, Mr. Johan Cervin\",male,14,0,0,7538,9.225,,S,0\n898,3,\"Connolly, Miss. Kate\",female,30,0,0,330972,7.6292,,Q,1\n899,2,\"Caldwell, Mr. Albert Francis\",male,26,1,1,248738,29,,S,0\n900,3,\"Abrahim, Mrs. Joseph (Sophie Halaut Easu)\",female,18,0,0,2657,7.2292,,C,1\n"
     ]
    }
   ],
   "source": [
    "# Using the rank predictor on test data and saving it to 'titanic_prediction_rank.csv':\n",
    "! python3 titanic_predict_rank.py titanic_test.csv > titanic_prediction_rank.csv\n",
    "! head titanic_prediction_rank.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that -rank decided to only look at the columns 'Sex','Parch' (Parent/child), and 'Fare'. This makes a lot of sense that the determining factors for survival on the Titanic were sex, how many parents or children they had on board, and how much their fare was. Seeing what attributes -rank chooses gives us powerful insight into understanding our data and its correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier Type:                    Neural Network\n",
      "System Type:                        Binary classifier\n",
      "Best-guess accuracy:                62.20%\n",
      "Model accuracy:                     77.75% (325/418 correct)\n",
      "Improvement over best guess:        15.55% (of possible 37.8%)\n",
      "Model capacity (MEC):               25 bits\n",
      "Generalization ratio:               12.44 bits/bit\n",
      "Model efficiency:                   0.62%/parameter\n",
      "System behavior\n",
      "True Negatives:                     52.39% (219/418)\n",
      "True Positives:                     25.36% (106/418)\n",
      "False Negatives:                    12.44% (52/418)\n",
      "False Positives:                    9.81% (41/418)\n",
      "True Pos. Rate/Sensitivity/Recall:  0.67\n",
      "True Neg. Rate/Specificity:         0.84\n",
      "Precision:                          0.72\n",
      "F-1 Measure:                        0.70\n",
      "False Negative Rate/Miss Rate:      0.33\n",
      "Critical Success Index:             0.53\n",
      "Confusion Matrix:\n",
      " [52.39% 9.81%]\n",
      " [12.44% 25.36%]\n"
     ]
    }
   ],
   "source": [
    "# Validating the -rank predictor\n",
    "! python3 titanic_predict_rank.py -validate titanic_validation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With -rank, our accuracy is 76.79%, again, an improvement over our original 74.64%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "Success! We've built our first predictor and used it to make predictions on the Titanic test data. From here, we can use our model on any new Titanic data or use other control options to try to improve our results even more.\n",
    "To check out some of the other control options, use '-h' to see the full list. You can also check out Brainome's How-to Guide and Glossary for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "usage: btc [-h] [-o [OUTPUT]] [-headerless] [-cm CLASSMAPPING] [-nc NCLASSES]\n           [-l LANGUAGE] [-target TARGET] [-nsamples NSAMPLES]\n           [-ignorecolumns IGNORECOLUMNS] [-ignorelabels IGNORELABELS]\n           [-rank [ATTRIBUTERANK]] [-v] [--quiet] [-biasmeter] [-measureonly]\n           [-json [JSON]] [-Wall] [-pedantic] [-nofun] [-f FORCEMODEL]\n           [-e EFFORT] [--yes] [-stopat STOPAT] [-modelonly] [-riskoverfit]\n           [-nopriming] [-novalidation] [-balance] [--runlocalonly]\n           input [input ...]\n\nBrainome Daimensions(tm) Table Compiler\n\npositional arguments:\n  input                 Table as CSV files and/or URLs.\n                        Alternatively, one of: {VERSION, TERMINATE, WIPE, CHPASSWD, LOGOUT}\n                        VERSION: Return version and exit.\n                        TERMINATE: Terminate all cloud processes.\n                        WIPE: Delete all files in the cloud.\n                        CHPASSWD: Change password.\n                        LOGOUT: Force relogin.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -o [OUTPUT], --output [OUTPUT]\n                        Output predictor filename.\n  -headerless           Headerless inputfile.\n  -cm CLASSMAPPING, --classmapping CLASSMAPPING\n                        Manually map class labels to contiguous numeric range. Json format. Example: {\\\"F\\\":0,\\\"T\\\":1}\n  -nc NCLASSES, --nclasses NCLASSES\n                        Specify number of classes. Stop if not matched by input.\n  -l LANGUAGE, --language LANGUAGE\n                        Predictor language: py, exe\n  -target TARGET        Specify target attribute (name or number).\n  -nsamples NSAMPLES    Work on n random samples (0 full dataset, default: 1000000). Balancing is not performed.\n  -ignorecolumns IGNORECOLUMNS\n                        Comma-separated list of attributes to ignore (names or numbers).\n  -ignorelabels IGNORELABELS\n                        Comma-separated list of rows of classes to ignore.\n  -rank [ATTRIBUTERANK], --attributerank [ATTRIBUTERANK]\n                        Rank columns by significance, only process contributing attributes. If optional parameter n is given, force the use top n attributes.\n  -v, --verbosity       Verbosity (debug level).\n  --quiet               Quiet operation.\n  -biasmeter            Measure bias (only NN).\n  -measureonly          Only output measurements, no compilation.\n  -json [JSON]          Output all measurement data in JSON format to filename.\n  -Wall                 Display all warnings\n  -pedantic             Display all notes and warnings.\n  -nofun                Stop compilation if there are warnings.\n  -f FORCEMODEL, --forcemodel FORCEMODEL\n                        Force model type: DT, NN\n  -e EFFORT, --effort EFFORT\n                        1=<effort<100. More careful model creation. Default: 1\n  --yes                 No interaction. Default to yes for all questions.\n  -stopat STOPAT        Stop when percentage goal has been reached. Default: 100\n  -modelonly            Output model only in ONNX file format. No predictor.\n  -riskoverfit          Prioritize validation accuracy over generalization. Default: Prioritize generalization over accuracy.\n  -nopriming            Do not prime the model.\n  -novalidation         Do not measure validation scores for created predictor.\n  -balance              Treat classes as if they were balanced.\n  --runlocalonly        Keep all data local and do not use cloud service (default).\n"
     ]
    }
   ],
   "source": [
    "! ./btc -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}