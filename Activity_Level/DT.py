#!/usr/bin/env python3
#
# This code has been produced by a free evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Brainome grants an exclusive (subject to our continuing rights to use and modify models),
# worldwide, non-sublicensable, and non-transferable limited license to use and modify this
# predictor produced through the input of your data:
# (i) for users accessing the service through a free evaluation account, solely for your
# own non-commercial purposes, including for the purpose of evaluating this service, and
# (ii) for users accessing the service through a paid, commercial use account, for your
# own internal  and commercial purposes.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.991 Table Compiler v0.99.
# Invocation: btc train.csv -headerless -f DT -o DT.py -riskoverfit --yes
# Total compiler execution time: 0:01:33.14. Finished on: Mar-03-2021 09:04:10.
# This source code requires Python 3.
#
"""
Classifier Type:                    Decision Tree
System Type:                         5-way classifier
Best-guess accuracy:                 22.82%
Overall Model accuracy:              99.97% (4176/4177 correct)
Overall Improvement over best guess: 77.15% (of possible 77.18%)
Model capacity (MEC):                2250 bits
Generalization ratio:                4.28 bits/bit
Model efficiency:                    0.03%/parameter
Confusion Matrix:
 [20.61% 0.00% 0.00% 0.00% 0.00%]
 [0.02% 18.46% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 16.83% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 21.26% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 22.82%]
Generalization index:                1.48
Percent of Data Memorized:           67.76%
Note: Unable to split dataset. The predictor was trained and evaluated on the same data.
Note: Labels have been remapped to '1.0'=0, '2.0'=1, '3.0'=2, '4.0'=3, '5.0'=4.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "train.csv"


#Number of attributes
num_attr = 561
n_classes = 5
transform_true = False

# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target=""
important_idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560]

def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[], trim=False):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target=""
    important_idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560]
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return -1
    if not trim:
        ignorecolumns = []
    if (testfile):
        target = ''
        hc = -1 
    with open(outputcsvfile, "w+", encoding='utf-8') as outputfile:
        with open(inputcsvfile, "r", encoding='utf-8') as csvfile:      # hardcoded utf-8 encoding per #717
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile, encoding='utf-8')):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        if len(important_idxs) == 1:
                            print(",", file=outputfile)
                        else:
                            print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        if len(important_idxs) == 1:
                            print(",", file=outputfile)
                        else:
                            print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False, trim=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'1.0': 0, '2.0': 1, '3.0': 2, '4.0': 3, '5.0': 4}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result=float(value)
                if math.isnan(result):
                    #if nan parse to string
                    raise ValueError('')
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename, encoding='utf-8') as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+", encoding='utf-8')
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            if not transform_true:
                rowlen = num_attr if trim else num_attr + len(ignorecolumns)
            else:
                rowlen = num_attr_before_transform if trim else num_attr_before_transform + len(ignorecolumns)      # noqa
            if (not testfile):
                rowlen = rowlen + 1    
            if ((len(row) - (1 if ((testfile and len(important_idxs) == 1)) else 0))  != rowlen) and not (row == ['','']):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs. Expected Row length: " + str(rowlen) + ", Actual Row Length: " + str(len(row)))
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([-375.95964000000004, -374.96166999999997, -373.323845, -372.847885, -372.67571, -372.0309, -371.74154, -371.63122, -371.54214, -371.353475, -371.25072, -371.14514999999994, -371.07047, -371.02676499999995, -370.86658, -370.713835, -370.63556, -370.546835, -370.52056500000003, -370.486935, -370.42697499999997, -370.301285, -370.165875, -370.03952000000004, -370.02221, -369.99327, -369.97092, -369.96381, -369.949815, -369.94087, -369.916765, -369.887065, -369.86712, -369.85543, -369.835875, -369.82662, -369.82398, -369.811975, -369.793525, -369.769725, -369.69381999999996, -369.69034, -369.62850000000003, -369.62012, -369.587765, -369.52297, -369.46582, -369.451235, -369.42834, -369.40981999999997, -369.31048499999997, -369.30602500000003, -369.261815, -369.166895, -369.15009499999996, -369.11749000000003, -369.11155, -369.0966, -369.06460000000004, -369.04532, -368.89167999999995, -368.8872, -368.87231999999995, -368.86026, -368.85646, -368.84790000000004, -368.8335, -368.79547, -368.72292, -368.716125, -368.70277, -368.695895, -368.673215, -368.650435, -368.62246999999996, -368.61758999999995, -368.53539, -368.47225, -368.402875, -368.395085, -368.38364, -368.37123499999996, -368.36539, -368.31154, -368.30019, -368.25915499999996, -368.149965, -368.11317499999996, -368.105805, -368.0895, -368.073805, -368.06149999999997, -368.05448, -368.036955, -368.009625, -368.003455, -367.995885, -367.98798500000004, -367.97049000000004, -367.961365, -367.9257, -367.89504999999997, -367.89158, -367.884355, -367.88055499999996, -367.87636, -367.856675, -367.79684, -367.78409, -367.772925, -367.759105, -367.75211, -367.719615, -367.70728499999996, -367.69158999999996, -367.69005, -367.6869, -367.67934, -367.667755, -367.653255, -367.646185, -367.605525, -367.59842000000003, -367.593745, -367.578055, -367.52617, -367.52065500000003, -367.516655, -367.473795, -367.43644500000005, -367.435455, -367.43390999999997, -367.427755, -367.39136499999995, -367.35729000000003, -367.352575, -367.34570499999995, -367.33633499999996, -367.324285, -367.29010000000005, -367.220635, -367.17947, -367.15689499999996, -367.15198999999996, -367.12278000000003, -367.097015, -367.071495, -367.04454, -367.02155500000003, -366.99461999999994, -366.90112, -366.86531, -366.85373, -366.837945, -366.82774, -366.81764499999997, -366.80611999999996, -366.7346, -366.726035, -366.697025, -366.6923, -366.66267, -366.66108999999994, -366.65446, -366.60612000000003, -366.565385, -366.544, -366.536345, -366.530705, -366.50498000000005, -366.44023000000004, -366.429785, -366.423385, -366.42176, -366.410805, -366.395305, -366.3921, -366.38867, -366.38477, -366.383185, -366.37548000000004, -366.36196, -366.358575, -366.33274, -366.29276, -366.2919, -366.283185, -366.264385, -366.246265, -366.24443499999995, -366.2308, -366.21441500000003, -366.203025, -366.18203, -366.16860499999996, -366.14086999999995, -366.13627499999996, -366.12189, -366.1176, -366.094555, -366.085045, -366.07482999999996, -366.05995, -366.05383500000005, -365.996575, -365.991775, -365.98081, -365.96674, -365.940565, -365.93733, -365.892805, -365.870595, -365.842635, -365.825635, -365.823245, -365.80882, -365.80048, -365.78994, -365.78783500000003, -365.78037500000005, -365.764975, -365.73174, -365.727315, -365.7088, -365.66535, -365.657315, -365.605375, -365.597965, -365.57948999999996, -365.573535, -365.54674, -365.54346499999997, -365.54161999999997, -365.53899, -365.526705, -365.52315999999996, -365.51999, -365.50686499999995, -365.501405, -365.494915, -365.48595, -365.47826499999996, -365.47426499999995, -365.471495, -365.469865, -365.429785, -365.42452, -365.41812500000003, -365.413605, -365.40536999999995, -365.39869, -365.38864, -365.37831, -365.36742, -365.36262999999997, -365.360675, -365.357175, -365.335015, -365.302015, -365.29609, -365.28713500000003, -365.23614000000003, -365.18264999999997, -365.179785, -365.11325, -365.10745499999996, -365.08947, -365.08212499999996, -365.07491, -365.074, -365.073885, -365.067325, -365.046915, -365.03896, -365.02049999999997, -364.993475, -364.98105499999997, -364.972045, -364.967315, -364.94608999999997, -364.90063000000004, -364.897235, -364.89081, -364.88131, -364.86163, -364.8415, -364.834155, -364.81585, -364.809265, -364.80163999999996, -364.78058999999996, -364.746055, -364.72877, -364.71032, -364.68772, -364.67422, -364.6222, -364.611935, -364.56917000000004, -364.53384, -364.52806499999997, -364.51509500000003, -364.499615, -364.464475, -364.447295, -364.431965, -364.428075, -364.4169, -364.396555, -364.370935, -364.36841, -364.36352, -364.31799, -364.29861500000004, -364.281555, -364.26272500000005, -364.25054, -364.24921, -364.24523999999997, -364.22749999999996, -364.19947, -364.16529, -364.152035, -364.14799999999997, -364.13829999999996, -364.114105, -364.10409500000003, -364.05629999999996, -364.04375, -364.03069500000004, -364.002225, -363.991175, -363.960325, -363.95806500000003, -363.91579, -363.88419, -363.87199499999997, -363.86478999999997, -363.862655, -363.853635, -363.84875999999997, -363.840895, -363.83676, -363.83071, -363.81137, -363.759005, -363.75816999999995, -363.74234, -363.73473, -363.69556, -363.67114499999997, -363.65542500000004, -363.64744500000006, -363.638295, -363.62541, -363.584375, -363.575695, -363.562685, -363.552045, -363.545355, -363.51469999999995, -363.512615, -363.50609, -363.49314, -363.47447, -363.42023, -363.39722, -363.372975, -363.34861, -363.28977499999996, -363.262835, -363.20489999999995, -363.17524000000003, -363.156145, -363.14036, -363.12558, -363.11265, -363.10821999999996, -363.10070499999995, -363.09192999999993, -363.075195, -363.06512, -363.06044499999996, -363.04769, -363.01122, -363.00722499999995, -362.95340999999996, -362.943235, -362.92669, -362.922055, -362.916165, -362.87764000000004, -362.87453500000004, -362.848525, -362.761685, -362.74447499999997, -362.72819, -362.71963, -362.70936, -362.68772, -362.66998, -362.66811500000006, -362.662445, -362.65654, -362.619195, -362.60286999999994, -362.58006, -362.57171500000004, -362.56368000000003, -362.553495, -362.54569000000004, -362.53128, -362.52412, -362.50044, -362.49321499999996, -362.47492, -362.43215499999997, -362.38159999999993, -362.3668, -362.35847, -362.32987, -362.30344, -362.29256499999997, -362.20406, -362.19174, -362.183015, -362.172715, -362.16873, -362.150525, -362.143635, -362.13645499999996, -362.131485, -362.12557999999996, -362.10582, -362.10409500000003, -362.087015, -362.064575, -362.05475, -362.04542, -362.02383, -362.01723499999997, -362.00966999999997, -361.97663, -361.93859, -361.92491, -361.91451499999994, -361.89531999999997, -361.88822999999996, -361.87838, -361.87244499999997, -361.85113, -361.81491500000004, -361.801715, -361.73125500000003, -361.72197500000004, -361.70779, -361.69397000000004, -361.664675, -361.643635, -361.63311, -361.627575, -361.62407999999994, -361.6157, -361.608645, -361.56505500000003, -361.56151, -361.52694999999994, -361.52191, -361.51481, -361.50348999999994, -361.488775, -361.45573, -361.419085, -361.343985, -361.33453, -361.299755, -361.28623500000003, -361.2768, -361.25509999999997, -361.23172, -361.202105, -361.18098499999996, -361.15713, -361.14387, -361.14073499999995, -361.12817999999993, -361.11474, -361.099795, -361.080605, -361.069335, -361.050935, -361.02792, -361.00142500000004, -360.943215, -360.93908999999996, -360.936045, -360.93404999999996, -360.930975, -360.90301, -360.87660999999997, -360.85654, -360.853865, -360.817995, -360.78108, -360.764015, -360.7513, -360.746715, -360.70651, -360.61398499999996, -360.51718, -360.51216, -360.4875, -360.473485, -360.43142, -360.410375, -360.34691999999995, -360.217705, -360.20289, -360.19214, -360.098855, -360.09457, -360.08777, -360.00276999999994, -359.99048500000004, -359.980955, -359.975145, -359.971365, -359.96737, -359.933085, -359.89741000000004, -359.88742, -359.876185, -359.87375000000003, -359.84749999999997, -359.83826999999997, -359.74874, -359.743145, -359.72336499999994, -359.68309999999997, -359.64974499999994, -359.62888999999996, -359.618425, -359.59772499999997, -359.588725, -359.57267, -359.56404499999996, -359.52854, -359.5145, -359.504585, -359.497155, -359.49237000000005, -359.45029, -359.437015, -359.418745, -359.38789999999995, -359.35479499999997, -359.32517499999994, -359.245925, -359.180445, -359.038175, -359.02423, -359.0189, -358.97111, -358.942255, -358.910755, -358.84938, -358.83966, -358.836275, -358.79792999999995, -358.79277, -358.783025, -358.74612, -358.66306499999996, -358.54981999999995, -358.50398, -358.4692, -358.45513, -358.434795, -358.413085, -358.391655, -358.369055, -358.27933, -358.2675, -358.22625, -358.184795, -358.136895, -358.10177, -358.03786499999995, -358.01559, -357.99649, -357.97839, -357.968935, -357.923385, -357.914875, -357.88503000000003, -357.856015, -357.83832500000005, -357.75699, -357.7475999999999, -357.73643, -357.71998, -357.684035, -357.680965, -357.66762, -357.65441, -357.61048500000004, -357.55524, -357.52258, -357.50489500000003, -357.48968, -357.44698999999997, -357.42492, -357.40217499999994, -357.389745, -357.355825, -357.30251999999996, -357.270745, -357.24458, -357.19760999999994, -357.12451, -357.10772, -356.949565, -356.919745, -356.76444, -356.76252, -356.74678, -356.70339, -356.663835, -356.62953500000003, -356.60429999999997, -356.543095, -356.48312999999996, -356.478795, -356.47112500000003, -356.445385, -356.42163999999997, -356.37041999999997, -356.364865, -356.27612, -356.25681, -356.240595, -356.16674, -356.129455, -356.08184, -356.058405, -356.006645, -355.902525, -355.89007000000004, -355.87866999999994, -355.85623999999996, -355.83799, -355.82129499999996, -355.786905, -355.709875, -355.663675, -355.61689, -355.61161, -355.58549999999997, -355.55726500000003, -355.54981000000004, -355.526165, -355.46247999999997, -355.43026499999996, -355.423925, -355.37685, -355.32693500000005, -355.21806, -355.209465, -355.1843, -355.13324, -355.047985, -354.93312000000003, -354.86562000000004, -354.81431499999997, -354.78297999999995, -354.738825, -354.649725, -354.56178, -354.528345, -354.520805, -354.497955, -354.309665, -354.29572, -354.25016, -354.20126999999997, -354.18261999999993, -354.12405, -354.08143499999994, -354.05435, -354.04440999999997, -354.036855, -353.95636, -353.866885, -353.73607, -353.69840999999997, -353.56779, -353.400395, -353.247115, -353.22605, -353.19636, -353.139485, -353.102215, -353.080225, -353.04452499999996, -353.01205, -353.000765, -352.986585, -352.94724499999995, -352.91446999999994, -352.89932999999996, -352.845065, -352.75211, -352.722895, -352.7068, -352.70197999999993, -352.66487499999994, -352.64555999999993, -352.57622, -352.54033, -352.501215, -352.408725, -352.36366, -352.32599999999996, -352.220455, -352.13964999999996, -352.081, -352.03371, -351.81715999999994, -351.805705, -351.69277999999997, -351.68305999999995, -351.61357499999997, -351.55206, -351.49807, -351.486855, -351.46399499999995, -351.45741999999996, -351.38163499999996, -351.261355, -351.129235, -351.036285, -351.02765, -350.94872, -350.79927, -350.62552, -350.580375, -350.522075, -350.42088, -350.19759, -350.067245, -350.00841, -349.890255, -349.882575, -349.771975, -349.753515, -349.708665, -349.40703499999995, -349.36384499999997, -349.326135, -349.20089500000006, -349.13423, -348.96308999999997, -348.954055, -348.92229499999996, -348.7359, -348.634915, -348.61544, -348.560705, -348.482205, -348.28646, -348.23913, -348.01275999999996, -347.97607, -347.91088, -347.85524, -347.83477999999997, -347.82689, -347.82529999999997, -347.699235, -347.490535, -347.43014500000004, -347.37938999999994, -347.35668, -347.302005, -347.18165, -347.14757499999996, -347.136975, -347.120145, -346.952495, -346.9212, -346.85924, -346.719895, -346.554465, -346.221945, -346.156775, -346.12986, -346.02866, -345.90922, -345.75199999999995, -345.54682, -345.53494, -345.06016999999997, -344.9939, -344.83564, -344.65073, -344.4486, -344.32308, -344.29348, -344.259675, -344.07521499999996, -343.80139499999996, -343.74114, -343.696675, -343.65314, -343.46553, -343.413485, -343.22891500000003, -342.78039, -342.74026000000003, -342.689505, -342.04228, -341.99972, -341.95241, -341.88125, -341.744335, -341.68010000000004, -341.621235, -341.574035, -341.337255, -341.283485, -340.648585, -339.72405000000003, -339.595575, -339.41191999999995, -339.363335, -339.212865, -339.021855, -338.84540000000004, -338.46074999999996, -337.68181000000004, -337.59975, -337.46270000000004, -337.26731, -337.23605499999996, -337.051425, -336.85774999999995, -336.43266, -336.29526, -335.99314000000004, -335.547085, -335.275755, -334.790695, -334.53765, -334.32359999999994, -334.309465, -333.91819499999997, -333.61827500000004, -333.44041500000003, -333.177135, -332.94246, -332.79224, -331.876195, -331.777965, -331.5966, -331.412005, -331.18195000000003, -331.027365, -330.894715, -330.6914, -330.15242, -330.06710499999997, -329.694655, -329.317585, -329.00568, -328.763775, -328.4251, -327.503395, -326.257795, -326.07253000000003, -324.92222, -324.75253, -321.41982, -320.74120999999997, -320.177975, -319.88712499999997, -319.34189000000003, -314.577745, -314.46629, -313.51520000000005, -312.18392500000004, -311.560625, -310.606805, -307.8913, -306.90778, -305.24376499999994, -303.17399, -301.81586500000003, -300.785895, -300.12989, -299.28319999999997, -297.82371, -296.89414, -296.33447, -295.43652, -292.82905500000004, -289.163735, -287.26757, -286.382065, -284.43186000000003, -284.14117999999996, -277.43406, -275.296, -274.152205, -273.461505, -273.199425, -268.46306500000003, -268.16090499999996, -267.75982, -266.729825, -266.510125, -266.317505, -266.172885, -266.02086, -265.92515000000003, -265.88174000000004, -265.77571500000005, -265.25755000000004, -264.56307499999997, -263.90303, -263.226325, -262.77405, -262.343475, -262.18113, -261.64258500000005, -261.49117, -261.375435, -261.24027, -261.112835, -260.98354, -260.872965, -260.81576499999994, -260.48762, -260.331635, -260.22124, -260.12441, -260.0761, -260.02528499999994, -259.85850500000004, -259.75171, -259.50122999999996, -259.48754499999995, -259.18659, -259.072375, -257.79579, -257.564355, -257.418635, -257.09438, -256.83209, -256.75083, -256.46355500000004, -256.2582, -256.079285, -256.044535, -255.42737499999998, -255.091675, -254.98769, -254.89673999999997, -254.869735, -254.710465, -254.67336999999998, -254.635775, -254.63081, -254.6046, -254.445955, -254.297925, -254.263625, -253.878635, -253.815455, -253.761275, -253.650105, -253.561825, -253.48289, -252.960445, -252.762345, -252.61958, -252.57919, -252.50377, -252.451635, -252.30553, -252.23859, -252.19118, -252.173405, -252.09231499999999, -251.8222, -251.813915, -251.74311999999998, -251.65421500000002, -251.546355, -251.491335, -251.361465, -251.34623, -251.325765, -251.24737500000003, -251.16346, -251.099445, -251.07354500000002, -251.02993000000004, -250.93584, -250.85613999999998, -250.67537499999997, -250.654085, -250.42494499999998, -250.37125, -250.32988999999998, -250.26073000000002, -250.13831, -250.04236500000002, -249.918995, -249.765485, -249.68437500000002, -249.4283, -249.37634, -249.30034, -249.13319, -249.06853, -248.987175, -248.96186, -248.95120500000002, -248.8042, -248.76442000000003, -248.48864499999996, -248.36196999999999, -248.01313, -247.87119, -247.65267, -247.568335, -247.376745, -247.306375, -247.297055, -247.05667, -246.93833, -246.85843, -246.77297500000003, -246.72810000000004, -246.37329499999998, -246.32879, -246.286315, -246.27669500000002, -246.249505, -246.19763, -246.16814499999998, -246.108365, -246.05267, -246.017585, -245.91657500000002, -245.82851, -245.81219000000002, -245.800025, -245.78805999999997, -245.733215, -245.68465, -245.655825, -245.529335, -245.52387, -245.5216, -245.474695, -245.42238500000002, -245.107945, -245.01419, -244.95922, -244.94189, -244.910355, -244.88755000000003, -244.78378499999997, -244.71753999999999, -244.474135, -244.38099999999997, -244.14416500000002, -244.04393, -243.97588, -243.83253, -243.703855, -243.43312, -243.395275, -243.32042500000003, -243.150435, -243.10045499999998, -243.05900499999998, -243.019805, -242.961345, -242.90996, -242.77666, -242.70781, -242.63782, -242.454075, -242.309625, -242.173565, -242.15838499999998, -242.069365, -241.96508999999998, -241.79703999999998, -241.691105, -241.52291499999998, -241.50845, -241.41952499999996, -241.35498, -241.24559499999998, -241.20755, -240.72836999999998, -240.640515, -240.623105, -240.59446499999999, -240.55461499999998, -240.53229499999998, -240.51450999999997, -240.50662999999997, -240.28524, -240.18795, -240.0512, -239.94376499999998, -239.511595, -239.462735, -239.42895, -239.425675, -239.28637500000002, -239.08624999999998, -238.94681, -238.913755, -238.893515, -238.83744000000002, -238.719685, -238.65366999999998, -238.60028, -238.56286, -238.44814499999998, -238.420795, -238.40108, -238.392225, -238.327535, -238.29188, -238.23388, -238.11518, -238.013465, -237.86502, -237.82650999999998, -237.637945, -237.52687, -237.36494499999998, -237.255945, -237.24174500000004, -237.19995500000002, -237.159055, -237.15517499999999, -237.070605, -236.995435, -236.97220000000002, -236.74971, -236.70161000000002, -236.696585, -236.46605, -236.41228, -236.39765999999997, -236.38360999999998, -236.32896, -236.29998, -236.18987500000003, -236.15648, -236.070285, -236.00047999999998, -235.981655, -235.874775, -235.70361499999999, -235.633505, -235.562695, -235.51058999999998, -235.47766000000001, -235.44344, -235.42930500000003, -235.42302500000002, -235.38803000000001, -235.309025, -235.21132, -234.70719, -234.66135500000001, -234.57572000000002, -234.49843, -234.45008, -234.34579, -234.20236, -234.15870999999999, -234.08715, -233.95839, -233.9497, -233.94146, -233.9301, -233.68842, -233.62626, -233.50753, -233.40869999999998, -233.34229, -233.13213000000002, -233.10541, -233.09573, -233.087975, -233.02187, -232.94689499999998, -232.914675, -232.88556499999999, -232.86595499999999, -232.819795, -232.77696, -232.74672500000003, -232.68307, -232.62583, -232.55330500000002, -232.48992, -232.44971500000003, -232.38495, -232.35769, -232.32421499999998, -232.261035, -232.15450999999996, -232.09073, -232.05163000000002, -231.985385, -231.92315000000002, -231.79595000000003, -231.44707, -231.41698000000002, -231.36042, -231.31142999999997, -231.05097999999998, -230.95518499999997, -230.85862500000002, -230.846525, -230.837025, -230.83070500000002, -230.809665, -230.57479999999998, -230.53617, -230.453805, -230.33326499999998, -230.220005, -230.168815, -230.13992000000002, -230.12974499999999, -230.04525999999998, -229.875305, -229.70938, -229.66973, -229.65677499999998, -229.639995, -229.6231, -229.571795, -229.47932000000003, -229.45699000000002, -229.3997, -229.277075, -229.269075, -229.23201999999998, -229.193345, -229.11253499999998, -229.07968, -229.046095, -229.00074, -228.96792500000004, -228.90418499999998, -228.83373, -228.63601, -228.51732, -228.427625, -228.35295, -228.230115, -228.07181000000003, -227.94394499999999, -227.854445, -227.84457500000002, -227.832655, -227.68759, -227.64432, -227.639645, -227.40453, -227.40063, -227.37668000000002, -227.353735, -227.26064, -226.975755, -226.921325, -226.76803, -226.72306000000003, -226.644105, -226.503395, -226.47411999999997, -226.33769999999998, -226.287125, -226.22421500000002, -226.07897499999999, -225.994415, -225.90399000000002, -225.88318, -225.78668, -225.63335, -225.44141, -225.389185, -225.316645, -225.27828499999998, -225.26510000000002, -225.249525, -225.20226, -225.156235, -225.13061499999998, -225.006735, -224.93778, -224.69644, -224.66128500000002, -224.65770500000002, -224.61511000000002, -224.438305, -224.436735, -224.410325, -224.15608, -224.141225, -224.13216, -224.002065, -223.96956, -223.890065, -223.87318500000003, -223.84859, -223.706905, -223.65687499999999, -223.59974, -223.55104500000002, -223.46239, -223.30552500000002, -223.23040500000002, -223.18412, -223.16897, -223.127925, -223.08727, -223.04979000000003, -222.82108499999998, -222.792385, -222.66618, -222.658295, -222.63664, -222.613645, -222.54416, -222.483655, -222.42793, -222.34278999999998, -222.236365, -222.16931999999997, -221.95761500000003, -221.89405, -221.81733, -221.701955, -221.658455, -221.60817500000002, -221.572005, -221.542075, -221.42070999999999, -221.273795, -221.21575, -221.171025, -220.93029, -220.63687, -220.51957000000002, -220.47614000000002, -220.470425, -220.42717000000002, -220.388325, -220.37761500000002, -220.36209000000002, -220.19888, -220.10038000000003, -220.03496, -219.965355, -219.90839999999997, -219.895175, -219.88011999999998, -219.86064499999998, -219.835095, -219.819565, -219.80723999999998, -219.72715, -219.7041, -219.69286, -219.670265, -219.32637, -219.28289999999998, -219.27189999999996, -219.25469999999999, -219.10061000000002, -219.028525, -219.02059500000001, -218.91613, -218.825915, -218.776365, -218.52973, -218.429775, -218.36652499999997, -218.32515999999998, -218.27933000000002, -217.803295, -217.77307000000002, -217.67751, -217.646515, -217.51245, -217.46097999999998, -217.45664499999998, -217.43737000000002, -217.410725, -217.383965, -217.36069500000002, -217.33675, -217.248355, -217.209905, -216.96500500000002, -216.88269000000003, -216.865365, -216.847015, -216.81867499999998, -216.76527499999997, -216.69848, -216.60553000000002, -216.54242, -216.50453000000002, -216.48047000000003, -216.27153, -216.062695, -216.00594, -215.87188, -215.833045, -215.781725, -215.75411, -215.678985, -215.40475999999998, -215.367665, -215.33689999999999, -215.32234, -215.31481499999998, -215.23151000000001, -214.972915, -214.940945, -214.92001, -214.87491000000003, -214.820555, -214.71815000000004, -214.65939500000002, -214.655555, -214.540265, -214.50351, -214.267965, -214.182245, -213.933475, -213.73282, -213.60089, -213.51063, -213.498415, -213.40756, -213.27424, -213.11906, -213.0038, -212.94293, -212.92556499999998, -212.79688, -212.661115, -212.631575, -212.60894000000002, -212.5428, -212.46773, -212.268505, -212.177075, -212.10156, -212.076325, -212.04488500000002, -211.978655, -211.64558499999998, -211.58074000000002, -211.28882, -211.262585, -211.254595, -211.19647, -211.12884, -211.02299, -210.91393499999998, -210.85473499999998, -210.78688, -210.74514499999998, -210.69113, -210.64591000000001, -210.555995, -210.490835, -210.39484, -210.35211500000003, -210.30851, -210.22381000000001, -210.126035, -210.02457500000003, -209.977745, -209.94562, -209.937555, -209.908705, -209.89810999999997, -209.88801999999998, -209.864045, -209.79034000000001, -209.709005, -209.67289, -209.65429, -209.540335, -209.44843500000002, -209.33526, -209.24665000000002, -209.18914, -209.15771, -209.06138499999997, -209.04075, -208.852055, -208.82142, -208.793405, -208.77683000000002, -208.755715, -208.72008000000002, -208.466975, -208.44207, -208.397855, -208.37300999999997, -208.23752000000002, -208.174675, -208.110455, -208.05841, -208.031385, -207.97950500000002, -207.94309, -207.81963000000002, -207.76812999999999, -207.71495499999997, -207.44218999999998, -207.40954, -207.341925, -207.219405, -207.20693, -207.194085, -207.106605, -207.08812, -207.051265, -206.894635, -206.73663, -206.58992999999998, -206.477615, -206.41364499999997, -206.34425499999998, -206.29007, -206.21805, -206.15535500000001, -206.12884, -206.10674999999998, -206.09542, -206.08326, -206.05338999999998, -206.02581, -205.98330500000003, -205.770305, -205.73559999999998, -205.72458, -205.699175, -205.63308, -205.59629999999999, -205.52905999999996, -205.48384499999997, -205.45987, -205.43572999999998, -205.32735000000002, -205.29978, -205.22717, -205.157955, -205.080735, -205.02607999999998, -204.98936499999996, -204.82718999999997, -204.641915, -204.63752, -204.430515, -204.40284000000003, -204.28415, -204.24294, -204.176045, -204.11133999999998, -204.05382, -203.78912499999998, -203.76114, -203.71721, -203.66911500000003, -203.61090500000003, -203.54494, -203.46695, -203.38611500000002, -203.25839, -203.20324, -203.195395, -203.183005, -203.167055, -203.15175, -203.09666499999997, -203.03105, -202.99462499999998, -202.98501, -202.87433, -202.84213499999998, -202.57198000000002, -202.441495, -202.405485, -202.10238999999999, -202.07748999999998, -202.04784, -202.00471, -201.97353, -201.97024, -201.87124999999997, -201.739895, -201.59357, -201.31497, -201.17512, -201.15972, -201.144935, -201.13332499999999, -201.069975, -201.048515, -200.89270499999998, -200.72954499999997, -200.71409, -200.69484, -200.618965, -200.57971999999998, -200.51219499999996, -200.44559999999998, -200.32466, -200.252665, -200.21759500000002, -200.15274, -200.07754, -199.95746000000003, -199.915545, -199.90265499999998, -199.83882, -199.78085499999997, -199.47728, -199.45802, -199.30304, -199.24956000000003, -199.21747, -199.132185, -199.060575, -199.03507000000002, -199.00333, -198.97245999999998, -198.9476, -198.927945, -198.91267, -198.89146, -198.859035, -198.72433999999998, -198.71854, -198.71152, -198.624995, -198.53524000000002, -198.445295, -198.40531, -198.35397, -198.31845, -198.17194999999998, -198.078395, -197.99856, -197.96708, -197.72628, -197.704435, -197.46233, -197.286245, -197.21210000000002, -197.07964, -197.03092, -196.89623, -196.638715, -196.636135, -196.53947499999998, -196.51774999999998, -196.47159499999998, -196.42145, -196.339215, -196.22007000000002, -196.154065, -196.10325, -196.05851, -196.02420999999998, -195.986875, -195.741265, -195.66203, -195.61158999999998, -195.561205, -195.47891500000003, -195.27166499999998, -195.22429499999998, -195.20664499999998, -195.16201999999998, -195.07698, -194.97529500000002, -194.73206500000003, -194.68527, -194.653855, -194.56154, -194.42767000000003, -194.352395, -194.23122, -194.047865, -193.80370499999998, -193.76998500000002, -193.695515, -193.57087, -193.521505, -193.42807, -193.332385, -193.207515, -193.194465, -193.176685, -193.15013499999998, -193.00558, -192.831185, -192.73812, -192.441275, -192.29558, -192.14861000000002, -192.11762499999998, -192.076245, -192.03006, -191.99736000000001, -191.936875, -191.86895499999997, -191.84339999999997, -191.83488499999999, -191.78906999999998, -191.706055, -191.576545, -191.37348000000003, -191.289645, -191.21912999999998, -191.171955, -191.150235, -191.084545, -190.96876499999996, -190.889715, -190.84449, -190.804515, -190.76712500000002, -190.7018, -190.680765, -190.66105499999998, -190.652375, -190.641455, -190.48477, -190.40682999999999, -190.34078, -190.302215, -190.23765999999998, -190.08991, -189.984315, -189.95727499999998, -189.92066499999999, -189.8496, -189.786595, -189.71764000000002, -189.652455, -189.64017, -189.635065, -189.360075, -189.340035, -189.334615, -189.306515, -189.27610499999997, -189.26603999999998, -189.100855, -189.03044, -188.94834, -188.87294, -188.79644, -188.7362, -188.72071, -188.66754, -188.592735, -188.553505, -188.52807, -188.48944, -188.42852, -188.35909499999997, -188.272505, -188.17516, -188.084, -187.746625, -187.64264500000002, -187.59519999999998, -187.49532, -187.44933, -187.386365, -187.30714999999998, -187.18809, -186.965675, -186.900535, -186.82053000000002, -186.7383, -186.70356, -186.67885, -186.550255, -186.38772999999998, -186.181465, -186.11883999999998, -186.00027, -185.892995, -185.79971, -185.786785, -185.64056, -185.59044999999998, -185.56631000000002, -185.55114500000002, -185.501175, -185.47116000000003, -185.44468, -185.32148999999998, -185.25426499999998, -185.160695, -184.972025, -184.67189, -184.503895, -184.20333499999998, -184.05575500000003, -183.94732000000002, -183.868895, -183.82854, -183.79843, -183.78983, -183.740095, -183.71943, -183.68733, -183.65681999999998, -183.62671, -183.53071999999997, -183.39446, -183.35403, -183.25448999999998, -183.1345, -182.88539, -182.81622, -182.79537, -182.57384, -182.445255, -182.332405, -182.25449, -182.0933, -181.86297000000002, -181.7493, -181.64521499999998, -181.62102499999997, -181.46722499999998, -181.35577999999998, -181.30752, -181.11702000000002, -180.98216, -180.68831, -180.595125, -180.54165999999998, -180.34406, -180.29620999999997, -179.94585999999998, -179.840565, -179.60394499999998, -179.43361499999997, -179.266035, -179.18794, -179.10287, -178.9355, -178.62825499999997, -178.567135, -178.48993000000002, -178.452655, -178.34126499999996, -178.18217499999997, -178.09505000000001, -177.939785, -177.911565, -177.881695, -177.872545, -177.83323000000001, -177.79467, -177.727955, -177.55491999999998, -176.88088000000002, -176.84106500000001, -176.772595, -176.692545, -176.52651000000003, -176.268305, -176.21056, -176.11928999999998, -175.92933499999998, -175.8107, -175.766035, -175.71361000000002, -175.692725, -175.67410999999998, -175.66565999999997, -175.607745, -175.48915, -175.39815, -175.393255, -175.37053, -175.275745, -175.25028, -175.205425, -175.083955, -175.07243000000003, -174.95343, -174.82936, -174.796295, -174.72857999999997, -174.64592, -174.349645, -174.26942000000003, -174.2148, -174.06774000000001, -174.001955, -173.94652, -173.905005, -173.87635, -173.68849, -173.53787, -173.44586, -173.343575, -173.28902499999998, -173.22797500000001, -173.20550000000003, -173.14624, -173.07144, -172.92139500000002, -172.82636000000002, -172.82442, -172.80677500000002, -172.741405, -172.615355, -172.48044499999997, -172.18237, -172.14252499999998, -172.12876, -172.10491, -172.071245, -172.05405000000002, -172.02035500000002, -171.958705, -171.89361, -171.82247, -171.73304000000002, -171.60758499999997, -171.60477500000002, -171.52237, -171.26716499999998, -171.154295, -170.94672000000003, -170.831765, -170.79713499999997, -170.717875, -170.60944, -170.55089, -170.44734499999998, -170.43192000000002, -170.17284, -170.10875499999997, -170.03094, -169.998495, -169.91582, -169.80808, -169.50964, -168.978455, -168.97593999999998, -168.95905, -168.88039500000002, -168.83128, -168.75778499999998, -168.65604000000002, -168.56887, -168.38331, -168.36538, -168.20010000000002, -168.15128500000003, -168.00228500000003, -167.95457, -167.841205, -167.76169000000002, -167.72076, -167.68772, -167.429235, -167.33076499999999, -167.323425, -167.233445, -167.10750000000002, -167.02948500000002, -166.9754, -166.914645, -166.66089499999998, -166.26871, -166.26184, -166.109215, -166.08233, -165.87282, -165.77128, -165.72499, -165.643235, -165.575335, -165.49745, -165.460855, -165.42296000000002, -165.35269000000002, -165.14319, -165.09063500000002, -165.00194500000003, -164.800125, -164.727585, -164.59515, -164.43706500000002, -164.35446000000002, -164.276225, -164.09952, -163.96518500000002, -163.72443, -163.4898, -163.410235, -163.375005, -163.347455, -163.300905, -163.233245, -163.14464500000003, -162.89259499999997, -162.03643, -161.92882, -161.77967999999998, -161.71227, -161.679605, -161.47929500000004, -161.08021, -160.97908999999999, -160.89815499999997, -160.87153999999998, -160.76364, -160.71914500000003, -160.642945, -160.467105, -160.36422, -160.06265, -159.94453499999997, -159.79386, -159.625955, -159.52605, -159.49206, -159.2988, -159.283115, -159.19464, -159.14776, -159.10295000000002, -159.01781, -158.977415, -158.92309, -158.493565, -158.46948, -158.251205, -158.0946, -157.93658, -157.905585, -157.7684, -157.691345, -157.588045, -157.41013499999997, -157.26709499999998, -157.15997499999997, -156.999635, -156.93540000000002, -156.91093, -156.76172499999998, -156.622235, -156.61092000000002, -156.425355, -156.327625, -156.119665, -155.992435, -155.892025, -155.82187499999998, -155.79366, -155.75998, -155.73416, -155.622425, -155.46515, -155.44241, -155.398555, -155.33438999999998, -154.440765, -154.371535, -154.338405, -154.204495, -154.03832, -153.88465, -153.65408, -153.54111999999998, -153.47553, -153.399925, -153.09416, -152.99399, -152.942795, -152.64666, -152.42583000000002, -152.410255, -152.394285, -152.208595, -152.01704, -151.76914499999998, -151.708575, -151.61854, -151.57920000000001, -151.29681999999997, -151.12103000000002, -151.07318, -151.035485, -151.00973, -150.87922500000002, -150.70854500000002, -150.38073, -150.34138, -150.28428, -150.15800000000002, -150.096695, -149.70312, -149.495945, -149.31482, -149.21862, -148.891935, -148.63172000000003, -148.57574000000002, -148.49792000000002, -148.41528000000002, -147.868715, -147.64063, -147.20035000000001, -147.114665, -147.02769, -146.94212, -146.866555, -146.597735, -146.51169, -146.43959, -146.368675, -146.14381, -146.04459500000002, -145.9691, -145.86768999999998, -145.74964, -145.62204, -145.54762499999998, -144.88586, -144.52570500000002, -142.990285, -142.916985, -142.74221, -142.66862, -142.4001, -141.995605, -141.71514, -141.52362999999997, -140.54649, -140.180145, -140.14526, -139.913115, -139.64645000000002, -139.362275, -139.10450500000002, -138.86387000000002, -138.09641, -137.800845, -137.719685, -137.67180000000002, -137.61605500000002, -137.480835, -137.040595, -136.618795, -136.29082, -135.764565, -135.731775, -134.89193, -134.57117, -134.20128499999998, -134.111115, -133.529095, -133.145755, -132.010765, -131.95472999999998, -131.547645, -131.371205, -131.117725, -130.97732000000002, -130.82567, -130.64593000000002, -130.32280500000002, -130.11271, -130.062555, -129.83753000000002, -129.367485, -129.0267, -128.899635, -128.184755, -127.74710999999999, -126.18482, -125.898785, -125.37081, -124.89488, -122.646175, -122.34410499999998, -120.62006, -119.460335, -118.53482, -118.44458499999999, -118.318955, -118.27380000000001, -117.13155, -116.88932000000001, -116.416775, -116.127495, -115.38253, -114.93213500000002, -107.478125, -107.339815, -106.85273000000001, -106.377805, -105.465305, -104.74905000000001, -103.57203, -103.38385, -103.18635, -103.073755, -102.68016999999999, -102.51877499999999, -92.54757, -91.65659, -90.91637, -88.158085, -77.942445, -77.00572500000001])
labels = array([4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 1.0, 3.0, 4.0, 3.0, 1.0, 3.0, 1.0, 4.0, 1.0, 0.0, 1.0, 4.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0])
def eqenergy(rows):
    try:
        return np.sum(rows, axis=1, dtype=np.float128)
    except:
        return np.sum(rows, axis=1, dtype=np.longdouble)
def classify(rows):
    energys = eqenergy(rows)

    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<=len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<=len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = labels[numers[indys]]
        if list(defaultindys):
            outputs[defaultindys] = 3
        return outputs
    return thresh_search(energys)

numthresholds = 2250

# Main method
model_cap = numthresholds


def Validate(file):
    #Load Array
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs, cleanarr[:, -1]


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping, trim=False):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    cleanarr = cleanarr.reshape(cleanarr.shape[0], -1)
    if not trim and ignorecolumns != []:
        cleanarr = cleanarr[:, important_idxs]
    with open(preprocessedfile, 'r', encoding='utf-8') as csvinput:
        dirtyreader = csv.reader(csvinput)
        if not headerless:
            header = next(dirtyreader, None)
        #print original header
        if (not headerless):
            print(','.join(header + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            if k >= outputs.shape[0]:
                continue
            print(str(','.join(str(j) for j in (['"' + i + '"' if ',' in i else i for i in row]))) + str(',' if len(important_idxs) != 1 else '') + str(get_key(int(outputs[k]), classmapping)))
                




#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", default=False, help="Output predictor file with only the selected important attributes (to be used with predictors built with -rank)")
    args = parser.parse_args()
    faulthandler.enable()


    if args.validate:
        args.trim = True


    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        output = preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate), trim=args.trim)
        get_key, classmapping = clean(preprocessedfile if output!=-1 else args.csvfile, cleanfile, -1, args.headerless, (not args.validate), trim=args.trim)
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}
        output = None

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile if output!=-1 else args.csvfile, classmapping, trim=args.trim)


    else:
        classifier_type = 'DT'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds, true_labels = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            classbalance = [float(num_class_0)/count, float(num_class_1)/count]
            H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))

            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            json_dict = {'instance_count':                        count ,
                         'classifier_type':                        classifier_type,
                         'classes':                            2 ,
                         'false_negative_instances':    num_FN ,
                         'false_positive_instances':    num_FP ,
                         'true_positive_instances':    num_TP ,
                         'true_negative_instances':    num_TN,
                         'false_negatives':                        FN ,
                         'false_positives':                        FP ,
                         'true_negatives':                        TN ,
                         'true_positives':                        TP ,
                         'number_correct':                        num_correct ,
                         'accuracy': {
                             'best_guess': randguess,
                             'improvement': modelacc-randguess,
                             'model_accuracy': modelacc,
                         },
                         'model_capacity':                        model_cap ,
                         'generalization_ratio':                int(float(num_correct * 100) / model_cap) * H/ 100.0,
                         'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0,
                        'shannon_entropy_of_labels':           H,
                        'classbalance':                        classbalance}
            if args.json:
                pass
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                elif classifier_type == 'RF':
                    print("Classifier Type:                    Random Forest")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0 * H) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            classbalance = [float(numofcertainclass) / count for numofcertainclass in numeachclass.values()]
            H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))

            if args.json:
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'classes':                            n_classes,
                             'number_correct': num_correct,
                             'accuracy': {
                                 'best_guess': randguess,
                                 'improvement': modelacc - randguess,
                                 'model_accuracy': modelacc,
                             },
                             'model_capacity': model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0 * H,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0,
                        'shannon_entropy_of_labels':           H,
                        'classbalance':                        classbalance}
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                elif classifier_type == 'RF':
                    print("Classifier Type:                    Random Forest")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0 * H) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):
            stats = {}
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")
            n_labels = labels.size

            for class_i in range(n_labels):
                stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}
                class_i_indices = np.argwhere(y_true==class_i)
                not_class_i_indices = np.argwhere(y_true!=class_i)
                stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices]==y_true[class_i_indices]))
                stats[int(class_i)]['FP'] = int(np.sum(y_pred[class_i_indices]!=y_true[class_i_indices]))
                stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices]==y_true[not_class_i_indices]))
                stats[int(class_i)]['FN'] = int(np.sum(y_pred[not_class_i_indices]!=y_true[not_class_i_indices]))
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                if not json:
                    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                    sys.exit()
                else:
                    return np.array([]), stats
                

            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------



            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]

            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm, stats
        mtrx, stats = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            json_dict['multiclass_stats'] = stats
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        if output!=-1:
            os.remove(preprocessedfile)
