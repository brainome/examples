#!/usr/bin/env python3
#
# This code has been produced by a free evaluation version of Brainome Table Compiler(tm).
# Portions of this code copyright (c) 2019-2021 by Brainome, Inc. All Rights Reserved.
# Brainome grants an exclusive (subject to our continuing rights to use and modify models),
# worldwide, non-sublicensable, and non-transferable limited license to use and modify this
# predictor produced through the input of your data:
# (i) for users accessing the service through a free evaluation account, solely for your
# own non-commercial purposes, including for the purpose of evaluating this service, and
# (ii) for users accessing the service through a paid, commercial use account, for your
# own internal  and commercial purposes.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome Table Compiler v0.991.
# Invocation: btc train.csv -headerless -f DT -o DT.py -riskoverfit --yes
# Total compiler execution time: 0:01:42.78. Finished on: Mar-17-2021 05:53:46.
# This source code requires Python 3.
#
"""
Classifier Type:                    Decision Tree
System Type:                         6-way classifier
Best-guess accuracy:                 18.88%
Overall Model accuracy:              99.98% (5148/5149 correct)
Overall Improvement over best guess: 81.10% (of possible 81.12%)
Model capacity (MEC):                3187 bits
Generalization ratio:                4.15 bits/bit
Model efficiency:                    0.02%/parameter
Confusion Matrix:
 [16.72% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.02% 14.97% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 13.65% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 17.25% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 18.51% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 18.88%]
Avg. noise resilience per instance:  -0.21dB
Percent of Data Memorized:           74.72%
Note: Unable to split dataset. The predictor was trained and evaluated on the same data.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "train.csv"


#Number of attributes
num_attr = 561
n_classes = 6
transform_true = False

# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target=""
important_idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560]

def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[], trim=False):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target=""
    important_idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560]
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return -1
    if not trim:
        ignorecolumns = []
    if (testfile):
        target = ''
        hc = -1 
    with open(outputcsvfile, "w+", encoding='utf-8') as outputfile:
        with open(inputcsvfile, "r", encoding='utf-8') as csvfile:      # hardcoded utf-8 encoding per #717
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile, encoding='utf-8')):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        if len(important_idxs) == 1:
                            print(",", file=outputfile)
                        else:
                            print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        if len(important_idxs) == 1:
                            print(",", file=outputfile)
                        else:
                            print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False, trim=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result=float(value)
                if math.isnan(result):
                    #if nan parse to string
                    raise ValueError('')
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename, encoding='utf-8') as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+", encoding='utf-8')
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            if not transform_true:
                rowlen = num_attr if trim else num_attr + len(ignorecolumns)
            else:
                rowlen = num_attr_before_transform if trim else num_attr_before_transform + len(ignorecolumns)      # noqa
            if (not testfile):
                rowlen = rowlen + 1    
            if ((len(row) - (1 if ((testfile and len(important_idxs) == 1)) else 0))  != rowlen) and not (row == ['','']):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs. Expected Row length: " + str(rowlen) + ", Actual Row Length: " + str(len(row)))
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([-376.48160500000006, -375.95964000000004, -375.39613499999996, -374.79122499999994, -374.14011, -373.86792, -373.60209999999995, -373.36249999999995, -373.323845, -372.89910499999996, -372.89104499999996, -372.847885, -372.67571, -372.177255, -371.92735999999996, -371.79138, -371.70563, -371.65198, -371.62741, -371.54775, -371.48324, -371.353475, -371.33087, -371.17177499999997, -371.14514999999994, -371.09833499999996, -371.05997, -371.02676499999995, -370.949795, -370.92564500000003, -370.86658, -370.83117000000004, -370.794185, -370.713835, -370.67235500000004, -370.60287, -370.546835, -370.52056500000003, -370.5129, -370.481875, -370.42697499999997, -370.378775, -370.352755, -370.301285, -370.165875, -370.143425, -370.12253, -370.107585, -370.074215, -370.03952000000004, -370.02221, -369.99327, -369.97092, -369.96381, -369.949815, -369.94087, -369.92215999999996, -369.909995, -369.893145, -369.8738, -369.86712, -369.85661500000003, -369.845525, -369.83619500000003, -369.82838000000004, -369.82662, -369.82398, -369.811975, -369.79887499999995, -369.7915899999999, -369.774365, -369.75784999999996, -369.74767499999996, -369.71906, -369.69381999999996, -369.69034, -369.660305, -369.643795, -369.62850000000003, -369.62012, -369.6123, -369.60893, -369.587765, -369.57766000000004, -369.568265, -369.55973500000005, -369.551415, -369.52297, -369.48898, -369.47885, -369.46582, -369.451235, -369.42834, -369.41272, -369.4066, -369.31048499999997, -369.30602500000003, -369.281345, -369.25597, -369.236145, -369.20739000000003, -369.177, -369.16333499999996, -369.15009499999996, -369.11749000000003, -369.11155, -369.09959499999997, -369.085195, -369.07363999999995, -369.06956, -369.06460000000004, -369.051645, -369.035575, -369.00364, -368.99974000000003, -368.975405, -368.96184, -368.94980999999996, -368.94310499999995, -368.89167999999995, -368.8872, -368.87231999999995, -368.86026, -368.85646, -368.84790000000004, -368.834465, -368.825305, -368.81602, -368.79086, -368.768235, -368.75677499999995, -368.74757, -368.73237, -368.72292, -368.716125, -368.70277, -368.695895, -368.673215, -368.650435, -368.622815, -368.618385, -368.61758999999995, -368.53539, -368.47744, -368.46348, -368.430485, -368.40977, -368.402875, -368.39699, -368.393555, -368.38364, -368.37123499999996, -368.36539, -368.36415, -368.362525, -368.31154, -368.30019, -368.29606, -368.29561, -368.280085, -368.2629, -368.25915499999996, -368.21638, -368.18715, -368.16582, -368.15162499999997, -368.149965, -368.1465, -368.11924999999997, -368.11885, -368.11116000000004, -368.105805, -368.09254999999996, -368.08119999999997, -368.074515, -368.06858, -368.067435, -368.056925, -368.05448, -368.036955, -368.009625, -368.003455, -367.995885, -367.98798500000004, -367.97049000000004, -367.961365, -367.949215, -367.90333, -367.89504999999997, -367.89158, -367.884355, -367.88055499999996, -367.87636, -367.856675, -367.835185, -367.82252500000004, -367.80361, -367.79457, -367.78409, -367.775715, -367.76826, -367.759105, -367.75211, -367.721495, -367.71040999999997, -367.70728499999996, -367.70414999999997, -367.6944, -367.69158999999996, -367.69005, -367.689155, -367.68670499999996, -367.68295, -367.67784000000006, -367.667755, -367.65332, -367.651845, -367.64684, -367.64124499999997, -367.610065, -367.60385, -367.59842000000003, -367.593745, -367.578055, -367.56438, -367.55136, -367.52810999999997, -367.52598, -367.52065500000003, -367.516655, -367.50337, -367.482305, -367.473795, -367.451275, -367.44329500000003, -367.43644500000005, -367.435455, -367.43390999999997, -367.427755, -367.39136499999995, -367.35745999999995, -367.3544, -367.352575, -367.34821, -367.342995, -367.33633499999996, -367.324285, -367.31577999999996, -367.30553499999996, -367.29436, -367.28176, -367.243805, -367.231575, -367.221025, -367.21079, -367.20723, -367.20258, -367.19800499999997, -367.188975, -367.17947, -367.1699, -367.161925, -367.15689499999996, -367.15198999999996, -367.12278000000003, -367.097015, -367.071495, -367.065655, -367.04378499999996, -367.02211, -367.02099499999997, -367.020265, -367.01458, -367.003885, -367.000675, -366.99896, -366.99366, -366.9774, -366.955645, -366.91862, -366.867825, -366.86531, -366.86087999999995, -366.85038999999995, -366.837945, -366.82774, -366.81764499999997, -366.80611999999996, -366.79708, -366.7514, -366.747515, -366.74454, -366.7346, -366.726035, -366.71660499999996, -366.70563, -366.697025, -366.694395, -366.688695, -366.68291999999997, -366.67328, -366.66267, -366.66108999999994, -366.65446, -366.64526, -366.63304, -366.60612000000003, -366.60377500000004, -366.59652500000004, -366.565385, -366.544, -366.537445, -366.53316, -366.530705, -366.50498000000005, -366.455415, -366.450345, -366.44558500000005, -366.43982500000004, -366.429785, -366.423385, -366.42176, -366.410805, -366.395305, -366.3921, -366.38867, -366.38477, -366.383185, -366.378795, -366.37214500000005, -366.36196, -366.358575, -366.353295, -366.34346500000004, -366.33274, -366.315465, -366.296415, -366.29276, -366.2919, -366.283185, -366.264385, -366.246265, -366.24443499999995, -366.239, -366.22378000000003, -366.21441500000003, -366.21204500000005, -366.19473, -366.18683, -366.183035, -366.18203, -366.16860499999996, -366.14086999999995, -366.13627499999996, -366.12189, -366.1176, -366.094555, -366.085045, -366.08099999999996, -366.07430999999997, -366.05995, -366.05383500000005, -366.049345, -366.00656000000004, -365.996575, -365.991775, -365.987475, -365.977715, -365.96674, -365.960505, -365.95234000000005, -365.94599, -365.94431, -365.94264999999996, -365.940475, -365.93733, -365.91048, -365.90405499999997, -365.899035, -365.8913, -365.870595, -365.86521500000003, -365.84718, -365.842635, -365.83757, -365.83198, -365.825635, -365.823245, -365.80882, -365.80048, -365.78994, -365.78783500000003, -365.781875, -365.77932, -365.77373, -365.760885, -365.73174, -365.727315, -365.72546, -365.71529499999997, -365.7088, -365.69845000000004, -365.68489, -365.66985, -365.66677, -365.66535, -365.66424499999994, -365.65635999999995, -365.63701000000003, -365.62018, -365.605375, -365.597965, -365.59251, -365.58826500000004, -365.57948999999996, -365.573535, -365.557545, -365.55324, -365.54674, -365.54346499999997, -365.54199, -365.54092, -365.53899, -365.526705, -365.52315999999996, -365.51999, -365.50686499999995, -365.505025, -365.50119, -365.494915, -365.48707, -365.48076000000003, -365.47826499999996, -365.47426499999995, -365.471495, -365.469865, -365.466675, -365.46241999999995, -365.429785, -365.42452, -365.41812500000003, -365.413605, -365.40536999999995, -365.40173, -365.39982, -365.39869, -365.38864, -365.37831, -365.36803, -365.36342, -365.36262999999997, -365.360675, -365.357175, -365.347495, -365.342995, -365.335015, -365.329125, -365.326415, -365.302015, -365.29609, -365.28713500000003, -365.28252000000003, -365.272025, -365.262605, -365.25152, -365.2478, -365.244155, -365.23614000000003, -365.22553500000004, -365.21711, -365.18394, -365.18134999999995, -365.179785, -365.16432, -365.12315, -365.11325, -365.10745499999996, -365.08947, -365.08212499999996, -365.07491, -365.074, -365.073885, -365.067325, -365.06683499999997, -365.05666, -365.046915, -365.039225, -365.032935, -365.02707499999997, -365.014905, -364.993475, -364.98105499999997, -364.972405, -364.96954999999997, -364.967315, -364.949345, -364.941985, -364.93492, -364.920035, -364.91247999999996, -364.904275, -364.90063000000004, -364.89828, -364.895885, -364.89081, -364.88131, -364.86163, -364.8415, -364.836585, -364.83279, -364.819635, -364.813215, -364.809265, -364.80618499999997, -364.798725, -364.789255, -364.775665, -364.764, -364.76037499999995, -364.75314000000003, -364.73944500000005, -364.72877, -364.714285, -364.69693500000005, -364.68772, -364.683475, -364.6793, -364.67422, -364.65803999999997, -364.652295, -364.62458, -364.62194999999997, -364.611935, -364.597675, -364.58580500000005, -364.56917000000004, -364.557825, -364.55452499999996, -364.549375, -364.540065, -364.53384, -364.52806499999997, -364.51509500000003, -364.506365, -364.49559, -364.48576, -364.475685, -364.464475, -364.45415, -364.43859499999996, -364.431965, -364.428075, -364.418505, -364.415825, -364.396555, -364.39370499999995, -364.374035, -364.370935, -364.36841, -364.36352, -364.356585, -364.329145, -364.31799, -364.31122, -364.29606, -364.281555, -364.263645, -364.26255000000003, -364.25987, -364.254985, -364.25054, -364.24921, -364.24525, -364.24129, -364.22749999999996, -364.21020500000003, -364.2049, -364.20084, -364.19561000000004, -364.16529, -364.156905, -364.15348, -364.152035, -364.14799999999997, -364.13829999999996, -364.12755000000004, -364.12143000000003, -364.114445, -364.11327, -364.10409500000003, -364.098305, -364.07312, -364.07115999999996, -364.0698, -364.06750999999997, -364.066315, -364.064925, -364.05099499999994, -364.04375, -364.03069500000004, -364.02528500000005, -364.012945, -364.004915, -363.99944999999997, -363.996735, -363.99467, -363.991175, -363.980135, -363.965465, -363.960325, -363.95806500000003, -363.947835, -363.939715, -363.92256499999996, -363.90432, -363.887605, -363.882295, -363.87199499999997, -363.86478999999997, -363.86408, -363.86145, -363.853635, -363.84875999999997, -363.843695, -363.84171000000003, -363.840895, -363.83676, -363.83071, -363.81620499999997, -363.801055, -363.78459, -363.767025, -363.759005, -363.75816999999995, -363.75206499999996, -363.748515, -363.74234, -363.74071, -363.72932000000003, -363.72515, -363.697605, -363.69556, -363.67114499999997, -363.65542500000004, -363.65308500000003, -363.650355, -363.64744500000006, -363.638295, -363.62541, -363.61888999999996, -363.60603000000003, -363.595, -363.591675, -363.584375, -363.575695, -363.562685, -363.552235, -363.54682, -363.54635499999995, -363.54508, -363.51469999999995, -363.512615, -363.50609, -363.49314, -363.48574499999995, -363.485055, -363.47447, -363.45811000000003, -363.44946, -363.43998999999997, -363.43689499999994, -363.42341, -363.41852, -363.40717, -363.38905, -363.372975, -363.35113, -363.3338, -363.32896999999997, -363.32434, -363.31770500000005, -363.31031, -363.28977499999996, -363.28407000000004, -363.280095, -363.275755, -363.26122, -363.205715, -363.192445, -363.1794, -363.1633, -363.156145, -363.14191, -363.13948, -363.12558, -363.113775, -363.112125, -363.10821999999996, -363.10070499999995, -363.09192999999993, -363.08220500000004, -363.068175, -363.06512, -363.06044499999996, -363.049405, -363.041195, -363.01122, -363.00722499999995, -362.96898000000004, -362.960025, -362.955635, -362.951605, -362.94833, -362.942185, -362.92669, -362.92474000000004, -362.92310499999996, -362.922055, -362.916165, -362.904905, -362.883985, -362.87764000000004, -362.87503, -362.87219500000003, -362.87051, -362.86255500000004, -362.85327, -362.847195, -362.82909, -362.791215, -362.786745, -362.781695, -362.764155, -362.76051, -362.75551499999995, -362.73335, -362.72954, -362.72497, -362.71963, -362.70936, -362.68846499999995, -362.677355, -362.66998, -362.66811500000006, -362.662445, -362.657945, -362.656465, -362.65085, -362.640625, -362.63014999999996, -362.61091999999996, -362.60286999999994, -362.58006, -362.57171500000004, -362.56368000000003, -362.553495, -362.54569000000004, -362.543585, -362.53891999999996, -362.53128, -362.527565, -362.521945, -362.50044, -362.49321499999996, -362.47492, -362.43403, -362.430755, -362.42783999999995, -362.40309, -362.38159999999993, -362.3668, -362.35847, -362.332205, -362.315195, -362.30344, -362.29256499999997, -362.20406, -362.19559000000004, -362.193375, -362.19174, -362.18524, -362.17636, -362.172715, -362.16873, -362.150525, -362.143635, -362.13645499999996, -362.1349, -362.12854, -362.12557999999996, -362.10582, -362.10459000000003, -362.104045, -362.09513499999997, -362.09281, -362.087015, -362.07965, -362.07376, -362.06465, -362.059195, -362.05475, -362.04542, -362.02590499999997, -362.02122499999996, -362.01838999999995, -362.016475, -362.00966999999997, -362.00766999999996, -361.99932, -361.98812, -361.96777, -361.95585, -361.94488, -361.9387, -361.93703000000005, -361.92510000000004, -361.92175, -361.91451499999994, -361.90887, -361.90202, -361.89531999999997, -361.88906499999996, -361.88256499999994, -361.87838, -361.87244499999997, -361.865835, -361.85952, -361.85557500000004, -361.847315, -361.84302499999995, -361.824745, -361.81491500000004, -361.80215, -361.80041500000004, -361.73125500000003, -361.72197500000004, -361.711095, -361.704115, -361.694215, -361.692985, -361.664675, -361.643635, -361.634235, -361.62944500000003, -361.627575, -361.624875, -361.622125, -361.6157, -361.608645, -361.60262, -361.59587999999997, -361.580295, -361.567775, -361.56616499999996, -361.56488, -361.56151, -361.554655, -361.55057, -361.52694999999994, -361.52191, -361.51481, -361.50348999999994, -361.488775, -361.467195, -361.44903999999997, -361.437565, -361.431935, -361.428715, -361.410215, -361.388585, -361.36565999999993, -361.343985, -361.337395, -361.330725, -361.327135, -361.31469000000004, -361.299755, -361.28623500000003, -361.2768, -361.258285, -361.24899500000004, -361.23172, -361.202105, -361.18098499999996, -361.15713, -361.14387, -361.14073499999995, -361.13228499999997, -361.126215, -361.11474, -361.10400500000003, -361.09643, -361.09112000000005, -361.069905, -361.069335, -361.050935, -361.039325, -361.03316, -361.02792, -361.02598, -361.02027, -361.00142500000004, -360.985375, -360.97997, -360.97871, -360.94732999999997, -360.943215, -360.93908999999996, -360.936045, -360.93404999999996, -360.930975, -360.92886, -360.928275, -360.918585, -360.88958499999995, -360.87660999999997, -360.85654, -360.853865, -360.817995, -360.78108, -360.77006, -360.75599, -360.7513, -360.746715, -360.726545, -360.70516499999997, -360.678995, -360.636175, -360.61398499999996, -360.602655, -360.58724499999994, -360.56721, -360.56326, -360.559665, -360.52788, -360.51718, -360.51216, -360.48802, -360.47804, -360.473485, -360.43142, -360.424835, -360.41972, -360.410375, -360.3986, -360.394045, -360.37756, -360.36420499999997, -360.34691999999995, -360.32273499999997, -360.29974, -360.2724, -360.236105, -360.219515, -360.21406, -360.20519, -360.19583, -360.19274, -360.19135, -360.16135999999995, -360.14865499999996, -360.12205, -360.10851, -360.098855, -360.09457, -360.08777, -360.0851, -360.05267, -360.02473499999996, -360.01712999999995, -360.01099, -360.00100999999995, -359.99048500000004, -359.980955, -359.975145, -359.971365, -359.96737, -359.957635, -359.90418, -359.89741000000004, -359.89069, -359.88071, -359.876185, -359.87375000000003, -359.87283, -359.865225, -359.85145, -359.842025, -359.83826999999997, -359.81813, -359.80720499999995, -359.80213000000003, -359.796965, -359.78467, -359.77112999999997, -359.764325, -359.75726499999996, -359.75196, -359.74805000000003, -359.743145, -359.74134499999997, -359.73486, -359.72336499999994, -359.696695, -359.691135, -359.68309999999997, -359.65382999999997, -359.638955, -359.62888999999996, -359.618425, -359.615375, -359.60389, -359.59772499999997, -359.588725, -359.57267, -359.56404499999996, -359.52854, -359.5145, -359.504585, -359.497155, -359.49237000000005, -359.468725, -359.4565, -359.45029, -359.443315, -359.433005, -359.418745, -359.38789999999995, -359.35479499999997, -359.33021499999995, -359.30341, -359.28918, -359.26757, -359.24609999999996, -359.24291500000004, -359.196345, -359.18016, -359.143965, -359.12532, -359.11456, -359.07793, -359.07154, -359.052955, -359.038175, -359.02423, -359.0189, -359.00781, -358.997995, -358.996565, -358.99336999999997, -358.973895, -358.97103500000003, -358.95067, -358.924675, -358.910755, -358.90316, -358.85672, -358.84938, -358.83966, -358.836275, -358.83087, -358.8124, -358.79792999999995, -358.79277, -358.783025, -358.77617499999997, -358.720945, -358.71416, -358.70088499999997, -358.68273999999997, -358.6389, -358.61554, -358.59488, -358.580415, -358.57034, -358.560965, -358.55533999999994, -358.54981999999995, -358.535825, -358.521695, -358.514805, -358.503255, -358.48614499999996, -358.47567499999997, -358.4692, -358.45513, -358.434795, -358.41814999999997, -358.41077, -358.39872, -358.383115, -358.37505999999996, -358.37335499999995, -358.369055, -358.3296, -358.29283, -358.28111, -358.27749, -358.2675, -358.25453, -358.203915, -358.19097999999997, -358.18256499999995, -358.173385, -358.166925, -358.14254000000005, -358.12997499999994, -358.116945, -358.10553, -358.10177, -358.09138499999995, -358.07237, -358.059755, -358.04929, -358.04238, -358.03781499999997, -358.03047, -357.99879999999996, -357.99649, -357.98879999999997, -357.982455, -357.97839, -357.968935, -357.923385, -357.917435, -357.91269, -357.8968, -357.87170000000003, -357.856015, -357.842495, -357.82872000000003, -357.82229500000005, -357.819545, -357.81869, -357.817005, -357.802665, -357.76538999999997, -357.75699, -357.75050999999996, -357.74555, -357.742015, -357.7312, -357.71998, -357.70664, -357.68517999999995, -357.684035, -357.680965, -357.67812, -357.677625, -357.66762, -357.65441, -357.649885, -357.63767999999993, -357.613915, -357.60578, -357.597235, -357.58931, -357.584545, -357.53371500000003, -357.523275, -357.521875, -357.51544, -357.509025, -357.50550499999997, -357.50205, -357.48968, -357.477415, -357.47517999999997, -357.46925, -357.46178499999996, -357.44698999999997, -357.42492, -357.412115, -357.40829499999995, -357.40217499999994, -357.389745, -357.37153, -357.33236999999997, -357.32925, -357.284175, -357.270745, -357.248545, -357.226905, -357.22140499999995, -357.197595, -357.15869, -357.142335, -357.13295, -357.114785, -357.10772, -357.09004500000003, -357.08502999999996, -357.07755, -357.04531, -357.01508, -356.99105, -356.978865, -356.9719, -356.95285, -356.94863499999997, -356.937055, -356.90702, -356.849945, -356.822445, -356.79121499999997, -356.76591999999994, -356.76444, -356.76252, -356.751935, -356.736815, -356.715635, -356.68402000000003, -356.676065, -356.66077, -356.637335, -356.61122, -356.60429999999997, -356.597085, -356.59290999999996, -356.570285, -356.51039499999996, -356.48312999999996, -356.478795, -356.47656, -356.46837500000004, -356.455645, -356.428615, -356.42163999999997, -356.37041999999997, -356.364865, -356.35774, -356.34826499999997, -356.337495, -356.32205, -356.29859, -356.286525, -356.27612, -356.25681, -356.243125, -356.23706, -356.22348, -356.19160999999997, -356.17997, -356.14392, -356.141175, -356.12721, -356.10521500000004, -356.09436500000004, -356.08184, -356.065775, -356.04895, -356.040765, -355.98975499999995, -355.965, -355.950645, -355.9152, -355.90889000000004, -355.90352, -355.898945, -355.8926, -355.88472, -355.880085, -355.876565, -355.87163, -355.864355, -355.85945, -355.85509, -355.83799, -355.82129499999996, -355.811995, -355.7693, -355.75222499999995, -355.74429, -355.7142, -355.684625, -355.671135, -355.648115, -355.64214000000004, -355.622255, -355.61717999999996, -355.613625, -355.61161, -355.60290499999996, -355.572855, -355.55726500000003, -355.54981000000004, -355.54169, -355.521535, -355.50131, -355.4947, -355.47413, -355.44408, -355.43026499999996, -355.423925, -355.41249500000004, -355.34163, -355.32693500000005, -355.319125, -355.2366, -355.21806, -355.212175, -355.20784000000003, -355.199035, -355.189935, -355.1843, -355.17041, -355.14793, -355.13324, -355.12739, -355.119975, -355.11087499999996, -355.09288, -355.05946, -355.03523, -355.00505, -354.982885, -354.96514, -354.90527499999996, -354.892615, -354.85102, -354.82209, -354.80180499999994, -354.78297999999995, -354.76237, -354.754715, -354.738825, -354.676065, -354.61983499999997, -354.616275, -354.61102999999997, -354.60616999999996, -354.57210999999995, -354.56178, -354.54769, -354.53495999999996, -354.528345, -354.520805, -354.497955, -354.47961, -354.47218, -354.47141, -354.461675, -354.44899, -354.32365, -354.317795, -354.31372, -354.311725, -354.30938000000003, -354.30368, -354.29208, -354.27005, -354.21863999999994, -354.20126999999997, -354.18261999999993, -354.12405, -354.08143499999994, -354.05435, -354.04440999999997, -354.036855, -354.031375, -354.0152, -353.96205, -353.947325, -353.92948, -353.92242, -353.90195, -353.894725, -353.866885, -353.73607, -353.69840999999997, -353.68224999999995, -353.65805, -353.56779, -353.52405999999996, -353.44904499999996, -353.400395, -353.247115, -353.23499000000004, -353.222955, -353.19636, -353.16922, -353.12503, -353.102215, -353.080225, -353.066055, -353.05795, -353.04452499999996, -353.02029, -353.01574999999997, -353.01205, -353.000765, -352.986585, -352.95609, -352.92936499999996, -352.91446999999994, -352.89932999999996, -352.88829, -352.88262499999996, -352.857685, -352.84416999999996, -352.75211, -352.722895, -352.7068, -352.70340999999996, -352.69995, -352.66487499999994, -352.64555999999993, -352.5854, -352.56088, -352.54033, -352.501215, -352.465595, -352.447245, -352.408725, -352.37683, -352.347425, -352.33656499999995, -352.316285, -352.220455, -352.14084, -352.12421, -352.117125, -352.068075, -352.03371, -351.983035, -351.915015, -351.81715999999994, -351.811325, -351.80278999999996, -351.77973499999996, -351.725095, -351.69277999999997, -351.68462, -351.67669, -351.61357499999997, -351.5855, -351.515705, -351.49807, -351.486855, -351.46399499999995, -351.45741999999996, -351.440075, -351.334925, -351.2916349999999, -351.28659, -351.261355, -351.12986, -351.119615, -351.08717, -351.055065, -351.036285, -351.02765, -351.01944499999996, -350.92318, -350.87081, -350.86171, -350.81521999999995, -350.77680999999995, -350.62552, -350.580375, -350.562835, -350.470055, -350.42088, -350.41614500000003, -350.369965, -350.33168, -350.302805, -350.298805, -350.2953, -350.28023499999995, -350.23936, -350.19759, -350.179315, -350.10580500000003, -350.073875, -350.0458, -350.01991, -349.98915, -349.92672000000005, -349.89694499999996, -349.890255, -349.888535, -349.877935, -349.864455, -349.78959499999996, -349.777475, -349.77026, -349.753515, -349.729335, -349.69573, -349.672145, -349.51991499999997, -349.480775, -349.464385, -349.411775, -349.40412000000003, -349.38295000000005, -349.333275, -349.326135, -349.28171, -349.26785, -349.20089500000006, -349.13423, -349.09184, -349.05935, -348.969685, -348.963075, -348.954055, -348.931555, -348.90221999999994, -348.83464499999997, -348.81206, -348.75431, -348.69039999999995, -348.64332, -348.625535, -348.61544, -348.602015, -348.51505, -348.482205, -348.41026, -348.34826, -348.28678, -348.24271999999996, -348.23913, -348.23263, -348.17731, -348.120795, -348.07517499999994, -348.02898500000003, -348.007765, -347.97992, -347.96445, -347.9307, -347.873125, -347.85524, -347.83477999999997, -347.82689, -347.82529999999997, -347.805655, -347.714835, -347.699235, -347.69204, -347.671915, -347.647585, -347.628935, -347.620635, -347.60328, -347.5037, -347.480455, -347.43014500000004, -347.37938999999994, -347.35668, -347.302005, -347.26914, -347.246315, -347.18165, -347.14757499999996, -347.136975, -347.13115, -347.117915, -346.952495, -346.9212, -346.885015, -346.84382500000004, -346.81480999999997, -346.71665499999995, -346.554465, -346.50975, -346.46511, -346.43089999999995, -346.29226499999993, -346.22198000000003, -346.180065, -346.156775, -346.12986, -346.12509, -346.09987, -346.06198499999994, -346.038535, -346.02866, -345.995805, -345.93339000000003, -345.91281000000004, -345.90093, -345.86429499999997, -345.61646999999994, -345.54682, -345.53494, -345.06016999999997, -345.046625, -345.01806, -344.9939, -344.990365, -344.950095, -344.90342999999996, -344.891485, -344.83564, -344.720145, -344.59492, -344.55669, -344.54485999999997, -344.4486, -344.35438999999997, -344.334615, -344.32308, -344.313175, -344.285505, -344.259675, -344.230385, -344.16108499999996, -344.11014, -344.04761, -344.02349, -343.931335, -343.853005, -343.80052, -343.74114, -343.696675, -343.65314, -343.59291499999995, -343.49506499999995, -343.46553, -343.413485, -343.22891500000003, -343.05323999999996, -342.854205, -342.78039, -342.754815, -342.73141499999997, -342.689505, -342.43006, -342.31701499999997, -342.299885, -342.19714, -342.04228, -341.99972, -341.97285, -341.96307, -341.95318499999996, -341.947715, -341.88125, -341.744335, -341.68573000000004, -341.62976000000003, -341.621235, -341.574035, -341.47251500000004, -341.376215, -341.337625, -341.32668, -341.306795, -341.2772, -341.23994999999996, -341.167945, -341.091455, -340.94722, -340.72265, -340.51211, -340.27838999999994, -340.08417, -340.03403, -340.01343499999996, -339.97176, -339.83803, -339.72405000000003, -339.70739000000003, -339.49505, -339.41191999999995, -339.36379999999997, -339.35571500000003, -339.31039499999997, -339.08015, -339.06456000000003, -338.981805, -338.94194500000003, -338.814115, -338.546865, -338.376035, -338.315295, -338.26078, -338.23861999999997, -338.16905499999996, -337.905095, -337.85358499999995, -337.74983999999995, -337.707175, -337.68181000000004, -337.59975, -337.46270000000004, -337.401785, -337.33649, -337.26731, -337.23605499999996, -337.16713000000004, -336.997125, -336.85774999999995, -336.43266, -336.29526, -336.03780000000006, -335.88503000000003, -335.74660500000005, -335.65446, -335.547085, -335.275755, -334.841815, -334.76774, -334.71651499999996, -334.49956000000003, -334.32359999999994, -334.309465, -333.91819499999997, -333.67671, -333.55850499999997, -333.44041500000003, -333.355135, -333.09475, -332.94246, -332.81534999999997, -332.676425, -332.56724999999994, -332.30560999999994, -332.18296, -331.947675, -331.876195, -331.777965, -331.5966, -331.412005, -331.18195000000003, -331.13718, -330.93512499999997, -330.894715, -330.760085, -330.681015, -330.15242, -330.06710499999997, -330.032235, -329.34535999999997, -329.317585, -329.295845, -329.18313, -329.00568, -328.9623, -328.885645, -328.79842, -328.711745, -328.4251, -328.129775, -327.74848, -327.69292499999995, -327.63426, -327.564625, -327.48021000000006, -326.699015, -326.37293, -326.30257500000005, -326.25494000000003, -326.07253000000003, -325.07358, -324.995995, -324.92222, -324.859875, -324.81661499999996, -324.75253, -324.36789999999996, -324.14957499999997, -324.094425, -324.081325, -323.215695, -322.68045500000005, -321.762645, -321.00042499999995, -320.82438, -320.63192499999997, -320.3232, -320.03310999999997, -319.89133499999997, -319.88358, -319.81286, -318.97956, -318.589155, -315.8502, -315.51483499999995, -315.43363, -315.256405, -314.87412, -314.577745, -314.467115, -314.406705, -314.404225, -314.381245, -313.55146, -312.92021, -312.788775, -312.44463, -312.18392500000004, -311.560625, -310.80401, -310.330085, -309.35304499999995, -308.81733499999996, -308.239035, -307.769715, -306.90778, -306.09898, -304.102785, -303.731575, -302.610285, -301.968215, -301.16004, -300.921305, -300.57070999999996, -300.12989, -299.645835, -298.78470000000004, -297.82371, -296.89414, -296.33447, -295.43652, -294.09466999999995, -291.737915, -289.163735, -287.26757, -286.382065, -285.51104999999995, -284.906165, -284.43186000000003, -284.14117999999996, -283.949515, -283.539995, -280.72789, -279.854795, -277.43406, -275.296, -274.661205, -273.58746499999995, -273.461505, -273.199425, -272.983005, -272.88847, -272.702675, -272.34217, -271.22826499999996, -270.79035999999996, -270.61883, -270.535265, -268.46306500000003, -268.16090499999996, -267.75982, -266.729825, -266.510125, -266.317505, -266.172885, -266.02086, -265.92515000000003, -265.88174000000004, -265.77571500000005, -265.25755000000004, -264.56307499999997, -263.90303, -263.226325, -262.77405, -262.343475, -262.18113, -261.64258500000005, -261.49117, -261.375435, -261.24027, -261.112835, -260.98354, -260.872965, -260.81576499999994, -260.48762, -260.331635, -260.22124, -260.12441, -260.0761, -260.02528499999994, -259.85850500000004, -259.75171, -259.50122999999996, -259.48754499999995, -259.18659, -259.072375, -257.79579, -257.564355, -257.418635, -257.09438, -256.83209, -256.75083, -256.46355500000004, -256.2582, -256.079285, -256.044535, -255.42737499999998, -255.091675, -254.98769, -254.89673999999997, -254.869735, -254.710465, -254.67336999999998, -254.635775, -254.63081, -254.6046, -254.445955, -254.297925, -254.263625, -253.878635, -253.815455, -253.761275, -253.650105, -253.561825, -253.48289, -252.960445, -252.762345, -252.61958, -252.57919, -252.50377, -252.451635, -252.30553, -252.23859, -252.19118, -252.173405, -252.09231499999999, -251.8222, -251.813915, -251.74311999999998, -251.65421500000002, -251.546355, -251.491335, -251.361465, -251.34623, -251.325765, -251.24737500000003, -251.16346, -251.099445, -251.07354500000002, -251.02993000000004, -250.93584, -250.85613999999998, -250.67537499999997, -250.654085, -250.42494499999998, -250.37125, -250.32988999999998, -250.26073000000002, -250.13831, -250.04236500000002, -249.918995, -249.765485, -249.68437500000002, -249.4283, -249.37634, -249.30034, -249.13319, -249.06853, -248.987175, -248.96186, -248.95120500000002, -248.8042, -248.76442000000003, -248.48864499999996, -248.36196999999999, -248.01313, -247.87119, -247.65267, -247.568335, -247.376745, -247.306375, -247.297055, -247.05667, -246.93833, -246.85843, -246.77297500000003, -246.72810000000004, -246.37329499999998, -246.32879, -246.286315, -246.27669500000002, -246.249505, -246.19763, -246.16814499999998, -246.108365, -246.05267, -246.017585, -245.91657500000002, -245.82851, -245.81219000000002, -245.800025, -245.78805999999997, -245.733215, -245.68465, -245.655825, -245.529335, -245.52387, -245.5216, -245.474695, -245.42238500000002, -245.107945, -245.01419, -244.95922, -244.94189, -244.910355, -244.88755000000003, -244.78378499999997, -244.71753999999999, -244.474135, -244.38099999999997, -244.14416500000002, -244.04393, -243.97588, -243.88645, -243.79262, -243.703855, -243.43312, -243.395275, -243.32042500000003, -243.150435, -243.10045499999998, -243.05900499999998, -243.019805, -242.961345, -242.90996, -242.77666, -242.70781, -242.63782, -242.454075, -242.309625, -242.173565, -242.15838499999998, -242.069365, -241.96508999999998, -241.79703999999998, -241.691105, -241.52291499999998, -241.50845, -241.41952499999996, -241.35498, -241.24559499999998, -241.20755, -240.72836999999998, -240.640515, -240.623105, -240.59446499999999, -240.55461499999998, -240.53229499999998, -240.51450999999997, -240.50662999999997, -240.28524, -240.18795, -240.0512, -239.94376499999998, -239.511595, -239.462735, -239.42895, -239.425675, -239.28637500000002, -239.08624999999998, -238.94681, -238.913755, -238.893515, -238.83744000000002, -238.719685, -238.65366999999998, -238.60028, -238.56286, -238.44814499999998, -238.420795, -238.40108, -238.392225, -238.327535, -238.29188, -238.23388, -238.11518, -238.013465, -237.86502, -237.82650999999998, -237.637945, -237.52687, -237.36494499999998, -237.255945, -237.24174500000004, -237.19995500000002, -237.159055, -237.15517499999999, -237.070605, -236.995435, -236.97220000000002, -236.74971, -236.70161000000002, -236.696585, -236.46605, -236.41228, -236.39765999999997, -236.38360999999998, -236.32896, -236.29998, -236.18987500000003, -236.15648, -236.070285, -236.00047999999998, -235.981655, -235.874775, -235.70361499999999, -235.633505, -235.562695, -235.51058999999998, -235.47766000000001, -235.44344, -235.42930500000003, -235.42302500000002, -235.38803000000001, -235.309025, -235.21132, -234.70719, -234.66135500000001, -234.57572000000002, -234.49843, -234.45008, -234.34579, -234.20236, -234.15870999999999, -234.08715, -233.95839, -233.9497, -233.94146, -233.9301, -233.68842, -233.62626, -233.50753, -233.40869999999998, -233.34229, -233.25689, -233.19091, -233.13213000000002, -233.10541, -233.09573, -233.087975, -233.02187, -232.94689499999998, -232.914675, -232.88556499999999, -232.86595499999999, -232.819795, -232.77696, -232.74672500000003, -232.68307, -232.62583, -232.55330500000002, -232.48992, -232.44971500000003, -232.38495, -232.35769, -232.32421499999998, -232.261035, -232.15450999999996, -232.09073, -232.05163000000002, -231.985385, -231.92315000000002, -231.79595000000003, -231.44707, -231.41698000000002, -231.36042, -231.31142999999997, -231.05097999999998, -230.95518499999997, -230.85862500000002, -230.846525, -230.837025, -230.83070500000002, -230.809665, -230.57479999999998, -230.53617, -230.453805, -230.33326499999998, -230.220005, -230.168815, -230.13992000000002, -230.12974499999999, -230.04525999999998, -229.875305, -229.70938, -229.66973, -229.65677499999998, -229.639995, -229.6231, -229.571795, -229.47932000000003, -229.45699000000002, -229.3997, -229.277075, -229.269075, -229.23201999999998, -229.193345, -229.11253499999998, -229.07968, -229.046095, -229.00074, -228.96792500000004, -228.90418499999998, -228.83373, -228.63601, -228.51732, -228.427625, -228.35295, -228.230115, -228.07181000000003, -227.94394499999999, -227.854445, -227.84457500000002, -227.832655, -227.68759, -227.64432, -227.639645, -227.40453, -227.40063, -227.37668000000002, -227.353735, -227.26064, -227.16540500000002, -227.06552000000002, -226.975755, -226.921325, -226.76803, -226.72306000000003, -226.644105, -226.503395, -226.47411999999997, -226.33769999999998, -226.287125, -226.22421500000002, -226.07897499999999, -225.994415, -225.90399000000002, -225.88318, -225.78668, -225.63335, -225.44141, -225.389185, -225.316645, -225.27828499999998, -225.26510000000002, -225.249525, -225.20226, -225.156235, -225.13061499999998, -225.006735, -224.93778, -224.69644, -224.66128500000002, -224.65770500000002, -224.61511000000002, -224.438305, -224.436735, -224.410325, -224.15608, -224.141225, -224.13216, -224.002065, -223.96956, -223.890065, -223.87318500000003, -223.84859, -223.706905, -223.65687499999999, -223.59974, -223.55104500000002, -223.46239, -223.30552500000002, -223.23040500000002, -223.18412, -223.16897, -223.127925, -223.08727, -223.04979000000003, -222.82108499999998, -222.792385, -222.66618, -222.658295, -222.63664, -222.613645, -222.54416, -222.483655, -222.42793, -222.34278999999998, -222.236365, -222.16931999999997, -221.95761500000003, -221.89405, -221.81733, -221.701955, -221.658455, -221.60817500000002, -221.572005, -221.542075, -221.42070999999999, -221.273795, -221.21575, -221.171025, -220.93029, -220.63687, -220.51957000000002, -220.47614000000002, -220.470425, -220.42717000000002, -220.388325, -220.37761500000002, -220.36209000000002, -220.19888, -220.10038000000003, -220.03496, -219.965355, -219.90839999999997, -219.895175, -219.88011999999998, -219.86064499999998, -219.835095, -219.819565, -219.80723999999998, -219.72715, -219.7041, -219.69286, -219.670265, -219.32637, -219.28289999999998, -219.27189999999996, -219.25469999999999, -219.10061000000002, -219.028525, -219.02059500000001, -218.91613, -218.825915, -218.776365, -218.52973, -218.429775, -218.36652499999997, -218.32515999999998, -218.27933000000002, -217.803295, -217.77307000000002, -217.67751, -217.646515, -217.51245, -217.46097999999998, -217.45664499999998, -217.43737000000002, -217.410725, -217.383965, -217.36069500000002, -217.33675, -217.248355, -217.209905, -216.96500500000002, -216.88269000000003, -216.865365, -216.847015, -216.81867499999998, -216.76527499999997, -216.69848, -216.60553000000002, -216.54242, -216.50453000000002, -216.48047000000003, -216.27153, -216.062695, -216.00594, -215.87188, -215.833045, -215.781725, -215.75411, -215.678985, -215.40475999999998, -215.367665, -215.33689999999999, -215.32234, -215.31481499999998, -215.23151000000001, -214.972915, -214.940945, -214.92001, -214.87491000000003, -214.820555, -214.71815000000004, -214.65939500000002, -214.655555, -214.540265, -214.50351, -214.267965, -214.182245, -213.933475, -213.73282, -213.60089, -213.51063, -213.498415, -213.40756, -213.27424, -213.11906, -213.0038, -212.94293, -212.941725, -212.92458, -212.79688, -212.661115, -212.631575, -212.60894000000002, -212.5428, -212.46773, -212.268505, -212.177075, -212.10156, -212.076325, -212.04488500000002, -211.978655, -211.64558499999998, -211.58074000000002, -211.28882, -211.262585, -211.254595, -211.19647, -211.12884, -211.02299, -210.91393499999998, -210.85473499999998, -210.78688, -210.74514499999998, -210.69113, -210.64591000000001, -210.555995, -210.490835, -210.39484, -210.35211500000003, -210.30851, -210.22381000000001, -210.126035, -210.02457500000003, -209.977745, -209.94562, -209.937555, -209.908705, -209.89810999999997, -209.88801999999998, -209.864045, -209.79034000000001, -209.709005, -209.67289, -209.65429, -209.540335, -209.44843500000002, -209.33526, -209.24665000000002, -209.18914, -209.15771, -209.06138499999997, -209.04075, -208.852055, -208.82142, -208.793405, -208.77683000000002, -208.755715, -208.72008000000002, -208.466975, -208.44207, -208.397855, -208.37300999999997, -208.23752000000002, -208.174675, -208.110455, -208.05841, -208.031385, -207.97950500000002, -207.94309, -207.81963000000002, -207.76812999999999, -207.71495499999997, -207.44218999999998, -207.40954, -207.341925, -207.219405, -207.20693, -207.194085, -207.106605, -207.08812, -207.051265, -206.894635, -206.73663, -206.58992999999998, -206.477615, -206.41364499999997, -206.34425499999998, -206.29007, -206.21805, -206.15535500000001, -206.12884, -206.10674999999998, -206.09542, -206.08326, -206.05338999999998, -206.02581, -205.98330500000003, -205.770305, -205.73559999999998, -205.72458, -205.699175, -205.63308, -205.59629999999999, -205.52905999999996, -205.48384499999997, -205.45987, -205.43572999999998, -205.32735000000002, -205.29978, -205.22717, -205.157955, -205.080735, -205.02607999999998, -204.98936499999996, -204.82718999999997, -204.641915, -204.63752, -204.430515, -204.40284000000003, -204.28415, -204.24294, -204.176045, -204.11133999999998, -204.05382, -203.78912499999998, -203.76114, -203.71721, -203.66911500000003, -203.61090500000003, -203.54494, -203.46695, -203.38611500000002, -203.25839, -203.20324, -203.195395, -203.183005, -203.167055, -203.15175, -203.09666499999997, -203.03105, -202.99462499999998, -202.98501, -202.87433, -202.84213499999998, -202.57198000000002, -202.441495, -202.405485, -202.10238999999999, -202.07748999999998, -202.04784, -202.00471, -201.97353, -201.97024, -201.87124999999997, -201.739895, -201.59357, -201.31497, -201.17512, -201.15972, -201.144935, -201.13332499999999, -201.069975, -201.048515, -200.89270499999998, -200.72954499999997, -200.71409, -200.69484, -200.618965, -200.57971999999998, -200.51219499999996, -200.44559999999998, -200.32466, -200.252665, -200.21759500000002, -200.15274, -200.07754, -199.95746000000003, -199.915545, -199.90265499999998, -199.83882, -199.78085499999997, -199.47728, -199.45802, -199.30304, -199.24956000000003, -199.21747, -199.132185, -199.060575, -199.03507000000002, -199.00333, -198.97245999999998, -198.9476, -198.927945, -198.91267, -198.89146, -198.859035, -198.72433999999998, -198.71854, -198.71152, -198.624995, -198.53524000000002, -198.445295, -198.40531, -198.35397, -198.31845, -198.17194999999998, -198.078395, -197.99856, -197.96708, -197.72628, -197.704435, -197.46233, -197.286245, -197.21210000000002, -197.07964, -197.03092, -196.89623, -196.638715, -196.636135, -196.53947499999998, -196.51774999999998, -196.47159499999998, -196.42145, -196.339215, -196.22007000000002, -196.154065, -196.10325, -196.05851, -196.02420999999998, -195.986875, -195.741265, -195.66203, -195.61158999999998, -195.561205, -195.47891500000003, -195.27166499999998, -195.22429499999998, -195.20664499999998, -195.16201999999998, -195.07698, -194.97529500000002, -194.73206500000003, -194.68527, -194.653855, -194.56154, -194.42767000000003, -194.352395, -194.23122, -194.047865, -193.80370499999998, -193.76998500000002, -193.695515, -193.57087, -193.521505, -193.42807, -193.332385, -193.207515, -193.194465, -193.176685, -193.15013499999998, -193.00558, -192.831185, -192.73812, -192.441275, -192.29558, -192.14861000000002, -192.11762499999998, -192.076245, -192.03006, -191.99736000000001, -191.936875, -191.86895499999997, -191.84339999999997, -191.83488499999999, -191.78906999999998, -191.706055, -191.576545, -191.37348000000003, -191.289645, -191.21912999999998, -191.171955, -191.150235, -191.084545, -190.96876499999996, -190.889715, -190.84449, -190.804515, -190.76712500000002, -190.7018, -190.680765, -190.66105499999998, -190.652375, -190.641455, -190.48477, -190.40682999999999, -190.34078, -190.302215, -190.23765999999998, -190.08991, -189.984315, -189.95727499999998, -189.92066499999999, -189.8496, -189.786595, -189.71764000000002, -189.652455, -189.64017, -189.635065, -189.360075, -189.340035, -189.334615, -189.306515, -189.27610499999997, -189.26603999999998, -189.100855, -189.03044, -188.94834, -188.87294, -188.79644, -188.7362, -188.72071, -188.66754, -188.592735, -188.553505, -188.52807, -188.48944, -188.42852, -188.35909499999997, -188.272505, -188.17516, -188.084, -187.746625, -187.64264500000002, -187.59519999999998, -187.49532, -187.44933, -187.386365, -187.30714999999998, -187.18809, -186.965675, -186.900535, -186.82053000000002, -186.7383, -186.70356, -186.67885, -186.550255, -186.38772999999998, -186.181465, -186.11883999999998, -186.00027, -185.892995, -185.79971, -185.786785, -185.64056, -185.59044999999998, -185.56631000000002, -185.55114500000002, -185.501175, -185.47116000000003, -185.44468, -185.32148999999998, -185.25426499999998, -185.160695, -184.972025, -184.67189, -184.503895, -184.20333499999998, -184.05575500000003, -183.94732000000002, -183.868895, -183.82854, -183.79843, -183.78983, -183.740095, -183.71943, -183.68733, -183.65681999999998, -183.62671, -183.53071999999997, -183.39446, -183.35403, -183.25448999999998, -183.1345, -182.88539, -182.81622, -182.79537, -182.57384, -182.445255, -182.332405, -182.25449, -182.0933, -181.86297000000002, -181.7493, -181.64521499999998, -181.62102499999997, -181.46722499999998, -181.35577999999998, -181.30752, -181.11702000000002, -180.98216, -180.68831, -180.595125, -180.54165999999998, -180.34406, -180.29620999999997, -179.94585999999998, -179.840565, -179.60394499999998, -179.43361499999997, -179.266035, -179.18794, -179.10287, -178.9355, -178.62825499999997, -178.567135, -178.48993000000002, -178.452655, -178.34126499999996, -178.18217499999997, -178.09505000000001, -177.939785, -177.911565, -177.881695, -177.872545, -177.83323000000001, -177.79467, -177.727955, -177.55491999999998, -176.88088000000002, -176.84106500000001, -176.772595, -176.692545, -176.52651000000003, -176.268305, -176.21056, -176.11928999999998, -175.92933499999998, -175.8107, -175.766035, -175.71361000000002, -175.692725, -175.67410999999998, -175.66565999999997, -175.607745, -175.48915, -175.39815, -175.393255, -175.37053, -175.275745, -175.25028, -175.205425, -175.083955, -175.07243000000003, -174.95343, -174.82936, -174.796295, -174.72857999999997, -174.64592, -174.349645, -174.26942000000003, -174.2148, -174.06774000000001, -174.001955, -173.94652, -173.905005, -173.87635, -173.68849, -173.53787, -173.44586, -173.343575, -173.28902499999998, -173.22797500000001, -173.20550000000003, -173.14624, -173.07144, -172.92139500000002, -172.82636000000002, -172.82442, -172.80677500000002, -172.741405, -172.615355, -172.48044499999997, -172.18237, -172.14252499999998, -172.12876, -172.10491, -172.071245, -172.05405000000002, -172.02035500000002, -171.958705, -171.89361, -171.82247, -171.73304000000002, -171.60758499999997, -171.60477500000002, -171.52237, -171.26716499999998, -171.154295, -170.94672000000003, -170.831765, -170.79713499999997, -170.717875, -170.60944, -170.55089, -170.44734499999998, -170.43192000000002, -170.17284, -170.10875499999997, -170.03094, -169.998495, -169.91582, -169.80808, -169.50964, -168.978455, -168.97593999999998, -168.95905, -168.88039500000002, -168.83128, -168.75778499999998, -168.65604000000002, -168.56887, -168.38331, -168.36538, -168.20010000000002, -168.15128500000003, -168.00228500000003, -167.95457, -167.841205, -167.76169000000002, -167.72076, -167.68772, -167.429235, -167.33076499999999, -167.323425, -167.233445, -167.10750000000002, -167.02948500000002, -166.9754, -166.914645, -166.66089499999998, -166.26871, -166.26184, -166.109215, -166.08233, -165.87282, -165.77128, -165.72499, -165.643235, -165.575335, -165.49745, -165.460855, -165.42296000000002, -165.35269000000002, -165.14319, -165.09063500000002, -165.00194500000003, -164.800125, -164.727585, -164.59515, -164.43706500000002, -164.35446000000002, -164.276225, -164.09952, -163.96518500000002, -163.72443, -163.4898, -163.410235, -163.375005, -163.347455, -163.300905, -163.233245, -163.14464500000003, -162.89259499999997, -162.03643, -161.92882, -161.77967999999998, -161.71227, -161.679605, -161.47929500000004, -161.08021, -160.97908999999999, -160.89815499999997, -160.87153999999998, -160.76364, -160.71914500000003, -160.642945, -160.467105, -160.36422, -160.06265, -159.94453499999997, -159.79386, -159.625955, -159.52605, -159.49206, -159.2988, -159.283115, -159.19464, -159.14776, -159.10295000000002, -159.01781, -158.977415, -158.92309, -158.493565, -158.46948, -158.251205, -158.0946, -157.93658, -157.905585, -157.7684, -157.691345, -157.588045, -157.41013499999997, -157.26709499999998, -157.15997499999997, -156.999635, -156.93540000000002, -156.91093, -156.76172499999998, -156.622235, -156.61092000000002, -156.425355, -156.327625, -156.119665, -155.992435, -155.892025, -155.82187499999998, -155.79366, -155.75998, -155.73416, -155.622425, -155.46515, -155.44241, -155.398555, -155.33438999999998, -154.440765, -154.371535, -154.338405, -154.204495, -154.03832, -153.88465, -153.65408, -153.54111999999998, -153.47553, -153.399925, -153.09416, -152.99399, -152.942795, -152.64666, -152.42583000000002, -152.410255, -152.394285, -152.208595, -152.01704, -151.76914499999998, -151.708575, -151.61854, -151.57920000000001, -151.29681999999997, -151.12103000000002, -151.07318, -151.035485, -151.00973, -150.87922500000002, -150.70854500000002, -150.38073, -150.34138, -150.28428, -150.15800000000002, -150.096695, -149.70312, -149.495945, -149.31482, -149.21862, -148.891935, -148.63172000000003, -148.57574000000002, -148.49792000000002, -148.41528000000002, -147.868715, -147.64063, -147.20035000000001, -147.114665, -147.02769, -146.94212, -146.866555, -146.597735, -146.51169, -146.43959, -146.368675, -146.14381, -146.04459500000002, -145.9691, -145.86768999999998, -145.74964, -145.62204, -145.54762499999998, -144.88586, -144.52570500000002, -142.990285, -142.916985, -142.74221, -142.66862, -142.4001, -141.995605, -141.71514, -141.52362999999997, -140.54649, -140.180145, -140.14526, -139.913115, -139.64645000000002, -139.362275, -139.10450500000002, -138.86387000000002, -138.09641, -137.800845, -137.719685, -137.67180000000002, -137.61605500000002, -137.480835, -137.040595, -136.618795, -136.29082, -135.764565, -135.731775, -134.89193, -134.57117, -134.20128499999998, -134.111115, -133.529095, -133.145755, -132.010765, -131.95472999999998, -131.547645, -131.371205, -131.117725, -130.97732000000002, -130.82567, -130.64593000000002, -130.32280500000002, -130.11271, -130.062555, -129.83753000000002, -129.367485, -129.0267, -128.899635, -128.184755, -127.74710999999999, -126.18482, -125.898785, -125.37081, -124.89488, -122.646175, -122.34410499999998, -120.62006, -119.460335, -118.53482, -118.44458499999999, -118.318955, -118.27380000000001, -117.13155, -116.88932000000001, -116.416775, -116.127495, -115.38253, -114.93213500000002, -107.478125, -107.339815, -106.85273000000001, -106.377805, -105.465305, -104.74905000000001, -103.57203, -103.38385, -103.18635, -103.073755, -102.68016999999999, -102.51877499999999, -92.54757, -91.65659, -90.91637, -88.158085, -77.942445, -77.00572500000001])
labels = array([3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 4.0, 3.0, 5.0, 3.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 5.0, 4.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 5.0, 4.0, 3.0, 5.0, 4.0, 5.0, 3.0, 5.0, 4.0, 5.0, 3.0, 4.0, 5.0, 3.0, 4.0, 1.0, 3.0, 4.0, 5.0, 3.0, 1.0, 3.0, 1.0, 5.0, 1.0, 4.0, 1.0, 5.0, 1.0, 5.0, 1.0, 0.0, 1.0, 5.0, 4.0, 0.0, 1.0, 5.0, 1.0, 5.0, 1.0, 5.0, 1.0, 5.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 5.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 5.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 5.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 5.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0])
def eqenergy(rows):
    try:
        return np.sum(rows, axis=1, dtype=np.float128)
    except:
        return np.sum(rows, axis=1, dtype=np.longdouble)
def classify(rows):
    energys = eqenergy(rows)

    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<=len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<=len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = labels[numers[indys]]
        if list(defaultindys):
            outputs[defaultindys] = 5
        return outputs
    return thresh_search(energys)

numthresholds = 3187

# Main method
model_cap = numthresholds


def Validate(file):
    #Load Array
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs, cleanarr[:, -1]


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping, trim=False):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    cleanarr = cleanarr.reshape(cleanarr.shape[0], -1)
    if not trim and ignorecolumns != []:
        cleanarr = cleanarr[:, important_idxs]
    with open(preprocessedfile, 'r', encoding='utf-8') as csvinput:
        dirtyreader = csv.reader(csvinput)
        if not headerless:
            header = next(dirtyreader, None)
        #print original header
        if (not headerless):
            print(','.join(header + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            if k >= outputs.shape[0]:
                continue
            print(str(','.join(str(j) for j in (['"' + i + '"' if ',' in i else i for i in row]))) + str(',' if len(important_idxs) != 1 else '') + str(get_key(int(outputs[k]), classmapping)))
                




#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", default=False, help="Output predictor file with only the selected important attributes (to be used with predictors built with -rank)")
    args = parser.parse_args()
    faulthandler.enable()


    if args.validate:
        args.trim = True


    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        output = preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate), trim=args.trim)
        get_key, classmapping = clean(preprocessedfile if output!=-1 else args.csvfile, cleanfile, -1, args.headerless, (not args.validate), trim=args.trim)
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}
        output = None

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile if output!=-1 else args.csvfile, classmapping, trim=args.trim)


    else:
        classifier_type = 'DT'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds, true_labels = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            classbalance = [float(num_class_0)/count, float(num_class_1)/count]
            H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))

            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            json_dict = {'instance_count':                        count ,
                         'classifier_type':                        classifier_type,
                         'classes':                            2 ,
                         'false_negative_instances':    num_FN ,
                         'false_positive_instances':    num_FP ,
                         'true_positive_instances':    num_TP ,
                         'true_negative_instances':    num_TN,
                         'false_negatives':                        FN ,
                         'false_positives':                        FP ,
                         'true_negatives':                        TN ,
                         'true_positives':                        TP ,
                         'number_correct':                        num_correct ,
                         'accuracy': {
                             'best_guess': randguess,
                             'improvement': modelacc-randguess,
                             'model_accuracy': modelacc,
                         },
                         'model_capacity':                        model_cap ,
                         'generalization_ratio':                int(float(num_correct * 100) / model_cap) * H/ 100.0,
                         'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0,
                        'shannon_entropy_of_labels':           H,
                        'classbalance':                        classbalance}
            if classifier_type == 'NN':
                json_dict['capacity_utilized_by_nn'] = cap_utilized # noqa
            if args.json:
                pass
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                elif classifier_type == 'RF':
                    print("Classifier Type:                    Random Forest")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                if classifier_type == 'NN':
                    print("Model Capacity Utilized:            {:.0f} bits".format(cap_utilized)) # noqa
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0 * H) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            classbalance = [float(numofcertainclass) / count for numofcertainclass in numeachclass.values()]
            H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))

            if args.json:
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'classes':                            n_classes,
                             'number_correct': num_correct,
                             'accuracy': {
                                 'best_guess': randguess,
                                 'improvement': modelacc - randguess,
                                 'model_accuracy': modelacc,
                             },
                             'model_capacity': model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0 * H,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0,
                        'shannon_entropy_of_labels':           H,
                        'classbalance':                        classbalance}
                if classifier_type == 'NN':
                    json_dict['capacity_utilized_by_nn'] = cap_utilized # noqa
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                elif classifier_type == 'RF':
                    print("Classifier Type:                    Random Forest")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                if classifier_type == 'NN':
                    print("Model Capacity Utilized:            {:.0f} bits".format(cap_utilized)) # noqa              
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0 * H) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):
            stats = {}
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")
            n_labels = labels.size

            for class_i in range(n_labels):
                stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}
                class_i_indices = np.argwhere(y_true==class_i) #indices with bus(call class_i=bus in this example)
                not_class_i_indices = np.argwhere(y_true!=class_i) #indices with not bus
                stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices] == class_i)) #indices where bus, and we predict == bus
                stats[int(class_i)]['FN'] = int(np.sum(y_pred[class_i_indices] != class_i)) #indices where bus, and we predict != bus
                stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices] != class_i)) #indices with not bus, where we predict != bus
                stats[int(class_i)]['FP'] = int(np.sum(y_pred[not_class_i_indices] == class_i)) #indices where not bus, we predict as bus
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                if not json:
                    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                    sys.exit()
                else:
                    return np.array([]), stats
                

            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------



            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]

            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm, stats
        mtrx, stats = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            json_dict['multiclass_stats'] = stats
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        if output!=-1:
            os.remove(preprocessedfile)
