{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Using Daimensions\n",
    "\n",
    "This notebook uses data from the Titanic competition on Kaggle (https://www.kaggle.com/c/titanic/overview).\n",
    "\n",
    "Kaggle's description of the competition:\n",
    "\"The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered 'unsinkable' RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: 'what sorts of people were more likely to survive?' using passenger data (ie name, age, gender, socio-economic class, etc).\"\n",
    "\n",
    "Goal: Make a predictor of survival from Titanic training data. We'll do this by using Daimensions to measure, build, and validate a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Measurements\n",
    "\n",
    "Measuring our data before building a predictor is important in order to avoid mistakes and optimize our model. If we don't measure our data, we have no way of knowing whether the predictor we build will actually do what we want it to do when it sees new data that it wasn’t trained on. We'll probably build a model that is much larger than it needs to be, meaning our training and run times will probably be much longer than they need to be. We could end up in a situation where we just don’t know whether we have the right amount or right type of training data, even after extensive training and testing. Because of these reasons, it's best to measure our data beforehand. Not to mention, Daimensions will tell us about learnability, the generalization ratio, noise resilience, and all the standard accuracy and confusion figures. \n",
    "For more information, you can read the Daimensions How-to Guide and Glossary.\n",
    "\n",
    "Below is a clip of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\r",
      "\r\n",
      "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\r",
      "\r\n",
      "2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C\r",
      "\r\n",
      "3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S\r",
      "\r\n",
      "4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S\r",
      "\r\n",
      "5,0,3,\"Allen, Mr. William Henry\",male,35,0,0,373450,8.05,,S\r",
      "\r\n",
      "6,0,3,\"Moran, Mr. James\",male,,0,0,330877,8.4583,,Q\r",
      "\r\n",
      "7,0,1,\"McCarthy, Mr. Timothy J\",male,54,0,0,17463,51.8625,E46,S\r",
      "\r\n",
      "8,0,3,\"Palsson, Master. Gosta Leonard\",male,2,3,1,349909,21.075,,S\r",
      "\r\n",
      "9,1,3,\"Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\",female,27,0,2,347742,11.1333,,S\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the target column (Survived) isn't the last column on the right. Because of this, we need to use '-target' so that Daimensions is looking at the correct target column for measuring and building a predictor.\n",
    "\n",
    "*(The clip of the training data may not show up. I plan on replacing the external file with a url, but I'm not sure how to do that yet. This part is to show that the target column isn't at the end, so we need to use '-target'.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainome Daimensions(tm) 0.97 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Ariana Park\n",
      "Expiration date: 2020-11-30 (138 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 4GB\n",
      "Connected to: https://beta.brainome.ai:8080\n",
      "\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 11\n",
      "Number of classes: 2\n",
      "Class balance: 61.5% 38.38%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.50%\n",
      "Capacity progression (# of decision points): [7, 8, 9, 9, 10, 10]\n",
      "Decision Tree: 422 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 118 parameters\n",
      "\n",
      "Risk that model needs to overfit for high accuracies...\n",
      "using Decision Tree: 94.73%\n",
      "using Neural Networks: 100.00%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 2.11 bits/bit\n",
      "using a Neural Network: 7.55 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Note: Maybe enough data to generalize. [yellow]\n",
      "Warning: Cannot find numpy. The output predictor may not run on this machine.\n",
      "\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 1s\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 1m 26s\n",
      "\n",
      "Time estimate for Decision Tree:\n",
      "Estimated time to prime a decision tree: a few seconds\n"
     ]
    }
   ],
   "source": [
    "! ./btc_linux -measureonly train.csv -target Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the predictor\n",
    "\n",
    "Because the learnability of the data (based on capacity progression and risk) is yellow, the how-to guide recommends to choose predictor with higher generalization and increase effort for best results. This means using a neural network with effort should work best. Here, I'm using '-f NN' to make the predictor a neural network. I'm also using '-o predict.py' to output the predictor as a python file. To increase the effort, I'm using '-e 10' for 10 times the effort. Again, we have to use '-target Survived' because the target column isn't the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainome Daimensions(tm) 0.97 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Ariana Park\n",
      "Expiration date: 2020-11-30 (133 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 4GB\n",
      "Connected to: https://beta.brainome.ai:8080\n",
      "\n",
      "Running btc will overwrite existing predict.py. OK? [y/N] yes\n",
      "Input: train.csv\n",
      "Sampling...done.\n",
      "Preprocessing...done.\n",
      "Cleaning...done.\n",
      "Splitting into training and validation...done.\n",
      "Pre-training measurements...done.\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 11\n",
      "Number of classes: 2\n",
      "Class balance: 61.5% 38.5%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.50%\n",
      "Capacity progression (# of decision points): [8, 9, 10, 10, 11, 11]\n",
      "Decision Tree: 422 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 118 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy...\n",
      "using Decision Tree: 94.73%\n",
      "using Neural Networks: 100.00%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 2.11 bits/bit\n",
      "using a Neural Network: 7.55 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Note: Maybe enough data to generalize. [yellow]\n",
      "Warning: Cannot find numpy. The output predictor may not run on this machine.\n",
      "\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 1s\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 3m 15s\n",
      "Note: Machine learner type NN given by user.\n",
      "Architecting model...done.\n",
      "Model capacity (MEC):     27 bits\n",
      "Architecture efficiency:   1.0 bits/parameter\n",
      "\n",
      "Estimating time to prime model...done.\n",
      "Estimated time to prime model: 0d 0h 2m 22s\n",
      "\n",
      "Priming model...done.\n",
      "Estimating training time...done.\n",
      "Estimated training time: 0d 0h 15m 48s\n",
      "\n",
      "Training...done.\n",
      "Model created:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=11, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Compiling predictor...done.\n",
      "Validating predictor...done.\n",
      "Classifier Type:                    Neural Network\n",
      "System Type:                        Binary classifier\n",
      "Best-guess accuracy:                61.61%\n",
      "Model accuracy:                     81.59% (727/891 correct)\n",
      "Improvement over best guess:        19.98% (of possible 38.39%)\n",
      "Model capacity (MEC):               27 bits\n",
      "Generalization ratio:               26.92 bits/bit\n",
      "Model efficiency:                   0.74%/parameter\n",
      "System behavior\n",
      "True Negatives:                     54.32% (484/891)\n",
      "True Positives:                     27.27% (243/891)\n",
      "False Negatives:                    11.11% (99/891)\n",
      "False Positives:                    7.30% (65/891)\n",
      "True Pos. Rate/Sensitivity/Recall:  0.71\n",
      "True Neg. Rate/Specificity:         0.88\n",
      "Precision:                          0.79\n",
      "F-1 Measure:                        0.75\n",
      "False Negative Rate/Miss Rate:      0.29\n",
      "Critical Success Index:             0.60\n",
      "Overfitting:                        No\n",
      "\n",
      "Output: predict.py \n",
      "READY.\n"
     ]
    }
   ],
   "source": [
    "! ./btc_linux -v -v -f NN train.csv -o predict.py -target Survived -e 10 --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate and Make Predictions\n",
    "\n",
    "We've built our first predictor! Now it's time to put it to use. If you have validation data, or data that has the target column but wasn't used for training, you can use it to validate the accuracy of your predictor. In the case of Titanic, we have test data instead, where it's different from the training data and doesn't include 'Survival'. We can use the model we built to make predictions for the test data and submit it to Kaggle. If you want to validate this data, you could split the training data into two files, using one to train and one to validate.\n",
    "In the following code, I'll save the model's prediction in 'prediction.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to validate\n",
    "# ! python3 predict.py -validate validation_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P,a,s,s,e,n,g,e,r,I,d,\",\",P,c,l,a,s,s,\",\",N,a,m,e,\",\",S,e,x,\",\",A,g,e,\",\",S,i,b,S,p,\",\",P,a,r,c,h,\",\",T,i,c,k,e,t,\",\",F,a,r,e,\",\",C,a,b,i,n,\",\",E,m,b,a,r,k,e,d,\",\",P,r,e,d,i,c,t,i,o,n\r\n",
      "892,3,\"Kelly, Mr. James\",male,34.5,0,0,330911,7.8292,,Q,0\r\n",
      "893,3,\"Wilkes, Mrs. James (Ellen Needs)\",female,47,1,0,363272,7,,S,0\r\n",
      "894,2,\"Myles, Mr. Thomas Francis\",male,62,0,0,240276,9.6875,,Q,0\r\n",
      "895,3,\"Wirz, Mr. Albert\",male,27,0,0,315154,8.6625,,S,0\r\n",
      "896,3,\"Hirvonen, Mrs. Alexander (Helga E Lindqvist)\",female,22,1,1,3101298,12.2875,,S,0\r\n",
      "897,3,\"Svensson, Mr. Johan Cervin\",male,14,0,0,7538,9.225,,S,0\r\n",
      "898,3,\"Connolly, Miss. Kate\",female,30,0,0,330972,7.6292,,Q,1\r\n",
      "899,2,\"Caldwell, Mr. Albert Francis\",male,26,1,1,248738,29,,S,0\r\n",
      "900,3,\"Abrahim, Mrs. Joseph (Sophie Halaut Easu)\",female,18,0,0,2657,7.2292,,C,1\r\n"
     ]
    }
   ],
   "source": [
    "! python3 predict.py test.csv > prediction.csv\n",
    "! head prediction.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the prediction file appended the model's prediction of survival as the last column. \n",
    "When the prediction is submitted to Kaggle, it has 74.641% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving Our Model\n",
    "\n",
    "Our model did pretty well, but let's see if we can improve it. A column that contains a unique value in each row (for example a database key) will never contribute to generalization, so we shouldn't include database keys or other unique ID columns. We can remove these columns by using '-ignorecolumns'. We'll try ignoring columns: PassengerId, Name, Ticket, Cabin, Embarked, because they're all unique ID columns. We could also use '-rank' to rank columns by significance and only process contributing attributes.\n",
    "To check out some of the other control options, use '-h' to see the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ./btc_linux -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainome Daimensions(tm) 0.97 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Ariana Park\n",
      "Expiration date: 2020-11-30 (138 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 4GB\n",
      "Connected to: https://beta.brainome.ai:8080\n",
      "\n",
      "Input: train.csv\n",
      "Sampling...done.\n",
      "Preprocessing...done.\n",
      "Cleaning...done.\n",
      "Splitting into training and validation...done.\n",
      "Pre-training measurements...done.\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 6\n",
      "Number of classes: 2\n",
      "Class balance: 61.62% 38.27%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.62%\n",
      "Capacity progression (# of decision points): [6, 8, 9, 10, 10, 12]\n",
      "Decision Tree: 285 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 73 parameters\n",
      "\n",
      "Risk that model needs to overfit for high accuracies...\n",
      "using Decision Tree: 63.97%\n",
      "using Neural Networks: 100.00%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 3.13 bits/bit\n",
      "using a Neural Network: 12.21 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Warning: Not enough data to generalize. [red]\n",
      "Warning: Cannot find numpy. The output predictor may not run on this machine.\n",
      "\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 0s\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 1m 21s\n",
      "Note: Machine learner type NN given by user.\n",
      "Architecting model...done.\n",
      "Model capacity (MEC):     17 bits\n",
      "Architecture efficiency:   1.0 bits/parameter\n",
      "\n",
      "Estimating time to prime model...done.\n",
      "Estimated time to prime model: 0d 0h 1m 19s\n",
      "\n",
      "Priming model...done.\n",
      "Estimating training time...done.\n",
      "Estimated training time: 0d 0h 8m 59s\n",
      "\n",
      "Training...done.\n",
      "Model created:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=6, out_features=2, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Compiling predictor...done.\n",
      "Validating predictor...done.\n",
      "Classifier Type:                    Neural Network\n",
      "System Type:                        Binary classifier\n",
      "Best-guess accuracy:                61.61%\n",
      "Model accuracy:                     81.48% (726/891 correct)\n",
      "Improvement over best guess:        19.87% (of possible 38.39%)\n",
      "Model capacity (MEC):               17 bits\n",
      "Generalization ratio:               42.70 bits/bit\n",
      "Model efficiency:                   1.16%/parameter\n",
      "System behavior\n",
      "True Negatives:                     58.02% (517/891)\n",
      "True Positives:                     23.46% (209/891)\n",
      "False Negatives:                    14.93% (133/891)\n",
      "False Positives:                    3.59% (32/891)\n",
      "True Pos. Rate/Sensitivity/Recall:  0.61\n",
      "True Neg. Rate/Specificity:         0.94\n",
      "Precision:                          0.87\n",
      "F-1 Measure:                        0.72\n",
      "False Negative Rate/Miss Rate:      0.39\n",
      "Critical Success Index:             0.56\n",
      "\n",
      "\n",
      "Output: predict_igcol.py \n",
      "READY.\n"
     ]
    }
   ],
   "source": [
    "! ./btc_linux -v -v -f NN train.csv -o predict_igcol.py -target Survived -ignorecolumns PassengerId,Name,Ticket,Cabin,Embarked -e 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainome Daimensions(tm) 0.97 Copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to: Ariana Park\n",
      "Expiration date: 2020-11-30 (133 days left)\n",
      "Number of threads: 1\n",
      "Maximum file size: 4GB\n",
      "Connected to: https://beta.brainome.ai:8080\n",
      "\n",
      "Running btc will overwrite existing predict_rank.py. OK? [y/N] yes\n",
      "Input: train.csv\n",
      "Sampling...done.\n",
      "Preprocessing...done.\n",
      "Cleaning...done.\n",
      "Ranking attributes...done.\n",
      "Splitting into training and validation...done.\n",
      "Pre-training measurements...done.\n",
      "\n",
      "Attribute Ranking:\n",
      "Using only the important columns: Sex Fare Parch \n",
      "\n",
      "Data:\n",
      "Number of instances: 891\n",
      "Number of attributes: 3\n",
      "Number of classes: 2\n",
      "Class balance: 61.5% 38.5%\n",
      "\n",
      "Learnability:\n",
      "Best guess accuracy: 61.50%\n",
      "Capacity progression (# of decision points): [11, 11, 12, 12, 14, 14]\n",
      "Decision Tree: 226 parameters\n",
      "Estimated Memory Equivalent Capacity for Neural Networks: 41 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy...\n",
      "using Decision Tree: 50.73%\n",
      "using Neural Networks: 89.13%\n",
      "\n",
      "Expected Generalization...\n",
      "using Decision Tree: 3.94 bits/bit\n",
      "using a Neural Network: 21.73 bits/bit\n",
      "\n",
      "Recommendations:\n",
      "Note: Maybe enough data to generalize. [yellow]\n",
      "Warning: Cannot find numpy. The output predictor may not run on this machine.\n",
      "\n",
      "Time estimate for a Neural Network:\n",
      "Estimated time to architect: 0d 0h 0m 1s\n",
      "Estimated time to prime (subject to change after model architecting): 0d 0h 3m 14s\n",
      "Note: Machine learner type NN given by user.\n",
      "Architecting model...done.\n",
      "Model capacity (MEC):     21 bits\n",
      "Architecture efficiency:   1.0 bits/parameter\n",
      "\n",
      "Estimating time to prime model...done.\n",
      "Estimated time to prime model: 0d 0h 2m 22s\n",
      "\n",
      "Priming model...done.\n",
      "Estimating training time...done.\n",
      "Estimated training time: 0d 0h 15m 49s\n",
      "\n",
      "Training...done.\n",
      "Model created:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Compiling predictor...done.\n",
      "Validating predictor...done.\n",
      "Classifier Type:                    Neural Network\n",
      "System Type:                        Binary classifier\n",
      "Best-guess accuracy:                61.61%\n",
      "Model accuracy:                     79.34% (707/891 correct)\n",
      "Improvement over best guess:        17.73% (of possible 38.39%)\n",
      "Model capacity (MEC):               16 bits\n",
      "Generalization ratio:               44.18 bits/bit\n",
      "Model efficiency:                   1.10%/parameter\n",
      "System behavior\n",
      "True Negatives:                     53.09% (473/891)\n",
      "True Positives:                     26.26% (234/891)\n",
      "False Negatives:                    12.12% (108/891)\n",
      "False Positives:                    8.53% (76/891)\n",
      "True Pos. Rate/Sensitivity/Recall:  0.68\n",
      "True Neg. Rate/Specificity:         0.86\n",
      "Precision:                          0.75\n",
      "F-1 Measure:                        0.72\n",
      "False Negative Rate/Miss Rate:      0.32\n",
      "Critical Success Index:             0.56\n",
      "Overfitting:                        No\n",
      "\n",
      "Output: predict_rank.py \n",
      "READY.\n"
     ]
    }
   ],
   "source": [
    "! ./btc_linux -v -v -f NN train.csv -o predict_rank.py -target Survived -rank --yes -e 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P,c,l,a,s,s,\",\",S,e,x,\",\",A,g,e,\",\",S,i,b,S,p,\",\",P,a,r,c,h,\",\",F,a,r,e,\",\",P,r,e,d,i,c,t,i,o,n\r\n",
      "3,male,34.5,0,0,7.8292,0\r\n",
      "3,female,47,1,0,7,0\r\n",
      "2,male,62,0,0,9.6875,0\r\n",
      "3,male,27,0,0,8.6625,0\r\n",
      "3,female,22,1,1,12.2875,0\r\n",
      "3,male,14,0,0,9.225,0\r\n",
      "3,female,30,0,0,7.6292,0\r\n",
      "2,male,26,1,1,29,0\r\n",
      "3,female,18,0,0,7.2292,1\r\n"
     ]
    }
   ],
   "source": [
    "! python3 predict_igcol.py test.csv > prediction_igcol.csv\n",
    "! head prediction_igcol.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S,e,x,\",\",P,a,r,c,h,\",\",F,a,r,e,\",\",P,r,e,d,i,c,t,i,o,n\r\n",
      "male,0,7.8292,0\r\n",
      "female,0,7,1\r\n",
      "male,0,9.6875,0\r\n",
      "male,0,8.6625,0\r\n",
      "female,1,12.2875,1\r\n",
      "male,0,9.225,0\r\n",
      "female,0,7.6292,1\r\n",
      "male,1,29,0\r\n",
      "female,0,7.2292,1\r\n"
     ]
    }
   ],
   "source": [
    "! python3 predict_rank.py test.csv > prediction_rank.csv\n",
    "! head prediction_rank.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that -rank decided to only look at the columns 'Sex','Parch' (Parent/child), and 'Fare'. This makes a lot of sense that what determined the survival of a person on the Titanic was their sex, how many parents or children they had on board, and how much their fare was.\n",
    "\n",
    "Score for prediction on kaggle: \n",
    "ignorecolumns: 77.751%\n",
    "rank: 76.794%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "Success! We've built our first predictor and used it to make predictions on the Titanic test data. From here, we can use our model on any new Titanic data or use other control options to try to improve our results even more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
