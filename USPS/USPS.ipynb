{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPS Data Prediction Using Daimensions\n",
    "\n",
    "This dataset is from OpenML who describes the data as, \"Normalized handwritten digits, automatically scanned from envelopes by the U.S. Postal Service.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "We'll get the csv from the OpenML link and use a pandas dataframe to split it into training and validation data in csv's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              int0      double1      double2      double3      double4  \\\n",
       "count  9298.000000  9298.000000  9298.000000  9298.000000  9298.000000   \n",
       "mean      4.892020    -0.991800    -0.972226    -0.930421    -0.852805   \n",
       "std       3.001086     0.050814     0.118296     0.195285     0.284053   \n",
       "min       1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%       2.000000    -1.000000    -1.000000    -1.000000    -0.999914   \n",
       "50%       5.000000    -1.000000    -0.999992    -0.999608    -0.991661   \n",
       "75%       7.000000    -0.999969    -0.998444    -0.979572    -0.861493   \n",
       "max      10.000000     0.000308     0.332928     0.479436     0.523534   \n",
       "\n",
       "           double5      double6      double7      double8      double9  ...  \\\n",
       "count  9298.000000  9298.000000  9298.000000  9298.000000  9298.000000  ...   \n",
       "mean     -0.733673    -0.578239    -0.391187    -0.228260    -0.220399  ...   \n",
       "std       0.372653     0.435317     0.452878     0.454537     0.446069  ...   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  ...   \n",
       "25%      -0.996085    -0.963110    -0.787003    -0.620084    -0.571667  ...   \n",
       "50%      -0.932991    -0.747495    -0.447743    -0.138583    -0.147614  ...   \n",
       "75%      -0.589829    -0.260331     0.000547     0.143727     0.148815  ...   \n",
       "max       0.527370     0.531509     0.531319     0.531368     0.531327  ...   \n",
       "\n",
       "         double247    double248    double249    double250    double251  \\\n",
       "count  9298.000000  9298.000000  9298.000000  9298.000000  9298.000000   \n",
       "mean     -0.292865    -0.118513    -0.138364    -0.357547    -0.595574   \n",
       "std       0.483898     0.453286     0.449512     0.456625     0.422421   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000   \n",
       "25%      -0.742622    -0.430494    -0.465961    -0.770638    -0.968697   \n",
       "50%      -0.283600    -0.022176    -0.039908    -0.392889    -0.755935   \n",
       "75%       0.153227     0.251788     0.220543     0.033934    -0.306862   \n",
       "max       0.531380     0.531834     0.531857     0.531830     0.531472   \n",
       "\n",
       "         double252    double253    double254    double255    double256  \n",
       "count  9298.000000  9298.000000  9298.000000  9298.000000  9298.000000  \n",
       "mean     -0.766226    -0.874332    -0.936784    -0.970873    -0.989597  \n",
       "std       0.340464     0.254392     0.183444     0.120247     0.058028  \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  \n",
       "25%      -0.997448    -0.999957    -1.000000    -1.000000    -1.000000  \n",
       "50%      -0.946957    -0.993475    -0.999771    -0.999996    -1.000000  \n",
       "75%      -0.654382    -0.885085    -0.979766    -0.998040    -0.999942  \n",
       "max       0.523678     0.524670     0.470479     0.314115    -0.162598  \n",
       "\n",
       "[8 rows x 257 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>int0</th>\n      <th>double1</th>\n      <th>double2</th>\n      <th>double3</th>\n      <th>double4</th>\n      <th>double5</th>\n      <th>double6</th>\n      <th>double7</th>\n      <th>double8</th>\n      <th>double9</th>\n      <th>...</th>\n      <th>double247</th>\n      <th>double248</th>\n      <th>double249</th>\n      <th>double250</th>\n      <th>double251</th>\n      <th>double252</th>\n      <th>double253</th>\n      <th>double254</th>\n      <th>double255</th>\n      <th>double256</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>...</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n      <td>9298.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.892020</td>\n      <td>-0.991800</td>\n      <td>-0.972226</td>\n      <td>-0.930421</td>\n      <td>-0.852805</td>\n      <td>-0.733673</td>\n      <td>-0.578239</td>\n      <td>-0.391187</td>\n      <td>-0.228260</td>\n      <td>-0.220399</td>\n      <td>...</td>\n      <td>-0.292865</td>\n      <td>-0.118513</td>\n      <td>-0.138364</td>\n      <td>-0.357547</td>\n      <td>-0.595574</td>\n      <td>-0.766226</td>\n      <td>-0.874332</td>\n      <td>-0.936784</td>\n      <td>-0.970873</td>\n      <td>-0.989597</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.001086</td>\n      <td>0.050814</td>\n      <td>0.118296</td>\n      <td>0.195285</td>\n      <td>0.284053</td>\n      <td>0.372653</td>\n      <td>0.435317</td>\n      <td>0.452878</td>\n      <td>0.454537</td>\n      <td>0.446069</td>\n      <td>...</td>\n      <td>0.483898</td>\n      <td>0.453286</td>\n      <td>0.449512</td>\n      <td>0.456625</td>\n      <td>0.422421</td>\n      <td>0.340464</td>\n      <td>0.254392</td>\n      <td>0.183444</td>\n      <td>0.120247</td>\n      <td>0.058028</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>...</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-0.999914</td>\n      <td>-0.996085</td>\n      <td>-0.963110</td>\n      <td>-0.787003</td>\n      <td>-0.620084</td>\n      <td>-0.571667</td>\n      <td>...</td>\n      <td>-0.742622</td>\n      <td>-0.430494</td>\n      <td>-0.465961</td>\n      <td>-0.770638</td>\n      <td>-0.968697</td>\n      <td>-0.997448</td>\n      <td>-0.999957</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.000000</td>\n      <td>-1.000000</td>\n      <td>-0.999992</td>\n      <td>-0.999608</td>\n      <td>-0.991661</td>\n      <td>-0.932991</td>\n      <td>-0.747495</td>\n      <td>-0.447743</td>\n      <td>-0.138583</td>\n      <td>-0.147614</td>\n      <td>...</td>\n      <td>-0.283600</td>\n      <td>-0.022176</td>\n      <td>-0.039908</td>\n      <td>-0.392889</td>\n      <td>-0.755935</td>\n      <td>-0.946957</td>\n      <td>-0.993475</td>\n      <td>-0.999771</td>\n      <td>-0.999996</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.000000</td>\n      <td>-0.999969</td>\n      <td>-0.998444</td>\n      <td>-0.979572</td>\n      <td>-0.861493</td>\n      <td>-0.589829</td>\n      <td>-0.260331</td>\n      <td>0.000547</td>\n      <td>0.143727</td>\n      <td>0.148815</td>\n      <td>...</td>\n      <td>0.153227</td>\n      <td>0.251788</td>\n      <td>0.220543</td>\n      <td>0.033934</td>\n      <td>-0.306862</td>\n      <td>-0.654382</td>\n      <td>-0.885085</td>\n      <td>-0.979766</td>\n      <td>-0.998040</td>\n      <td>-0.999942</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10.000000</td>\n      <td>0.000308</td>\n      <td>0.332928</td>\n      <td>0.479436</td>\n      <td>0.523534</td>\n      <td>0.527370</td>\n      <td>0.531509</td>\n      <td>0.531319</td>\n      <td>0.531368</td>\n      <td>0.531327</td>\n      <td>...</td>\n      <td>0.531380</td>\n      <td>0.531834</td>\n      <td>0.531857</td>\n      <td>0.531830</td>\n      <td>0.531472</td>\n      <td>0.523678</td>\n      <td>0.524670</td>\n      <td>0.470479</td>\n      <td>0.314115</td>\n      <td>-0.162598</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 257 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# using pandas to get csv as a dataframe and see how it looks\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_url = 'https://www.openml.org/data/get_csv/19329737/usps.csv'\n",
    "data = pd.read_csv(dataset_url)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing csv's, y is for the target column (int0)\n",
    "y = data.int0\n",
    "X = data.drop('int0', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\n",
    "pd.concat([X_train, y_train], axis=1).to_csv('usps_train.csv',index=False)\n",
    "pd.concat([X_test, y_test], axis=1).to_csv('usps_valid.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Measurements\n",
    "\n",
    "We always want to measure our data before building our predictor in order to ensure we are building the right model. For more information about how to use Daimensions and why we want to measure our data beforehand, check out the Titanic notebook. Don't forget to use -target int0 because the target column is not on the very right for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "Brainome Daimensions(tm) 0.99 Copyright (c) 2019 - 2021 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:              Alexander Makhratchev  (Evaluation)\n",
      "Expiration Date:          2021-04-30   56 days left\n",
      "Number of Threads:        1\n",
      "Maximum File Size:        30 GB\n",
      "Maximum Instances:        unlimited\n",
      "Maximum Attributes:       unlimited\n",
      "Maximum Classes:          unlimited\n",
      "Connected to:             daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\n",
      "\n",
      "Command:\n",
      "    btc -measureonly usps_train.csv -target int0\n",
      "\n",
      "Start Time:                 03/05/2021, 18:30\n",
      "\n",
      "\n",
      "Data:\n",
      "    Input:                      usps_train.csv\n",
      "    Target Column:              int0\n",
      "    Number of instances:        7438\n",
      "    Number of attributes:       256\n",
      "    Number of classes:          10\n",
      "    Class Balance:              0: 7.56%, 1: 8.86%, 2: 16.79%, 3: 13.70%, 4: 9.75%, 5: 8.67%, 6: 9.38%, 7: 9.08%, 8: 8.55%, 9: 7.66%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          16.79%\n",
      "    Data Sufficiency:            Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:            at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Optimal Machine Learner:          10,  11,  12,  12,  13,  13\n",
      "\n",
      "\n",
      "Estimated Memory Equivalent Capacity for...\n",
      "    Decision Tree:                  5973 parameters\n",
      "    Neural Networks:                 241 parameters\n",
      "    Random Forest:                   132 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy using...\n",
      "    Decision Tree:                90.00%\n",
      "    Neural Networks:               9.89%\n",
      "    Random Forest:                 4.11%\n",
      "\n",
      "Expected Generalization using...\n",
      "    Decision Tree:                  4.07 bits/bit\n",
      "    Neural Network:                15.42 bits/bit\n",
      "    Random Forest:                 56.35 bits/bit\n",
      "\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time to Build Estimates:\n",
      "    Decision Tree:             a few seconds\n",
      "    Neural Network:              40 minutes\n",
      "\n",
      "Messages:\n",
      "Warning: Remapped class labels to be contiguous. Use -cm if DET/ROC-based accuracy measurements are wrong.\n",
      "\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from btc-dockerfile.12650             0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 239B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (5/7)                                                         \n",
      "\u001b[34m => [internal] load build definition from btc-dockerfile.12650             0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 239B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/brainome/btc_local_cpu:alpha    0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.37kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/brainome/btc_local_cpu:alpha                      0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (8/8) FINISHED                                                \n",
      "\u001b[34m => [internal] load build definition from btc-dockerfile.12650             0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 239B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/brainome/btc_local_cpu:alpha    0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.37kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => [1/3] FROM docker.io/brainome/btc_local_cpu:alpha                      0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/3] RUN adduser --disabled-password --gecos '' --uid 501 --g  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/3] COPY --chown=501:20 .daimensions.key /btc-alex            0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:43dd56f18fcffcdd02c290bfcce87c7a6cd0b9ccf4db3  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/btc-alex:latest                         0.0s\n",
      "\u001b[0m\u001b[?25hDocker image btc-alex:latest updated successfully.\n"
     ]
    }
   ],
   "source": [
    "! btc -measureonly usps_train.csv -target int0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Predictor\n",
    "\n",
    "Based on our measurements, Daimensions recommends we use a neural network (higher expected generalization) and more effort for this dataset. Don't forget to use -target because the target column isn't on the very right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING: Could not detect a GPU. Neural Network generation will be slow.\n",
      "\n",
      "Brainome Daimensions(tm) 0.99 Copyright (c) 2019 - 2021 by Brainome, Inc. All Rights Reserved.\n",
      "Licensed to:              Alexander Makhratchev  (Evaluation)\n",
      "Expiration Date:          2021-04-30   56 days left\n",
      "Number of Threads:        1\n",
      "Maximum File Size:        30 GB\n",
      "Maximum Instances:        unlimited\n",
      "Maximum Attributes:       unlimited\n",
      "Maximum Classes:          unlimited\n",
      "Connected to:             daimensions.brainome.ai  (local execution)\n",
      "\n",
      "\n",
      "\n",
      "Command:\n",
      "    btc -f NN usps_train.csv -o usps_predict.py -target int0 -e 5 --yes\n",
      "\n",
      "Start Time:                 03/05/2021, 20:09\n",
      "\n",
      "\n",
      "Data:\n",
      "    Input:                      usps_train.csv\n",
      "    Target Column:              int0\n",
      "    Number of instances:        7438\n",
      "    Number of attributes:       256\n",
      "    Number of classes:          10\n",
      "    Class Balance:              0: 7.56%, 1: 8.86%, 2: 16.79%, 3: 13.70%, 4: 9.75%, 5: 8.67%, 6: 9.38%, 7: 9.08%, 8: 8.55%, 9: 7.66%\n",
      "\n",
      "Learnability:\n",
      "    Best guess accuracy:          16.79%\n",
      "    Data Sufficiency:            Maybe enough data to generalize. [yellow]\n",
      "\n",
      "Capacity Progression:            at [ 5%, 10%, 20%, 40%, 80%, 100% ]\n",
      "    Optimal Machine Learner:          10,  11,  12,  12,  13,  13\n",
      "\n",
      "\n",
      "Estimated Memory Equivalent Capacity for...\n",
      "    Decision Tree:                  5973 parameters\n",
      "    Neural Networks:                 241 parameters\n",
      "    Random Forest:                   132 parameters\n",
      "\n",
      "Risk that model needs to overfit for 100% accuracy using...\n",
      "    Decision Tree:                90.00%\n",
      "    Neural Networks:               9.89%\n",
      "    Random Forest:                 4.11%\n",
      "\n",
      "Expected Generalization using...\n",
      "    Decision Tree:                  4.07 bits/bit\n",
      "    Neural Network:                15.42 bits/bit\n",
      "    Random Forest:                 56.35 bits/bit\n",
      "\n",
      "\n",
      "Recommendations:\n",
      "\n",
      "\n",
      "    Note: Machine learner type NN given by user.\n",
      "\n",
      "\n",
      "Time to Build Estimates:\n",
      "\n",
      "    Neural Network:              42 minutes\n",
      "\n",
      "\n",
      "\n",
      "Messages:\n",
      "Warning: Remapped class labels to be contiguous. Use -cm if DET/ROC-based accuracy measurements are wrong.\n",
      "Error Error: Predictor building failed. Output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! btc -f NN usps_train.csv -o usps_predict.py -target int0 -e 5 --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate the Model\n",
    "\n",
    "Now we can validate our model on our test data, a separate set of data that wasn't used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier Type:                    Neural Network\n",
      "System Type:                        10-way classifier\n",
      "Best-guess accuracy:                16.34%\n",
      "Model accuracy:                     93.49% (1739/1860 correct)\n",
      "Improvement over best guess:        77.15% (of possible 83.66%)\n",
      "Model capacity (MEC):               574 bits\n",
      "Generalization ratio:               9.89 bits/bit\n",
      "Model efficiency:                   0.13%/parameter\n",
      "Confusion Matrix:\n",
      " [6.88% 0.22% 0.05% 0.00% 0.11% 0.05% 0.32% 0.05% 0.05% 0.11%]\n",
      " [0.05% 8.39% 0.00% 0.00% 0.05% 0.00% 0.00% 0.00% 0.05% 0.32%]\n",
      " [0.05% 0.05% 15.81% 0.00% 0.22% 0.00% 0.00% 0.05% 0.05% 0.11%]\n",
      " [0.05% 0.05% 0.00% 13.17% 0.05% 0.05% 0.00% 0.05% 0.00% 0.00%]\n",
      " [0.22% 0.05% 0.16% 0.00% 10.05% 0.00% 0.27% 0.05% 0.00% 0.16%]\n",
      " [0.22% 0.00% 0.00% 0.00% 0.11% 8.60% 0.27% 0.00% 0.22% 0.05%]\n",
      " [0.00% 0.00% 0.16% 0.00% 0.00% 0.27% 7.74% 0.00% 0.00% 0.11%]\n",
      " [0.05% 0.00% 0.05% 0.00% 0.11% 0.00% 0.16% 8.17% 0.00% 0.00%]\n",
      " [0.00% 0.11% 0.11% 0.00% 0.05% 0.11% 0.05% 0.00% 7.96% 0.00%]\n",
      " [0.11% 0.38% 0.22% 0.05% 0.00% 0.11% 0.11% 0.11% 0.05% 6.72%]\n"
     ]
    }
   ],
   "source": [
    "! python3 usps_predict.py -validate usps_valid.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We have validated the accuracy of our model and found that it has a 92.9% accuracy for the test data. We can also see the confusion matrix, which tells us the percentage of data points from each class (columns) that were predicted to be in a certain class (rows). The diagonals are correctly predicted data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}