#!/usr/bin/env python3
#
# This code has been produced by a free evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Brainome grants an exclusive (subject to our continuing rights to use and modify models),
# worldwide, non-sublicensable, and non-transferable limited license to use and modify this
# predictor produced through the input of your data:
# (i) for users accessing the service through a free evaluation account, solely for your
# own non-commercial purposes, including for the purpose of evaluating this service, and
# (ii) for users accessing the service through a paid, commercial use account, for your
# own internal  and commercial purposes.
# Please contact support@brainome.ai with any questions.
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.991 Table Compiler v0.99.
# Invocation: btc train.csv -headerless -f NN -o NN.py -riskoverfit --yes
# Total compiler execution time: 0:03:45.42. Finished on: Mar-03-2021 20:05:56.
# This source code requires Python 3.
#
"""
Classifier Type:                    Neural Network
System Type:                         Binary classifier
Best-guess accuracy:                 56.62%
Overall Model accuracy:              71.19% (435/611 correct)
Overall Improvement over best guess: 14.57% (of possible 43.38%)
Model capacity (MEC):                1 bits
Generalization ratio:                429.47 bits/bit
Model efficiency:                    14.57%/parameter
System behavior
True Negatives:                      30.28% (185/611)
True Positives:                      40.92% (250/611)
False Negatives:                     15.71% (96/611)
False Positives:                     13.09% (80/611)
True Pos. Rate/Sensitivity/Recall:   0.72
True Neg. Rate/Specificity:          0.70
Precision:                           0.76
F-1 Measure:                         0.74
False Negative Rate/Miss Rate:       0.28
Critical Success Index:              0.59
Confusion Matrix:
 [30.28% 13.09%]
 [15.71% 40.92%]
Generalization index:                213.68
Percent of Data Memorized:           0.47%
Note: Unable to split dataset. The predictor was trained and evaluated on the same data.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "train.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 10
num_attr_before_transform = 17
n_classes = 2

mappings = [{14438374.0: 0, 19275536.0: 1, 25608676.0: 2, 34483034.0: 3, 42206356.0: 4, 52356230.0: 5, 57751729.0: 6, 72481559.0: 7, 97048107.0: 8, 98183848.0: 9, 101999541.0: 10, 121592750.0: 11, 128712510.0: 12, 150223520.0: 13, 185335163.0: 14, 210020023.0: 15, 219395316.0: 16, 230474062.0: 17, 254030430.0: 18, 260741084.0: 19, 261333654.0: 20, 267476096.0: 21, 287256246.0: 22, 292357650.0: 23, 294253604.0: 24, 307356341.0: 25, 322627699.0: 26, 332481740.0: 27, 356985180.0: 28, 368256995.0: 29, 387874234.0: 30, 392814586.0: 31, 396144881.0: 32, 431944539.0: 33, 434210966.0: 34, 438220847.0: 35, 449560183.0: 36, 453494825.0: 37, 460658976.0: 38, 507517280.0: 39, 517595976.0: 40, 518148871.0: 41, 524035481.0: 42, 536411770.0: 43, 568661155.0: 44, 569375457.0: 45, 574345445.0: 46, 583223728.0: 47, 605016732.0: 48, 606865115.0: 49, 610166173.0: 50, 655492053.0: 51, 660921865.0: 52, 673768396.0: 53, 675733664.0: 54, 683371973.0: 55, 691127611.0: 56, 703791733.0: 57, 704484947.0: 58, 728217260.0: 59, 770591215.0: 60, 771930264.0: 61, 773607164.0: 62, 779751350.0: 63, 780909562.0: 64, 785383211.0: 65, 798453299.0: 66, 844342231.0: 67, 879877343.0: 68, 886057477.0: 69, 915888911.0: 70, 921730038.0: 71, 923189083.0: 72, 932818398.0: 73, 942484986.0: 74, 947593753.0: 75, 950364298.0: 76, 952652543.0: 77, 955319217.0: 78, 961077930.0: 79, 965856914.0: 80, 993745085.0: 81, 994465821.0: 82, 1000047214.0: 83, 1015147800.0: 84, 1041310687.0: 85, 1045033435.0: 86, 1059139687.0: 87, 1061996912.0: 88, 1062743419.0: 89, 1064426734.0: 90, 1069634172.0: 91, 1075388431.0: 92, 1078328850.0: 93, 1085427910.0: 94, 1089070354.0: 95, 1099269285.0: 96, 1100471843.0: 97, 1108664302.0: 98, 1112057650.0: 99, 1116865990.0: 100, 1130530067.0: 101, 1140713955.0: 102, 1156065551.0: 103, 1160032643.0: 104, 1181445565.0: 105, 1183688952.0: 106, 1184782631.0: 107, 1195006344.0: 108, 1195198587.0: 109, 1197924389.0: 110, 1202906823.0: 111, 1206813863.0: 112, 1211705288.0: 113, 1215728838.0: 114, 1228007915.0: 115, 1236927418.0: 116, 1237925370.0: 117, 1239799767.0: 118, 1244451331.0: 119, 1246729982.0: 120, 1251792878.0: 121, 1256100349.0: 122, 1283284949.0: 123, 1289812566.0: 124, 1294170013.0: 125, 1321396846.0: 126, 1327086521.0: 127, 1327992878.0: 128, 1334645300.0: 129, 1361090188.0: 130, 1374688421.0: 131, 1397511659.0: 132, 1417763362.0: 133, 1430661785.0: 134, 1443094732.0: 135, 1447267001.0: 136, 1474260765.0: 137, 1487136559.0: 138, 1491012404.0: 139, 1528658381.0: 140, 1545243182.0: 141, 1545631928.0: 142, 1575964265.0: 143, 1604712024.0: 144, 1605816289.0: 145, 1612169585.0: 146, 1614698557.0: 147, 1615129550.0: 148, 1623899406.0: 149, 1663867838.0: 150, 1678642569.0: 151, 1687372354.0: 152, 1695271323.0: 153, 1704380541.0: 154, 1716284491.0: 155, 1716426346.0: 156, 1716830930.0: 157, 1762802763.0: 158, 1763412382.0: 159, 1766545846.0: 160, 1767292537.0: 161, 1786538196.0: 162, 1817297805.0: 163, 1818102010.0: 164, 1826749532.0: 165, 1827721892.0: 166, 1833135769.0: 167, 1843298816.0: 168, 1851814389.0: 169, 1857592438.0: 170, 1879952774.0: 171, 1885817866.0: 172, 1893366975.0: 173, 1911470262.0: 174, 1949014675.0: 175, 1951846852.0: 176, 2001646876.0: 177, 2006345565.0: 178, 2015780890.0: 179, 2025314456.0: 180, 2034820859.0: 181, 2035497991.0: 182, 2040430863.0: 183, 2054312979.0: 184, 2059006482.0: 185, 2092901915.0: 186, 2112882251.0: 187, 2118783026.0: 188, 2123201168.0: 189, 2131786717.0: 190, 2138558408.0: 191, 2166488695.0: 192, 2168805132.0: 193, 2170422731.0: 194, 2173531120.0: 195, 2191596592.0: 196, 2199396670.0: 197, 2206061404.0: 198, 2206843424.0: 199, 2225198138.0: 200, 2226407419.0: 201, 2262196297.0: 202, 2269652413.0: 203, 2272284665.0: 204, 2291635305.0: 205, 2304436299.0: 206, 2311132311.0: 207, 2314899728.0: 208, 2318212753.0: 209, 2328072520.0: 210, 2348226903.0: 211, 2348848582.0: 212, 2348931288.0: 213, 2352840952.0: 214, 2356091159.0: 215, 2375336174.0: 216, 2384242561.0: 217, 2459127098.0: 218, 2480629956.0: 219, 2530633102.0: 220, 2538375979.0: 221, 2544157014.0: 222, 2547598508.0: 223, 2573187445.0: 224, 2585531323.0: 225, 2585575067.0: 226, 2605912272.0: 227, 2637151446.0: 228, 2641325034.0: 229, 2643203998.0: 230, 2646264792.0: 231, 2650025071.0: 232, 2650154989.0: 233, 2651204780.0: 234, 2656309072.0: 235, 2677410266.0: 236, 2691974267.0: 237, 2693114017.0: 238, 2704141538.0: 239, 2730866272.0: 240, 2746423771.0: 241, 2750583238.0: 242, 2752230648.0: 243, 2771258979.0: 244, 2777870491.0: 245, 2813584707.0: 246, 2813883003.0: 247, 2817506516.0: 248, 2821533334.0: 249, 2823552578.0: 250, 2835934863.0: 251, 2902039846.0: 252, 2908309946.0: 253, 2925502825.0: 254, 2936427368.0: 255, 2937498213.0: 256, 2938023358.0: 257, 2944869461.0: 258, 2954644661.0: 259, 2957856773.0: 260, 2961060956.0: 261, 2976515959.0: 262, 2977404082.0: 263, 2998182320.0: 264, 3005102281.0: 265, 3007004827.0: 266, 3014206860.0: 267, 3016030946.0: 268, 3024796147.0: 269, 3027438308.0: 270, 3029133314.0: 271, 3032199890.0: 272, 3032557274.0: 273, 3062673287.0: 274, 3064647399.0: 275, 3066251234.0: 276, 3068833222.0: 277, 3078168974.0: 278, 3078536508.0: 279, 3079590098.0: 280, 3083442690.0: 281, 3103194898.0: 282, 3118076876.0: 283, 3120299359.0: 284, 3121634756.0: 285, 3130093046.0: 286, 3136168014.0: 287, 3154456304.0: 288, 3155477865.0: 289, 3184726212.0: 290, 3185041485.0: 291, 3211893110.0: 292, 3212251018.0: 293, 3231084251.0: 294, 3238869153.0: 295, 3256361169.0: 296, 3294997098.0: 297, 3321419372.0: 298, 3332599810.0: 299, 3337883487.0: 300, 3353809852.0: 301, 3354474659.0: 302, 3368002933.0: 303, 3399117997.0: 304, 3417616276.0: 305, 3425601542.0: 306, 3429424927.0: 307, 3450443271.0: 308, 3472355565.0: 309, 3477550270.0: 310, 3489095984.0: 311, 3495298176.0: 312, 3497265397.0: 313, 3513927979.0: 314, 3523548446.0: 315, 3539625758.0: 316, 3544822463.0: 317, 3548854625.0: 318, 3582020915.0: 319, 3585860572.0: 320, 3587134081.0: 321, 3599319652.0: 322, 3608708965.0: 323, 3609755037.0: 324, 3612307560.0: 325, 3625357121.0: 326, 3632220754.0: 327, 3645384836.0: 328, 3646564796.0: 329, 3675094347.0: 330, 3693548982.0: 331, 3718014315.0: 332, 3730427942.0: 333, 3737890909.0: 334, 3747919933.0: 335, 3774288471.0: 336, 3777667027.0: 337, 3786452800.0: 338, 3794955860.0: 339, 3796450538.0: 340, 3806340213.0: 341, 3812324823.0: 342, 3817040114.0: 343, 3819965360.0: 344, 3821087570.0: 345, 3823060489.0: 346, 3824622271.0: 347, 3835797112.0: 348, 3837056040.0: 349, 3838247038.0: 350, 3840103622.0: 351, 3852202685.0: 352, 3869377260.0: 353, 3869580040.0: 354, 3878238530.0: 355, 3885733761.0: 356, 3891374463.0: 357, 3896487929.0: 358, 3898232937.0: 359, 3909353149.0: 360, 3911330153.0: 361, 3916004863.0: 362, 3918063541.0: 363, 3926109017.0: 364, 3944903699.0: 365, 3952414092.0: 366, 3962678306.0: 367, 3974443357.0: 368, 3975999329.0: 369, 3996119901.0: 370, 4008949871.0: 371, 4024018085.0: 372, 4025692512.0: 373, 4029101205.0: 374, 4031432752.0: 375, 4040739001.0: 376, 4060501596.0: 377, 4061345962.0: 378, 4068582345.0: 379, 4072224595.0: 380, 4074952108.0: 381, 4085279709.0: 382, 4092791281.0: 383, 4099429781.0: 384, 4121198927.0: 385, 4138928921.0: 386, 4141481240.0: 387, 4147099920.0: 388, 4163155599.0: 389, 4189425140.0: 390, 4204061959.0: 391, 4209319047.0: 392, 4212349008.0: 393, 4224031685.0: 394, 4226558647.0: 395, 4235264983.0: 396, 4254308918.0: 397, 4264383717.0: 398, 4267144896.0: 399, 4268984148.0: 400, 4273431933.0: 401, 4279584261.0: 402, 4292095473.0: 403}, {7.0: 0, 23.0: 1, 24912438.0: 2, 27781528.0: 3, 34509546.0: 4, 57800972.0: 5, 58546343.0: 6, 67305272.0: 7, 74694161.0: 8, 78232637.0: 9, 90023502.0: 10, 92445772.0: 11, 94032900.0: 12, 100442470.0: 13, 110529953.0: 14, 110932625.0: 15, 126708493.0: 16, 135467093.0: 17, 135639440.0: 18, 160175220.0: 19, 173786018.0: 20, 175412407.0: 21, 178531705.0: 22, 178929408.0: 23, 183115686.0: 24, 204552627.0: 25, 212756140.0: 26, 220107671.0: 27, 233402521.0: 28, 233654802.0: 29, 245904554.0: 30, 254634217.0: 31, 261553110.0: 32, 267463349.0: 33, 273280760.0: 34, 286533941.0: 35, 311766385.0: 36, 325152987.0: 37, 363461320.0: 38, 373883067.0: 39, 378919727.0: 40, 382722490.0: 41, 387142210.0: 42, 415184118.0: 43, 421984931.0: 44, 429031778.0: 45, 437375818.0: 46, 438149806.0: 47, 439885711.0: 48, 451538897.0: 49, 470906278.0: 50, 471422401.0: 51, 476971466.0: 52, 478244247.0: 53, 480444424.0: 54, 486740609.0: 55, 487350276.0: 56, 490248599.0: 57, 502047778.0: 58, 512271888.0: 59, 518878421.0: 60, 543138258.0: 61, 545535916.0: 62, 549156740.0: 63, 549460847.0: 64, 555919303.0: 65, 556837560.0: 66, 560750387.0: 67, 567007230.0: 68, 571649819.0: 69, 574928312.0: 70, 589118363.0: 71, 589485885.0: 72, 600229260.0: 73, 610107062.0: 74, 615692734.0: 75, 629011236.0: 76, 653482784.0: 77, 660921865.0: 78, 661623687.0: 79, 675276297.0: 80, 705736444.0: 81, 707889831.0: 82, 718680117.0: 83, 724880469.0: 84, 726169373.0: 85, 735096199.0: 86, 742334599.0: 87, 749320570.0: 88, 773651511.0: 89, 780909562.0: 90, 784234174.0: 91, 786286951.0: 92, 800319913.0: 93, 801189426.0: 94, 803205625.0: 95, 826674162.0: 96, 830615833.0: 97, 831569975.0: 98, 835722575.0: 99, 837729635.0: 100, 853393974.0: 101, 857161933.0: 102, 864182021.0: 103, 872101728.0: 104, 874056148.0: 105, 878558674.0: 106, 900234966.0: 107, 906407436.0: 108, 910137747.0: 109, 924583470.0: 110, 960612051.0: 111, 961077930.0: 112, 962274340.0: 113, 970930427.0: 114, 974262596.0: 115, 976821116.0: 116, 990221162.0: 117, 1000640485.0: 118, 1014731276.0: 119, 1016362467.0: 120, 1018723774.0: 121, 1018736935.0: 122, 1019149365.0: 123, 1026342451.0: 124, 1038309929.0: 125, 1042151339.0: 126, 1044702708.0: 127, 1049838412.0: 128, 1051412405.0: 129, 1058792138.0: 130, 1062832653.0: 131, 1072975817.0: 132, 1074015435.0: 133, 1078240422.0: 134, 1079304826.0: 135, 1079680473.0: 136, 1085502704.0: 137, 1100513452.0: 138, 1104803137.0: 139, 1107098894.0: 140, 1121725713.0: 141, 1136257877.0: 142, 1145669966.0: 143, 1148640636.0: 144, 1149261055.0: 145, 1152041945.0: 146, 1154193882.0: 147, 1164586403.0: 148, 1167214796.0: 149, 1169455533.0: 150, 1197209637.0: 151, 1202710475.0: 152, 1202906823.0: 153, 1204091664.0: 154, 1214475308.0: 155, 1214940315.0: 156, 1225056430.0: 157, 1232658706.0: 158, 1240955073.0: 159, 1242314671.0: 160, 1244151527.0: 161, 1244451331.0: 162, 1247244113.0: 163, 1266933612.0: 164, 1283968110.0: 165, 1295029657.0: 166, 1300693632.0: 167, 1303005520.0: 168, 1303613027.0: 169, 1309332820.0: 170, 1327554668.0: 171, 1341728912.0: 172, 1344310001.0: 173, 1349838342.0: 174, 1354734563.0: 175, 1355451507.0: 176, 1371060110.0: 177, 1379555617.0: 178, 1388636530.0: 179, 1396090051.0: 180, 1416329589.0: 181, 1416782011.0: 182, 1417459000.0: 183, 1418598190.0: 184, 1426154839.0: 185, 1438155925.0: 186, 1443094732.0: 187, 1453185156.0: 188, 1458909352.0: 189, 1459550278.0: 190, 1462049307.0: 191, 1489289611.0: 192, 1502932386.0: 193, 1514711906.0: 194, 1515522387.0: 195, 1541638097.0: 196, 1544185941.0: 197, 1555297337.0: 198, 1557879152.0: 199, 1574587461.0: 200, 1584543630.0: 201, 1593061191.0: 202, 1594137860.0: 203, 1594417462.0: 204, 1595129426.0: 205, 1612756568.0: 206, 1613734226.0: 207, 1655630219.0: 208, 1659540268.0: 209, 1662994089.0: 210, 1678326563.0: 211, 1685886588.0: 212, 1687986856.0: 213, 1692326872.0: 214, 1693304982.0: 215, 1705001493.0: 216, 1706742528.0: 217, 1721295446.0: 218, 1726177906.0: 219, 1734783177.0: 220, 1747650913.0: 221, 1755920201.0: 222, 1758030381.0: 223, 1761313302.0: 224, 1763040103.0: 225, 1766445161.0: 226, 1782459374.0: 227, 1793058452.0: 228, 1823093092.0: 229, 1834289762.0: 230, 1842501564.0: 231, 1845816575.0: 232, 1862863552.0: 233, 1870885585.0: 234, 1870938630.0: 235, 1873373635.0: 236, 1876185064.0: 237, 1882993470.0: 238, 1884709855.0: 239, 1905579645.0: 240, 1911255327.0: 241, 1912547561.0: 242, 1918480601.0: 243, 1949925606.0: 244, 1958738199.0: 245, 1966036495.0: 246, 1970638036.0: 247, 1985024410.0: 248, 2001682744.0: 249, 2025302972.0: 250, 2026751518.0: 251, 2029251254.0: 252, 2030269775.0: 253, 2049133090.0: 254, 2057976543.0: 255, 2073444678.0: 256, 2077445850.0: 257, 2077558450.0: 258, 2079768190.0: 259, 2087590737.0: 260, 2088362560.0: 261, 2101023719.0: 262, 2107051770.0: 263, 2118152583.0: 264, 2119552911.0: 265, 2124386493.0: 266, 2131129727.0: 267, 2140073204.0: 268, 2141535652.0: 269, 2152689192.0: 270, 2157121829.0: 271, 2193771654.0: 272, 2195567960.0: 273, 2203622686.0: 274, 2206468513.0: 275, 2206843424.0: 276, 2207585883.0: 277, 2211064200.0: 278, 2232181639.0: 279, 2235517682.0: 280, 2239439855.0: 281, 2275333408.0: 282, 2311132311.0: 283, 2318212753.0: 284, 2328072520.0: 285, 2335885480.0: 286, 2358601623.0: 287, 2358788886.0: 288, 2385511062.0: 289, 2386564347.0: 290, 2400106481.0: 291, 2416614906.0: 292, 2423634773.0: 293, 2459317540.0: 294, 2462746012.0: 295, 2464543972.0: 296, 2472849858.0: 297, 2474478805.0: 298, 2491458842.0: 299, 2504735548.0: 300, 2514202532.0: 301, 2516018870.0: 302, 2533191715.0: 303, 2540084244.0: 304, 2543344946.0: 305, 2547598508.0: 306, 2548123924.0: 307, 2552750204.0: 308, 2554763798.0: 309, 2563578405.0: 310, 2574801322.0: 311, 2582292864.0: 312, 2605273634.0: 313, 2638470172.0: 314, 2644987116.0: 315, 2650620910.0: 316, 2651204780.0: 317, 2672245846.0: 318, 2673434092.0: 319, 2675528111.0: 320, 2679040920.0: 321, 2681151718.0: 322, 2697352614.0: 323, 2712498678.0: 324, 2714382534.0: 325, 2716558316.0: 326, 2719068558.0: 327, 2727950343.0: 328, 2735672137.0: 329, 2736770535.0: 330, 2760510656.0: 331, 2762006528.0: 332, 2763190271.0: 333, 2768345833.0: 334, 2771902425.0: 335, 2777014306.0: 336, 2777870491.0: 337, 2799911430.0: 338, 2803217824.0: 339, 2803957365.0: 340, 2806408839.0: 341, 2808436761.0: 342, 2812123245.0: 343, 2829802721.0: 344, 2834190751.0: 345, 2835112928.0: 346, 2839528597.0: 347, 2841888979.0: 348, 2844077120.0: 349, 2851699527.0: 350, 2855021960.0: 351, 2857609458.0: 352, 2859725685.0: 353, 2863453296.0: 354, 2866508787.0: 355, 2868056072.0: 356, 2875611965.0: 357, 2889109913.0: 358, 2889955871.0: 359, 2897638812.0: 360, 2908439648.0: 361, 2930137942.0: 362, 2940039229.0: 363, 2941372240.0: 364, 2949301720.0: 365, 2981159714.0: 366, 2998741153.0: 367, 2998836177.0: 368, 3000966139.0: 369, 3002954046.0: 370, 3027438308.0: 371, 3029194631.0: 372, 3031634380.0: 373, 3043322698.0: 374, 3044411177.0: 375, 3053844648.0: 376, 3057442944.0: 377, 3059739868.0: 378, 3065302639.0: 379, 3067356446.0: 380, 3070264142.0: 381, 3079798799.0: 382, 3084632768.0: 383, 3109233127.0: 384, 3111989762.0: 385, 3155477865.0: 386, 3162199369.0: 387, 3162504229.0: 388, 3164453934.0: 389, 3165753393.0: 390, 3165980828.0: 391, 3166440751.0: 392, 3166494274.0: 393, 3176331138.0: 394, 3177526928.0: 395, 3186675955.0: 396, 3192317775.0: 397, 3201615477.0: 398, 3207606841.0: 399, 3208299250.0: 400, 3218526205.0: 401, 3226031101.0: 402, 3229074124.0: 403, 3230654386.0: 404, 3236810174.0: 405, 3244229386.0: 406, 3244353412.0: 407, 3244660118.0: 408, 3245213785.0: 409, 3250060553.0: 410, 3259693709.0: 411, 3262811287.0: 412, 3295758135.0: 413, 3311101490.0: 414, 3315722205.0: 415, 3336437972.0: 416, 3351180676.0: 417, 3353809852.0: 418, 3363217858.0: 419, 3385882911.0: 420, 3394882861.0: 421, 3396268083.0: 422, 3399039577.0: 423, 3403004650.0: 424, 3406973901.0: 425, 3414060082.0: 426, 3416300731.0: 427, 3429115925.0: 428, 3433143077.0: 429, 3433762710.0: 430, 3435472775.0: 431, 3448917835.0: 432, 3450723742.0: 433, 3456467497.0: 434, 3459398929.0: 435, 3466412997.0: 436, 3467629909.0: 437, 3474580565.0: 438, 3480842084.0: 439, 3485698993.0: 440, 3491642946.0: 441, 3521486723.0: 442, 3534984881.0: 443, 3548510933.0: 444, 3553799656.0: 445, 3554050154.0: 446, 3556730453.0: 447, 3564225998.0: 448, 3577854031.0: 449, 3578388110.0: 450, 3579133125.0: 451, 3581537998.0: 452, 3583948997.0: 453, 3585567766.0: 454, 3588596234.0: 455, 3602024962.0: 456, 3604312831.0: 457, 3619071004.0: 458, 3621829235.0: 459, 3633666797.0: 460, 3636017715.0: 461, 3644473344.0: 462, 3652877990.0: 463, 3654694029.0: 464, 3663267648.0: 465, 3665142704.0: 466, 3686965720.0: 467, 3693160279.0: 468, 3699895476.0: 469, 3707045745.0: 470, 3716325640.0: 471, 3717750416.0: 472, 3721719776.0: 473, 3749478540.0: 474, 3759385364.0: 475, 3771630096.0: 476, 3776412816.0: 477, 3777058074.0: 478, 3778312590.0: 479, 3780548023.0: 480, 3781763175.0: 481, 3786635286.0: 482, 3796721982.0: 483, 3801035841.0: 484, 3823250358.0: 485, 3824533185.0: 486, 3826699694.0: 487, 3828523958.0: 488, 3830877046.0: 489, 3845739567.0: 490, 3846638654.0: 491, 3853247909.0: 492, 3879506526.0: 493, 3915905870.0: 494, 3932151512.0: 495, 3932990794.0: 496, 3935877661.0: 497, 3939406475.0: 498, 3950978558.0: 499, 3953059257.0: 500, 3958844543.0: 501, 3973995420.0: 502, 3979639648.0: 503, 3984012914.0: 504, 3992548868.0: 505, 3999311057.0: 506, 4002885777.0: 507, 4006193247.0: 508, 4013473256.0: 509, 4019810774.0: 510, 4029086902.0: 511, 4043050231.0: 512, 4068553435.0: 513, 4095138452.0: 514, 4102205785.0: 515, 4107124938.0: 516, 4118108704.0: 517, 4126975345.0: 518, 4128341618.0: 519, 4132722579.0: 520, 4134992367.0: 521, 4170404721.0: 522, 4180791840.0: 523, 4187390584.0: 524, 4208917383.0: 525, 4218316020.0: 526, 4220124471.0: 527, 4220638899.0: 528, 4224031685.0: 529, 4231452819.0: 530, 4234349297.0: 531, 4238468782.0: 532, 4241556815.0: 533, 4259076990.0: 534, 4265487810.0: 535, 4279375643.0: 536, 4281466663.0: 537, 4282714744.0: 538, 4294374089.0: 539}, {23.0: 0, 5001639.0: 1, 7464789.0: 2, 15549123.0: 3, 17259925.0: 4, 20094859.0: 5, 34509546.0: 6, 35528601.0: 7, 46826353.0: 8, 55013246.0: 9, 73725205.0: 10, 78232637.0: 11, 80590836.0: 12, 85046866.0: 13, 90023502.0: 14, 96870942.0: 15, 101210381.0: 16, 102556620.0: 17, 109856110.0: 18, 112457176.0: 19, 126229530.0: 20, 126708493.0: 21, 128976492.0: 22, 134348685.0: 23, 135467093.0: 24, 135639440.0: 25, 149857078.0: 26, 157530780.0: 27, 158069839.0: 28, 160175220.0: 29, 173786018.0: 30, 175412407.0: 31, 178531705.0: 32, 179968866.0: 33, 181178000.0: 34, 186943357.0: 35, 191135231.0: 36, 194715622.0: 37, 224585765.0: 38, 240153696.0: 39, 245904554.0: 40, 252782083.0: 41, 256661173.0: 42, 257928404.0: 43, 286533941.0: 44, 292458720.0: 45, 297880831.0: 46, 300640493.0: 47, 305271677.0: 48, 336036178.0: 49, 343489468.0: 50, 360948266.0: 51, 385844085.0: 52, 387142210.0: 53, 407494832.0: 54, 409392778.0: 55, 411708419.0: 56, 412713407.0: 57, 415184118.0: 58, 420512728.0: 59, 431666554.0: 60, 438149806.0: 61, 439885711.0: 62, 443950784.0: 63, 446033690.0: 64, 456621386.0: 65, 470906278.0: 66, 480444424.0: 67, 484227320.0: 68, 486120724.0: 69, 486740609.0: 70, 504959913.0: 71, 520333990.0: 72, 525149040.0: 73, 530047364.0: 74, 533017648.0: 75, 537335443.0: 76, 541661055.0: 77, 545535916.0: 78, 546805319.0: 79, 549460847.0: 80, 549921497.0: 81, 550976035.0: 82, 556837560.0: 83, 570072855.0: 84, 571649819.0: 85, 581294726.0: 86, 585673871.0: 87, 587511766.0: 88, 589136110.0: 89, 589485885.0: 90, 615692734.0: 91, 628268444.0: 92, 629076479.0: 93, 644021811.0: 94, 646380074.0: 95, 651998645.0: 96, 653482784.0: 97, 661623687.0: 98, 662298405.0: 99, 671517342.0: 100, 672011935.0: 101, 675276297.0: 102, 677568498.0: 103, 682044392.0: 104, 700417740.0: 105, 701634798.0: 106, 701662190.0: 107, 705736444.0: 108, 708323899.0: 109, 711667872.0: 110, 714195495.0: 111, 724880469.0: 112, 743310145.0: 113, 747075981.0: 114, 763876539.0: 115, 773108186.0: 116, 781009553.0: 117, 790626052.0: 118, 800319913.0: 119, 803205625.0: 120, 816313663.0: 121, 828903060.0: 122, 830615833.0: 123, 835465494.0: 124, 839755767.0: 125, 844947176.0: 126, 864785913.0: 127, 872663621.0: 128, 877161081.0: 129, 900147256.0: 130, 906407436.0: 131, 910137747.0: 132, 924583470.0: 133, 933464125.0: 134, 953329083.0: 135, 967048886.0: 136, 967827268.0: 137, 980071176.0: 138, 1000147678.0: 139, 1005152652.0: 140, 1005617468.0: 141, 1011490657.0: 142, 1016362467.0: 143, 1018736935.0: 144, 1019149365.0: 145, 1024626505.0: 146, 1025704892.0: 147, 1026342451.0: 148, 1030049972.0: 149, 1035955469.0: 150, 1051412405.0: 151, 1058792138.0: 152, 1061750042.0: 153, 1066951520.0: 154, 1079304826.0: 155, 1079680473.0: 156, 1109508113.0: 157, 1124213105.0: 158, 1129115198.0: 159, 1143641245.0: 160, 1145669966.0: 161, 1149261055.0: 162, 1150262565.0: 163, 1151425244.0: 164, 1152041945.0: 165, 1153441727.0: 166, 1154193882.0: 167, 1160349156.0: 168, 1161257120.0: 169, 1164586403.0: 170, 1167214796.0: 171, 1175041092.0: 172, 1176195192.0: 173, 1196569046.0: 174, 1197209637.0: 175, 1204091664.0: 176, 1214940315.0: 177, 1220902789.0: 178, 1221841885.0: 179, 1230341268.0: 180, 1232658706.0: 181, 1239808586.0: 182, 1240625911.0: 183, 1244151527.0: 184, 1254346587.0: 185, 1264217788.0: 186, 1271903900.0: 187, 1300693632.0: 188, 1303005520.0: 189, 1314333349.0: 190, 1314861336.0: 191, 1353999074.0: 192, 1365678725.0: 193, 1388636530.0: 194, 1412446703.0: 195, 1413348975.0: 196, 1416329589.0: 197, 1426154839.0: 198, 1437881425.0: 199, 1438410634.0: 200, 1447379432.0: 201, 1447920682.0: 202, 1458909352.0: 203, 1476895712.0: 204, 1477550872.0: 205, 1478871495.0: 206, 1483575294.0: 207, 1493269803.0: 208, 1515837975.0: 209, 1535893723.0: 210, 1541638097.0: 211, 1547826661.0: 212, 1551060117.0: 213, 1551206981.0: 214, 1560285521.0: 215, 1565543960.0: 216, 1572545147.0: 217, 1573289480.0: 218, 1573518520.0: 219, 1576406864.0: 220, 1580837964.0: 221, 1584543630.0: 222, 1593061191.0: 223, 1593381810.0: 224, 1595129426.0: 225, 1609695111.0: 226, 1613734226.0: 227, 1613777122.0: 228, 1643910571.0: 229, 1654502306.0: 230, 1659645549.0: 231, 1662994089.0: 232, 1685886588.0: 233, 1687986856.0: 234, 1697228326.0: 235, 1702177814.0: 236, 1705592113.0: 237, 1723417264.0: 238, 1726177906.0: 239, 1733889840.0: 240, 1740400250.0: 241, 1751467247.0: 242, 1758030381.0: 243, 1762134021.0: 244, 1763363443.0: 245, 1765428190.0: 246, 1766445161.0: 247, 1770058094.0: 248, 1780072936.0: 249, 1790955053.0: 250, 1802774133.0: 251, 1804722075.0: 252, 1813951472.0: 253, 1823093092.0: 254, 1829609079.0: 255, 1829825616.0: 256, 1873329119.0: 257, 1882993470.0: 258, 1883154711.0: 259, 1885626160.0: 260, 1885795465.0: 261, 1901384177.0: 262, 1902253760.0: 263, 1912547561.0: 264, 1930036788.0: 265, 1951397457.0: 266, 1962491979.0: 267, 1972304979.0: 268, 1982138802.0: 269, 2001682744.0: 270, 2012587448.0: 271, 2012647547.0: 272, 2019204357.0: 273, 2033788508.0: 274, 2034141426.0: 275, 2038221848.0: 276, 2054303838.0: 277, 2057894595.0: 278, 2057976543.0: 279, 2060105701.0: 280, 2062347744.0: 281, 2062554705.0: 282, 2075024078.0: 283, 2079601839.0: 284, 2092572731.0: 285, 2098303096.0: 286, 2101023719.0: 287, 2105317412.0: 288, 2107051770.0: 289, 2112272240.0: 290, 2122008394.0: 291, 2124486544.0: 292, 2126353282.0: 293, 2130574420.0: 294, 2134893297.0: 295, 2136002017.0: 296, 2138664491.0: 297, 2140073204.0: 298, 2145106316.0: 299, 2149981693.0: 300, 2154483372.0: 301, 2157121829.0: 302, 2159788953.0: 303, 2165797850.0: 304, 2181035406.0: 305, 2182025560.0: 306, 2199005317.0: 307, 2202215285.0: 308, 2202722407.0: 309, 2205046664.0: 310, 2206468513.0: 311, 2222585015.0: 312, 2232220939.0: 313, 2236933882.0: 314, 2239439855.0: 315, 2252420300.0: 316, 2259059784.0: 317, 2262273411.0: 318, 2265316074.0: 319, 2275333408.0: 320, 2277950194.0: 321, 2288337029.0: 322, 2294890871.0: 323, 2303615277.0: 324, 2304711118.0: 325, 2305855286.0: 326, 2309523431.0: 327, 2309645349.0: 328, 2311132311.0: 329, 2313811334.0: 330, 2319053561.0: 331, 2336903941.0: 332, 2338337801.0: 333, 2347144938.0: 334, 2356178653.0: 335, 2404045202.0: 336, 2412948137.0: 337, 2436068717.0: 338, 2441129888.0: 339, 2450933841.0: 340, 2459317540.0: 341, 2463368803.0: 342, 2464543972.0: 343, 2465292388.0: 344, 2474478805.0: 345, 2477715168.0: 346, 2478672703.0: 347, 2489837374.0: 348, 2519168579.0: 349, 2525085528.0: 350, 2529622387.0: 351, 2533191715.0: 352, 2542583321.0: 353, 2553642129.0: 354, 2563578405.0: 355, 2582292864.0: 356, 2603428403.0: 357, 2607263939.0: 358, 2620129841.0: 359, 2641419906.0: 360, 2644987116.0: 361, 2648364130.0: 362, 2672245846.0: 363, 2675528111.0: 364, 2680632303.0: 365, 2682451763.0: 366, 2695024471.0: 367, 2701670701.0: 368, 2716558316.0: 369, 2718975354.0: 370, 2725910909.0: 371, 2729676603.0: 372, 2731720237.0: 373, 2741053856.0: 374, 2745049530.0: 375, 2753113533.0: 376, 2757489708.0: 377, 2762006528.0: 378, 2762768202.0: 379, 2777014306.0: 380, 2783161289.0: 381, 2803217824.0: 382, 2803957365.0: 383, 2806408839.0: 384, 2807805450.0: 385, 2835112928.0: 386, 2836381797.0: 387, 2839528597.0: 388, 2841888979.0: 389, 2842874590.0: 390, 2844077120.0: 391, 2846539468.0: 392, 2855021960.0: 393, 2863453296.0: 394, 2866508787.0: 395, 2873981684.0: 396, 2875090221.0: 397, 2881343719.0: 398, 2881906563.0: 399, 2883250546.0: 400, 2884897541.0: 401, 2889409088.0: 402, 2892480739.0: 403, 2897638812.0: 404, 2900683440.0: 405, 2908439648.0: 406, 2908689726.0: 407, 2915438693.0: 408, 2940039229.0: 409, 2947485018.0: 410, 2950788258.0: 411, 2960225539.0: 412, 2963023756.0: 413, 2997499426.0: 414, 2998741153.0: 415, 3005137768.0: 416, 3013184002.0: 417, 3017099046.0: 418, 3020158098.0: 419, 3026459893.0: 420, 3029194631.0: 421, 3031634380.0: 422, 3037040993.0: 423, 3038502437.0: 424, 3040586516.0: 425, 3043322698.0: 426, 3053844648.0: 427, 3057442944.0: 428, 3058979951.0: 429, 3062939327.0: 430, 3078859377.0: 431, 3101058021.0: 432, 3111989762.0: 433, 3120173937.0: 434, 3140351797.0: 435, 3147506589.0: 436, 3162504229.0: 437, 3164453934.0: 438, 3166494274.0: 439, 3167567621.0: 440, 3175411053.0: 441, 3176331138.0: 442, 3196154180.0: 443, 3209449983.0: 444, 3211826497.0: 445, 3218526205.0: 446, 3224540893.0: 447, 3226031101.0: 448, 3229074124.0: 449, 3244353412.0: 450, 3245213785.0: 451, 3257388465.0: 452, 3262811287.0: 453, 3269691037.0: 454, 3287931638.0: 455, 3309790153.0: 456, 3311425906.0: 457, 3334203795.0: 458, 3335404793.0: 459, 3335950420.0: 460, 3347935379.0: 461, 3351180676.0: 462, 3357684963.0: 463, 3361504655.0: 464, 3363217858.0: 465, 3365384252.0: 466, 3377248650.0: 467, 3380695998.0: 468, 3381188508.0: 469, 3385882911.0: 470, 3394882861.0: 471, 3405528217.0: 472, 3407315945.0: 473, 3412631000.0: 474, 3414060082.0: 475, 3415622881.0: 476, 3416046103.0: 477, 3427782960.0: 478, 3432906440.0: 479, 3435472775.0: 480, 3450723742.0: 481, 3459398929.0: 482, 3491092368.0: 483, 3491642946.0: 484, 3492953923.0: 485, 3510837146.0: 486, 3519149792.0: 487, 3520875026.0: 488, 3521422318.0: 489, 3521486723.0: 490, 3547285199.0: 491, 3554016166.0: 492, 3554050154.0: 493, 3555631364.0: 494, 3561319611.0: 495, 3563667860.0: 496, 3564225998.0: 497, 3569114888.0: 498, 3579133125.0: 499, 3581537998.0: 500, 3588596234.0: 501, 3595526730.0: 502, 3601268508.0: 503, 3603428727.0: 504, 3604312831.0: 505, 3608356714.0: 506, 3628335731.0: 507, 3633666797.0: 508, 3635462962.0: 509, 3638234106.0: 510, 3638703029.0: 511, 3665783735.0: 512, 3668841019.0: 513, 3685528929.0: 514, 3685560512.0: 515, 3686965720.0: 516, 3687115300.0: 517, 3693160279.0: 518, 3708084825.0: 519, 3717750416.0: 520, 3718344660.0: 521, 3720725273.0: 522, 3727350739.0: 523, 3758964797.0: 524, 3759385364.0: 525, 3761919599.0: 526, 3764982725.0: 527, 3780548023.0: 528, 3794591681.0: 529, 3802198267.0: 530, 3805638891.0: 531, 3812754218.0: 532, 3822113387.0: 533, 3832385514.0: 534, 3839200423.0: 535, 3839871301.0: 536, 3843939254.0: 537, 3847205484.0: 538, 3855136047.0: 539, 3856169278.0: 540, 3888055729.0: 541, 3900582555.0: 542, 3902711704.0: 543, 3919999850.0: 544, 3920141117.0: 545, 3924806026.0: 546, 3931013035.0: 547, 3934184943.0: 548, 3947734338.0: 549, 3951563729.0: 550, 3953059257.0: 551, 3959555168.0: 552, 3965958334.0: 553, 3966939864.0: 554, 3969852427.0: 555, 3973995420.0: 556, 3975264880.0: 557, 3978315650.0: 558, 3984012914.0: 559, 4008024429.0: 560, 4013473256.0: 561, 4022384510.0: 562, 4030862332.0: 563, 4053335186.0: 564, 4059852052.0: 565, 4061873233.0: 566, 4074388366.0: 567, 4092987136.0: 568, 4107124938.0: 569, 4113042200.0: 570, 4118108704.0: 571, 4140059619.0: 572, 4164461342.0: 573, 4170404721.0: 574, 4170844960.0: 575, 4197165354.0: 576, 4214773709.0: 577, 4224469401.0: 578, 4231452819.0: 579, 4233468770.0: 580, 4238468782.0: 581, 4245112254.0: 582, 4245357819.0: 583, 4254656478.0: 584, 4259076990.0: 585, 4271990786.0: 586, 4289871436.0: 587, 4293322137.0: 588, 4294370758.0: 589, 4294374089.0: 590}, {5935544.0: 0, 11227332.0: 1, 24717878.0: 2, 30363807.0: 3, 32253252.0: 4, 35629038.0: 5, 35647572.0: 6, 50414311.0: 7, 59551881.0: 8, 68321384.0: 9, 75653729.0: 10, 76645343.0: 11, 81697952.0: 12, 85206091.0: 13, 90368390.0: 14, 100078020.0: 15, 105552570.0: 16, 107713227.0: 17, 108144222.0: 18, 108542143.0: 19, 125949389.0: 20, 127212872.0: 21, 132626724.0: 22, 133799700.0: 23, 140189192.0: 24, 142014363.0: 25, 154797796.0: 26, 158031523.0: 27, 162070053.0: 28, 165685398.0: 29, 167524087.0: 30, 169573852.0: 31, 191482116.0: 32, 192616932.0: 33, 202446461.0: 34, 216182952.0: 35, 225598385.0: 36, 232807348.0: 37, 240911020.0: 38, 254274468.0: 39, 254782071.0: 40, 270085719.0: 41, 271204416.0: 42, 271736835.0: 43, 289845245.0: 44, 291105741.0: 45, 296080432.0: 46, 304053836.0: 47, 311000964.0: 48, 313717997.0: 49, 322504694.0: 50, 325440428.0: 51, 330023644.0: 52, 331206447.0: 53, 338822397.0: 54, 342271944.0: 55, 345612117.0: 56, 346044713.0: 57, 348055217.0: 58, 352420705.0: 59, 356644232.0: 60, 362458127.0: 61, 365943673.0: 62, 366637542.0: 63, 375128744.0: 64, 388052831.0: 65, 392110193.0: 66, 394528910.0: 67, 406692999.0: 68, 410710728.0: 69, 411182284.0: 70, 418415147.0: 71, 425788830.0: 72, 430001307.0: 73, 436304571.0: 74, 470686495.0: 75, 472068312.0: 76, 477594216.0: 77, 488341466.0: 78, 504880347.0: 79, 506542898.0: 80, 517762609.0: 81, 519518409.0: 82, 521803327.0: 83, 522704250.0: 84, 535346028.0: 85, 535765427.0: 86, 536092472.0: 87, 536674820.0: 88, 539436446.0: 89, 552020267.0: 90, 559305445.0: 91, 561258345.0: 92, 568456975.0: 93, 590139317.0: 94, 606502165.0: 95, 608386947.0: 96, 623236609.0: 97, 626162299.0: 98, 628628906.0: 99, 635364437.0: 100, 644967792.0: 101, 645057463.0: 102, 646384524.0: 103, 661665254.0: 104, 662565609.0: 105, 663977854.0: 106, 676323873.0: 107, 679569945.0: 108, 681605229.0: 109, 695737974.0: 110, 700377003.0: 111, 706360718.0: 112, 706560657.0: 113, 710581151.0: 114, 722316407.0: 115, 725684897.0: 116, 740676888.0: 117, 751486511.0: 118, 757284836.0: 119, 759466383.0: 120, 766621230.0: 121, 768660604.0: 122, 772584165.0: 123, 790324703.0: 124, 828737380.0: 125, 832044092.0: 126, 849683798.0: 127, 850279148.0: 128, 853768779.0: 129, 868889197.0: 130, 874495749.0: 131, 904626810.0: 132, 910331216.0: 133, 911549041.0: 134, 928624857.0: 135, 931434303.0: 136, 939984893.0: 137, 958344897.0: 138, 958519060.0: 139, 982063666.0: 140, 987397539.0: 141, 991315822.0: 142, 999479024.0: 143, 1008626261.0: 144, 1020473584.0: 145, 1020789874.0: 146, 1033667761.0: 147, 1048150744.0: 148, 1052551190.0: 149, 1054474363.0: 150, 1062238441.0: 151, 1073972575.0: 152, 1077596568.0: 153, 1078563465.0: 154, 1083916512.0: 155, 1092708304.0: 156, 1093539263.0: 157, 1094069535.0: 158, 1095595418.0: 159, 1106103515.0: 160, 1117957061.0: 161, 1119774105.0: 162, 1124693811.0: 163, 1124841013.0: 164, 1137868768.0: 165, 1138802904.0: 166, 1139354425.0: 167, 1143802364.0: 168, 1144142784.0: 169, 1144300239.0: 170, 1153946612.0: 171, 1154624516.0: 172, 1167137835.0: 173, 1168234180.0: 174, 1171166916.0: 175, 1173654985.0: 176, 1187026868.0: 177, 1193838571.0: 178, 1202239560.0: 179, 1202493973.0: 180, 1211745152.0: 181, 1215324515.0: 182, 1222974376.0: 183, 1225199166.0: 184, 1227860617.0: 185, 1243518866.0: 186, 1252330010.0: 187, 1256689227.0: 188, 1259258411.0: 189, 1268796158.0: 190, 1280006900.0: 191, 1292076854.0: 192, 1293291084.0: 193, 1306670182.0: 194, 1322154647.0: 195, 1340847312.0: 196, 1344692959.0: 197, 1348234038.0: 198, 1366261274.0: 199, 1368006942.0: 200, 1370737413.0: 201, 1393689422.0: 202, 1401540238.0: 203, 1415645730.0: 204, 1432899319.0: 205, 1436472822.0: 206, 1443299819.0: 207, 1446925902.0: 208, 1477728018.0: 209, 1494107385.0: 210, 1500222302.0: 211, 1513547425.0: 212, 1514893648.0: 213, 1518882448.0: 214, 1528344031.0: 215, 1528459177.0: 216, 1549876909.0: 217, 1567712078.0: 218, 1574569766.0: 219, 1587926045.0: 220, 1595339376.0: 221, 1595433548.0: 222, 1597227853.0: 223, 1603187621.0: 224, 1604483000.0: 225, 1611043175.0: 226, 1617742165.0: 227, 1618890039.0: 228, 1630692959.0: 229, 1633181287.0: 230, 1637514041.0: 231, 1641742854.0: 232, 1658820416.0: 233, 1664719784.0: 234, 1666216459.0: 235, 1674050683.0: 236, 1683008787.0: 237, 1693292379.0: 238, 1705140724.0: 239, 1734637814.0: 240, 1736865015.0: 241, 1741876929.0: 242, 1745456409.0: 243, 1762672562.0: 244, 1765608435.0: 245, 1792503183.0: 246, 1792593400.0: 247, 1802080227.0: 248, 1802869339.0: 249, 1803571663.0: 250, 1817747098.0: 251, 1830771236.0: 252, 1843851026.0: 253, 1846526725.0: 254, 1846713770.0: 255, 1852999814.0: 256, 1854748846.0: 257, 1862489046.0: 258, 1863700811.0: 259, 1865987126.0: 260, 1875858461.0: 261, 1883633427.0: 262, 1884625420.0: 263, 1896282472.0: 264, 1903019131.0: 265, 1904567564.0: 266, 1905052767.0: 267, 1905600946.0: 268, 1906129897.0: 269, 1909763884.0: 270, 1910983972.0: 271, 1921497904.0: 272, 1925073569.0: 273, 1929395468.0: 274, 1933348704.0: 275, 1967875195.0: 276, 1978756174.0: 277, 1985067575.0: 278, 2001244377.0: 279, 2010422210.0: 280, 2020609234.0: 281, 2020766302.0: 282, 2040298303.0: 283, 2044129449.0: 284, 2050125136.0: 285, 2060670914.0: 286, 2062781538.0: 287, 2067029907.0: 288, 2073829678.0: 289, 2076118822.0: 290, 2079560083.0: 291, 2081752463.0: 292, 2085046711.0: 293, 2086997589.0: 294, 2092228883.0: 295, 2096224555.0: 296, 2100895344.0: 297, 2107390994.0: 298, 2117328965.0: 299, 2123593044.0: 300, 2126163239.0: 301, 2127801873.0: 302, 2131285204.0: 303, 2145918363.0: 304, 2152123252.0: 305, 2162782132.0: 306, 2163068024.0: 307, 2167748051.0: 308, 2174227543.0: 309, 2190879897.0: 310, 2202852739.0: 311, 2206822601.0: 312, 2209460067.0: 313, 2210613667.0: 314, 2210625296.0: 315, 2217696589.0: 316, 2219784538.0: 317, 2223315272.0: 318, 2236643910.0: 319, 2241528645.0: 320, 2249002006.0: 321, 2257657226.0: 322, 2260277990.0: 323, 2268943563.0: 324, 2271493970.0: 325, 2275033391.0: 326, 2286734752.0: 327, 2292749627.0: 328, 2294416816.0: 329, 2300673426.0: 330, 2307516988.0: 331, 2322640773.0: 332, 2324209546.0: 333, 2325259617.0: 334, 2329111701.0: 335, 2334824004.0: 336, 2341462332.0: 337, 2342794878.0: 338, 2343999519.0: 339, 2348263492.0: 340, 2352610431.0: 341, 2353211832.0: 342, 2357651498.0: 343, 2358242935.0: 344, 2370805180.0: 345, 2375009603.0: 346, 2379375014.0: 347, 2380257799.0: 348, 2386264398.0: 349, 2395573700.0: 350, 2399347439.0: 351, 2399984547.0: 352, 2411838333.0: 353, 2413716602.0: 354, 2421040709.0: 355, 2425188990.0: 356, 2455128522.0: 357, 2463248072.0: 358, 2469883961.0: 359, 2480950685.0: 360, 2497924953.0: 361, 2500061078.0: 362, 2502476713.0: 363, 2503097341.0: 364, 2503989552.0: 365, 2511348111.0: 366, 2520882091.0: 367, 2557087800.0: 368, 2560822555.0: 369, 2567629270.0: 370, 2568298793.0: 371, 2569319971.0: 372, 2570450596.0: 373, 2573229091.0: 374, 2576779620.0: 375, 2577849156.0: 376, 2586395932.0: 377, 2587370686.0: 378, 2589560970.0: 379, 2591245052.0: 380, 2592386414.0: 381, 2596535022.0: 382, 2597944096.0: 383, 2598710601.0: 384, 2611697862.0: 385, 2623000353.0: 386, 2623582853.0: 387, 2634174122.0: 388, 2634324469.0: 389, 2646009479.0: 390, 2646575704.0: 391, 2659366138.0: 392, 2659816966.0: 393, 2663961223.0: 394, 2691014198.0: 395, 2696857899.0: 396, 2714876601.0: 397, 2719479740.0: 398, 2733533308.0: 399, 2758723685.0: 400, 2763986279.0: 401, 2780629437.0: 402, 2783119259.0: 403, 2786838318.0: 404, 2789776792.0: 405, 2791989570.0: 406, 2800864299.0: 407, 2817258363.0: 408, 2820224094.0: 409, 2829696401.0: 410, 2837940381.0: 411, 2851266886.0: 412, 2853545767.0: 413, 2865943895.0: 414, 2869430986.0: 415, 2872218844.0: 416, 2873770515.0: 417, 2881909618.0: 418, 2884272883.0: 419, 2884994404.0: 420, 2892910358.0: 421, 2921340296.0: 422, 2922517274.0: 423, 2930519604.0: 424, 2960628360.0: 425, 2973355293.0: 426, 2973462744.0: 427, 2985062889.0: 428, 2985166265.0: 429, 2990234082.0: 430, 2991288991.0: 431, 2994533500.0: 432, 3000305165.0: 433, 3024181013.0: 434, 3024686810.0: 435, 3026928728.0: 436, 3027854004.0: 437, 3028027594.0: 438, 3036851464.0: 439, 3050872641.0: 440, 3052193176.0: 441, 3052684929.0: 442, 3053703431.0: 443, 3073749240.0: 444, 3083200953.0: 445, 3087306051.0: 446, 3093502081.0: 447, 3109245727.0: 448, 3111715411.0: 449, 3118574236.0: 450, 3118765536.0: 451, 3148611505.0: 452, 3162481453.0: 453, 3167372468.0: 454, 3168868659.0: 455, 3187874746.0: 456, 3188693048.0: 457, 3196427639.0: 458, 3208888358.0: 459, 3219054960.0: 460, 3225920916.0: 461, 3264185967.0: 462, 3265952097.0: 463, 3268255847.0: 464, 3270396580.0: 465, 3273944807.0: 466, 3284741359.0: 467, 3289504538.0: 468, 3322347995.0: 469, 3330491819.0: 470, 3333810842.0: 471, 3335852989.0: 472, 3338988046.0: 473, 3345049196.0: 474, 3351429979.0: 475, 3355821080.0: 476, 3363384707.0: 477, 3363807322.0: 478, 3366300788.0: 479, 3379756695.0: 480, 3385192041.0: 481, 3387230485.0: 482, 3392914745.0: 483, 3397168439.0: 484, 3400708739.0: 485, 3403495996.0: 486, 3406517564.0: 487, 3415205771.0: 488, 3443347112.0: 489, 3446783117.0: 490, 3449218061.0: 491, 3459445349.0: 492, 3463824248.0: 493, 3475280640.0: 494, 3484901761.0: 495, 3491367588.0: 496, 3492568569.0: 497, 3510303319.0: 498, 3513831999.0: 499, 3514843193.0: 500, 3523406771.0: 501, 3525024655.0: 502, 3536312393.0: 503, 3539657606.0: 504, 3543140058.0: 505, 3552368918.0: 506, 3552985367.0: 507, 3579002906.0: 508, 3596057443.0: 509, 3627315365.0: 510, 3636077568.0: 511, 3644911956.0: 512, 3645753479.0: 513, 3649479189.0: 514, 3654089501.0: 515, 3669846435.0: 516, 3674371366.0: 517, 3683679854.0: 518, 3684298416.0: 519, 3686713673.0: 520, 3703419368.0: 521, 3714766391.0: 522, 3718919853.0: 523, 3722198682.0: 524, 3732470382.0: 525, 3733629608.0: 526, 3743409361.0: 527, 3748463643.0: 528, 3752413139.0: 529, 3766681764.0: 530, 3766824323.0: 531, 3773386662.0: 532, 3774498209.0: 533, 3789897884.0: 534, 3795732038.0: 535, 3819795624.0: 536, 3821136146.0: 537, 3823408887.0: 538, 3823504591.0: 539, 3826420702.0: 540, 3839942041.0: 541, 3843222225.0: 542, 3857093366.0: 543, 3875127695.0: 544, 3882427978.0: 545, 3896124451.0: 546, 3899484081.0: 547, 3904994475.0: 548, 3905418529.0: 549, 3909675064.0: 550, 3920330579.0: 551, 3922234927.0: 552, 3922898027.0: 553, 3933380670.0: 554, 3946367015.0: 555, 3989805517.0: 556, 3998188166.0: 557, 4001085577.0: 558, 4004630991.0: 559, 4007466618.0: 560, 4017785174.0: 561, 4020804406.0: 562, 4024826816.0: 563, 4028893538.0: 564, 4036503217.0: 565, 4047323676.0: 566, 4054755667.0: 567, 4057820409.0: 568, 4080768097.0: 569, 4096004203.0: 570, 4125585141.0: 571, 4130641899.0: 572, 4131421003.0: 573, 4151109725.0: 574, 4158530053.0: 575, 4177206474.0: 576, 4181057233.0: 577, 4216048374.0: 578, 4220840046.0: 579, 4221698021.0: 580, 4224905902.0: 581, 4226203414.0: 582, 4226792809.0: 583, 4231268245.0: 584, 4235487485.0: 585, 4250474775.0: 586, 4251828317.0: 587, 4252627391.0: 588, 4254616934.0: 589, 4257089378.0: 590, 4259460063.0: 591, 4267125409.0: 592, 4273658069.0: 593, 4273802804.0: 594, 4283226131.0: 595, 4284551422.0: 596, 4285735142.0: 597, 4293768777.0: 598}, {61032.0: 0, 106329.0: 1, 108303.0: 2, 109373.0: 3, 117185.0: 4, 117192.0: 5, 117551.0: 6, 120048.0: 7, 120120.0: 8, 120857.0: 9, 121880.0: 10, 123429.0: 11, 123480.0: 12, 125320.0: 13, 126000.0: 14, 126960.0: 15, 127333.0: 16, 127560.0: 17, 128525.0: 18, 131674.0: 19, 131747.0: 20, 133885.0: 21, 133933.0: 22, 135010.0: 23, 136032.0: 24, 136800.0: 25, 138053.0: 26, 139200.0: 27, 140773.0: 28, 140957.0: 29, 141037.0: 30, 142320.0: 31, 142707.0: 32, 142840.0: 33, 143427.0: 34, 143778.0: 35, 144073.0: 36, 144080.0: 37, 144227.0: 38, 145603.0: 39, 145893.0: 40, 146067.0: 41, 146360.0: 42, 147094.0: 43, 147600.0: 44, 147840.0: 45, 148376.0: 46, 148759.0: 47, 149000.0: 48, 149307.0: 49, 150043.0: 50, 150587.0: 51, 150773.0: 52, 151054.0: 53, 151181.0: 54, 151427.0: 55, 151837.0: 56, 151989.0: 57, 152000.0: 58, 152351.0: 59, 152373.0: 60, 152440.0: 61, 152813.0: 62, 153333.0: 63, 153707.0: 64, 154024.0: 65, 155576.0: 66, 156168.0: 67, 156356.0: 68, 156667.0: 69, 157432.0: 70, 157480.0: 71, 157657.0: 72, 158206.0: 73, 158372.0: 74, 158710.0: 75, 159307.0: 76, 159385.0: 77, 159428.0: 78, 160063.0: 79, 160560.0: 80, 160713.0: 81, 160907.0: 82, 161377.0: 83, 161835.0: 84, 162053.0: 85, 162187.0: 86, 162581.0: 87, 163467.0: 88, 163500.0: 89, 163613.0: 90, 163960.0: 91, 164888.0: 92, 164894.0: 93, 164955.0: 94, 165093.0: 95, 165123.0: 96, 165373.0: 97, 165381.0: 98, 165427.0: 99, 165853.0: 100, 165978.0: 101, 166640.0: 102, 166928.0: 103, 167273.0: 104, 167347.0: 105, 167442.0: 106, 168000.0: 107, 168013.0: 108, 168733.0: 109, 168791.0: 110, 168840.0: 111, 169017.0: 112, 169027.0: 113, 169255.0: 114, 169482.0: 115, 169557.0: 116, 169613.0: 117, 169924.0: 118, 169999.0: 119, 170000.0: 120, 170067.0: 121, 170747.0: 122, 171067.0: 123, 171120.0: 124, 171333.0: 125, 171533.0: 126, 171600.0: 127, 172196.0: 128, 172467.0: 129, 172663.0: 130, 172867.0: 131, 172933.0: 132, 172987.0: 133, 173040.0: 134, 173427.0: 135, 173867.0: 136, 174000.0: 137, 174067.0: 138, 174093.0: 139, 174100.0: 140, 174333.0: 141, 174480.0: 142, 175000.0: 143, 175200.0: 144, 175280.0: 145, 175711.0: 146, 175721.0: 147, 175755.0: 148, 176187.0: 149, 176453.0: 150, 176959.0: 151, 177466.0: 152, 177806.0: 153, 177975.0: 154, 178267.0: 155, 178720.0: 156, 178823.0: 157, 179449.0: 158, 179893.0: 159, 180000.0: 160, 180160.0: 161, 180267.0: 162, 180522.0: 163, 180598.0: 164, 180891.0: 165, 180898.0: 166, 181145.0: 167, 181333.0: 168, 181360.0: 169, 181481.0: 170, 181493.0: 171, 181653.0: 172, 182520.0: 173, 183001.0: 174, 183493.0: 175, 183667.0: 176, 183682.0: 177, 183871.0: 178, 184000.0: 179, 184011.0: 180, 184867.0: 181, 184893.0: 182, 184918.0: 183, 185000.0: 184, 185023.0: 185, 185133.0: 186, 185183.0: 187, 185240.0: 188, 185417.0: 189, 185440.0: 190, 185916.0: 191, 186000.0: 192, 186267.0: 193, 186517.0: 194, 186613.0: 195, 186945.0: 196, 187500.0: 197, 187867.0: 198, 188000.0: 199, 188388.0: 200, 188400.0: 201, 188893.0: 202, 188948.0: 203, 188987.0: 204, 189323.0: 205, 189333.0: 206, 189487.0: 207, 189667.0: 208, 190173.0: 209, 190345.0: 210, 190476.0: 211, 190547.0: 212, 190560.0: 213, 190680.0: 214, 191160.0: 215, 191227.0: 216, 191406.0: 217, 191550.0: 218, 191687.0: 219, 191747.0: 220, 191904.0: 221, 192000.0: 222, 192070.0: 223, 192191.0: 224, 192471.0: 225, 192566.0: 226, 192587.0: 227, 192751.0: 228, 192779.0: 229, 192941.0: 230, 192973.0: 231, 192987.0: 232, 194000.0: 233, 194760.0: 234, 194771.0: 235, 194893.0: 236, 195000.0: 237, 195027.0: 238, 195187.0: 239, 195587.0: 240, 195636.0: 241, 195637.0: 242, 195681.0: 243, 195789.0: 244, 196030.0: 245, 196093.0: 246, 197216.0: 247, 197640.0: 248, 197667.0: 249, 197674.0: 250, 198000.0: 251, 198053.0: 252, 198333.0: 253, 198408.0: 254, 198774.0: 255, 199053.0: 256, 199307.0: 257, 199893.0: 258, 200000.0: 259, 200010.0: 260, 200088.0: 261, 200164.0: 262, 200480.0: 263, 200625.0: 264, 200760.0: 265, 200967.0: 266, 200987.0: 267, 201498.0: 268, 201553.0: 269, 201600.0: 270, 201693.0: 271, 201800.0: 272, 201960.0: 273, 202080.0: 274, 202093.0: 275, 202144.0: 276, 202520.0: 277, 202573.0: 278, 203027.0: 279, 203080.0: 280, 203227.0: 281, 203267.0: 282, 203507.0: 283, 203827.0: 284, 204213.0: 285, 204533.0: 286, 204733.0: 287, 205040.0: 288, 206013.0: 289, 206100.0: 290, 206320.0: 291, 206427.0: 292, 206524.0: 293, 206773.0: 294, 206945.0: 295, 207634.0: 296, 208011.0: 297, 208222.0: 298, 208518.0: 299, 208933.0: 300, 209162.0: 301, 209921.0: 302, 210000.0: 303, 210120.0: 304, 210432.0: 305, 211027.0: 306, 211547.0: 307, 211666.0: 308, 211733.0: 309, 212040.0: 310, 212047.0: 311, 212107.0: 312, 212200.0: 313, 212336.0: 314, 212493.0: 315, 213160.0: 316, 213308.0: 317, 213394.0: 318, 213787.0: 319, 213853.0: 320, 214587.0: 321, 214707.0: 322, 214733.0: 323, 214825.0: 324, 214854.0: 325, 214947.0: 326, 214960.0: 327, 216000.0: 328, 216347.0: 329, 216441.0: 330, 216800.0: 331, 217867.0: 332, 218284.0: 333, 218573.0: 334, 218707.0: 335, 219087.0: 336, 219193.0: 337, 219680.0: 338, 220027.0: 339, 220200.0: 340, 220730.0: 341, 220800.0: 342, 221133.0: 343, 221398.0: 344, 222400.0: 345, 222587.0: 346, 222640.0: 347, 222787.0: 348, 223053.0: 349, 223112.0: 350, 223543.0: 351, 223653.0: 352, 224000.0: 353, 224061.0: 354, 224145.0: 355, 224907.0: 356, 225246.0: 357, 225280.0: 358, 225373.0: 359, 226000.0: 360, 226397.0: 361, 227027.0: 362, 228434.0: 363, 228713.0: 364, 228856.0: 365, 228880.0: 366, 229360.0: 367, 229714.0: 368, 230427.0: 369, 230920.0: 370, 231120.0: 371, 232680.0: 372, 233143.0: 373, 233267.0: 374, 233293.0: 375, 233507.0: 376, 233560.0: 377, 234560.0: 378, 235603.0: 379, 235720.0: 380, 235840.0: 381, 236260.0: 382, 236520.0: 383, 236748.0: 384, 237000.0: 385, 237417.0: 386, 237838.0: 387, 238018.0: 388, 238387.0: 389, 238530.0: 390, 238627.0: 391, 238880.0: 392, 239520.0: 393, 239836.0: 394, 239853.0: 395, 240413.0: 396, 240508.0: 397, 240527.0: 398, 240800.0: 399, 241640.0: 400, 241800.0: 401, 241933.0: 402, 241987.0: 403, 242012.0: 404, 242181.0: 405, 243189.0: 406, 244160.0: 407, 244252.0: 408, 244798.0: 409, 245040.0: 410, 245250.0: 411, 245707.0: 412, 245929.0: 413, 246760.0: 414, 246800.0: 415, 246827.0: 416, 246960.0: 417, 247059.0: 418, 247760.0: 419, 248133.0: 420, 248156.0: 421, 248293.0: 422, 248347.0: 423, 248471.0: 424, 248587.0: 425, 249094.0: 426, 249227.0: 427, 249293.0: 428, 250400.0: 429, 250840.0: 430, 250867.0: 431, 250907.0: 432, 250987.0: 433, 251120.0: 434, 251507.0: 435, 252189.0: 436, 252307.0: 437, 252520.0: 438, 253333.0: 439, 253390.0: 440, 253427.0: 441, 253887.0: 442, 254413.0: 443, 255238.0: 444, 256000.0: 445, 256004.0: 446, 256347.0: 447, 257400.0: 448, 258475.0: 449, 258547.0: 450, 258626.0: 451, 259556.0: 452, 259907.0: 453, 259947.0: 454, 261160.0: 455, 261933.0: 456, 262827.0: 457, 263787.0: 458, 264504.0: 459, 265213.0: 460, 265507.0: 461, 266015.0: 462, 266213.0: 463, 267000.0: 464, 269116.0: 465, 269200.0: 466, 270000.0: 467, 270715.0: 468, 270746.0: 469, 270760.0: 470, 270773.0: 471, 271000.0: 472, 271560.0: 473, 271652.0: 474, 272507.0: 475, 272627.0: 476, 273627.0: 477, 275173.0: 478, 276133.0: 479, 278000.0: 480, 278600.0: 481, 278680.0: 482, 279227.0: 483, 279692.0: 484, 281147.0: 485, 281613.0: 486, 282880.0: 487, 282920.0: 488, 283453.0: 489, 284000.0: 490, 286220.0: 491, 286987.0: 492, 288280.0: 493, 289133.0: 494, 290933.0: 495, 292227.0: 496, 292373.0: 497, 292707.0: 498, 293080.0: 499, 295533.0: 500, 295890.0: 501, 297640.0: 502, 300827.0: 503, 301133.0: 504, 301400.0: 505, 301720.0: 506, 302467.0: 507, 303320.0: 508, 303521.0: 509, 303587.0: 510, 304219.0: 511, 308120.0: 512, 311000.0: 513, 311227.0: 514, 312533.0: 515, 313400.0: 516, 313573.0: 517, 313840.0: 518, 314880.0: 519, 317831.0: 520, 320704.0: 521, 321093.0: 522, 323253.0: 523, 325440.0: 524, 326707.0: 525, 328067.0: 526, 329653.0: 527, 333893.0: 528, 336053.0: 529, 338255.0: 530, 343099.0: 531, 345400.0: 532, 347720.0: 533, 348100.0: 534, 349613.0: 535, 349756.0: 536, 354320.0: 537, 354520.0: 538, 357707.0: 539, 358000.0: 540, 359333.0: 541, 365000.0: 542, 368000.0: 543, 369600.0: 544, 370867.0: 545, 373627.0: 546, 374400.0: 547, 375947.0: 548, 376250.0: 549, 379373.0: 550, 382507.0: 551, 399373.0: 552, 399453.0: 553, 403333.0: 554, 404093.0: 555, 404480.0: 556, 408467.0: 557, 412333.0: 558, 424960.0: 559, 431800.0: 560, 433067.0: 561, 433107.0: 562, 434720.0: 563, 438204.0: 564, 442000.0: 565, 442693.0: 566, 446240.0: 567, 449453.0: 568, 456173.0: 569, 461400.0: 570, 461946.0: 571, 464667.0: 572, 466160.0: 573, 471160.0: 574, 473080.0: 575, 475667.0: 576, 480333.0: 577, 481125.0: 578, 484960.0: 579, 486773.0: 580, 490227.0: 581, 503067.0: 582, 516533.0: 583, 517125.0: 584, 520787.0: 585, 563000.0: 586, 602176.0: 587, 616560.0: 588, 783587.0: 589, 1133027.0: 590, 1895040.0: 591}]
list_of_cols_to_normalize = [1, 2, 3, 4, 15]

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values())) + 1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize, mappings):
            if i >= data_arr.shape[1]:
                break
            col = data_arr[:, i]
            normcol = column_norm(col,mapping)
            data_arr[:, i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([610.4402618657938, 208.87561374795416, 268.04418985270047, 292.90998363338787, 299.8772504091653, 0.5861644844517185, 0.680011438625204, 5.364975450081833, -7.760833060556469, 0.5728314238952537, 0.1084335515548281, 0.09498151446808509, 0.1805144026186579, 0.4573209492635024, 123.59588543371503, 295.5597381342062, 3.909983633387889])
        components = np.array([array([-9.98097833e-01,  7.70058294e-03,  2.02311655e-02,  1.74638632e-02,
       -5.45514157e-02, -8.22995133e-06, -2.05478092e-05, -2.59273006e-04,
       -5.61595840e-04, -1.08621916e-04, -2.90625973e-06,  3.02423979e-05,
        8.53500454e-06, -5.98141885e-06, -2.39655948e-03,  6.71810033e-03,
        9.25456145e-06]), array([ 3.45468789e-02, -3.79726130e-04,  4.94031842e-01,  7.60121606e-01,
       -2.47784160e-01,  1.67565841e-04,  1.50063772e-04,  6.80289560e-04,
        2.90609308e-03, -8.84609516e-05,  7.15578780e-05, -2.97790515e-05,
       -1.05577582e-05,  4.84456250e-05,  7.28974172e-03, -3.39845336e-01,
        3.06745454e-04]), array([ 3.33690018e-02,  2.56502803e-02, -2.04132616e-01, -3.19110395e-01,
       -8.33936333e-01,  2.23410560e-04,  1.09602794e-04,  7.55868258e-04,
        3.28633085e-03, -1.22791151e-05,  3.14966393e-05, -1.64690494e-04,
        1.66571546e-06,  1.47168229e-04, -6.05736891e-04, -3.99080585e-01,
        1.83156903e-04]), array([-3.78235736e-02, -3.20770937e-02, -1.51599506e-01, -1.20867747e-01,
        4.87549112e-01,  3.99426350e-04,  1.86198889e-04, -1.11580499e-04,
        6.86325031e-03, -7.47049872e-05,  1.76022400e-04, -2.40313088e-04,
       -6.63979218e-06,  3.35943019e-04,  7.28485267e-03, -8.49790633e-01,
        4.14311931e-04]), array([-5.94575152e-03, -1.73128567e-01, -8.18677006e-01,  5.43518985e-01,
       -3.86772183e-02, -8.74545606e-05, -3.52548887e-05,  5.74295064e-04,
       -2.35650525e-03, -1.32301201e-04, -3.77554773e-05,  5.79648655e-05,
        6.41686904e-05, -3.89272289e-05, -9.03294885e-05,  5.33324406e-02,
       -1.26535570e-04]), array([-4.69095666e-03, -9.83989616e-01,  1.43607416e-01, -1.00193034e-01,
       -3.11729441e-02,  1.02146200e-05,  1.36411280e-04,  3.44571837e-04,
       -9.16991044e-05,  1.61574050e-04, -5.76837515e-05,  2.78853844e-05,
        1.87470411e-05,  1.43887313e-04,  6.49905924e-03,  8.15342585e-03,
        1.72828500e-04]), array([-2.34220086e-03,  6.63274686e-03, -3.64441157e-03, -4.07238130e-03,
       -2.17143094e-03, -6.13504665e-04,  2.15842062e-03,  1.05230325e-02,
        4.33784738e-02,  5.71786407e-04,  2.85526529e-04, -1.22144943e-03,
        4.17095556e-04,  6.16203078e-04,  9.98918283e-01,  8.74979682e-03,
       -1.31528665e-03]), array([ 4.25561824e-04,  6.37400517e-04,  1.47346922e-03, -1.12690525e-03,
       -9.01499235e-05, -1.04026665e-02, -3.55835102e-02, -3.46243171e-03,
       -9.97880273e-01,  5.01426309e-03,  2.52299234e-04,  2.52901497e-02,
       -2.65634635e-03, -9.33079232e-03,  4.35269548e-02, -7.91241265e-03,
       -1.13320025e-02]), array([ 2.78862625e-04, -3.46777958e-04, -2.73546781e-04,  5.25246438e-04,
       -8.94315036e-04, -2.34988481e-03, -2.31696484e-04, -9.99757793e-01,
        3.94312871e-03,  1.64445053e-02, -1.11883563e-03, -2.89591739e-04,
        2.61514075e-03, -1.78568497e-03,  1.03661379e-02, -2.85665592e-04,
        8.51822101e-03]), array([ 1.12364392e-04, -1.47470147e-04,  1.05410085e-04, -1.40694605e-04,
       -1.53952652e-05,  4.62144960e-02, -1.91736789e-02, -1.68456426e-02,
       -3.87894735e-03, -9.97134992e-01,  1.00026294e-02,  3.02254067e-03,
        5.73017500e-03, -2.69781208e-02,  9.42591803e-04,  4.55217839e-05,
       -4.52634331e-02])])
        whiten = False
        explained_variance = np.array([126929.2577807146, 34576.30173099864, 29716.84297055816, 28440.765352438964, 19942.58844291642, 13775.96857019189, 900.2552580967749, 25.638350861624875, 12.52740996844969, 0.2391750248871615])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target=""
important_idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]

def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[], trim=False):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target=""
    important_idxs=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return -1
    if not trim:
        ignorecolumns = []
    if (testfile):
        target = ''
        hc = -1 
    with open(outputcsvfile, "w+", encoding='utf-8') as outputfile:
        with open(inputcsvfile, "r", encoding='utf-8') as csvfile:      # hardcoded utf-8 encoding per #717
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile, encoding='utf-8')):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        if len(important_idxs) == 1:
                            print(",", file=outputfile)
                        else:
                            print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        if len(important_idxs) == 1:
                            print(",", file=outputfile)
                        else:
                            print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False, trim=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result=float(value)
                if math.isnan(result):
                    #if nan parse to string
                    raise ValueError('')
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename, encoding='utf-8') as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+", encoding='utf-8')
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            if not transform_true:
                rowlen = num_attr if trim else num_attr + len(ignorecolumns)
            else:
                rowlen = num_attr_before_transform if trim else num_attr_before_transform + len(ignorecolumns)      # noqa
            if (not testfile):
                rowlen = rowlen + 1    
            if ((len(row) - (1 if ((testfile and len(important_idxs) == 1)) else 0))  != rowlen) and not (row == ['','']):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs. Expected Row length: " + str(rowlen) + ", Actual Row Length: " + str(len(row)))
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier

def single_classify(row, return_probabilities=False):
    #inits
    x = row
    o = [0] * num_output_logits


    #Nueron Equations
    h_0 = max((((-89.89453 * float(x[0]))+ (-29.04317 * float(x[1]))+ (-7.678798 * float(x[2]))+ (-63.41959 * float(x[3]))+ (-35.050655 * float(x[4]))+ (-2.029976 * float(x[5]))+ (8.046565 * float(x[6]))+ (-1.8384135 * float(x[7]))+ (0.235594 * float(x[8]))+ (2.5691123 * float(x[9]))) + -1.0805529), 0)
    h_1 = max((((16.951128 * float(x[0]))+ (-2.937515 * float(x[1]))+ (-52.737892 * float(x[2]))+ (5.2741504 * float(x[3]))+ (23.306042 * float(x[4]))+ (-13.137918 * float(x[5]))+ (4.2203636 * float(x[6]))+ (6.5161963 * float(x[7]))+ (-1.8815374 * float(x[8]))+ (0.02729739 * float(x[9]))) + -6.615162), 0)
    h_2 = max((((42.41759 * float(x[0]))+ (-17.898228 * float(x[1]))+ (18.452108 * float(x[2]))+ (-22.40325 * float(x[3]))+ (6.5735636 * float(x[4]))+ (14.892492 * float(x[5]))+ (0.22269298 * float(x[6]))+ (-0.5853333 * float(x[7]))+ (1.4742696 * float(x[8]))+ (1.247426 * float(x[9]))) + 0.08622091), 0)
    h_3 = max((((-3.8795497 * float(x[0]))+ (-1.3403095 * float(x[1]))+ (-0.450956 * float(x[2]))+ (-2.6206493 * float(x[3]))+ (-1.6340916 * float(x[4]))+ (-0.1468084 * float(x[5]))+ (0.10927291 * float(x[6]))+ (-0.39182967 * float(x[7]))+ (-0.7640243 * float(x[8]))+ (-0.2782471 * float(x[9]))) + -0.048939142), 0)
    h_4 = max((((-4.1320796 * float(x[0]))+ (-1.2618626 * float(x[1]))+ (-0.25210986 * float(x[2]))+ (-3.0090394 * float(x[3]))+ (-1.5102628 * float(x[4]))+ (-0.042703588 * float(x[5]))+ (0.5654734 * float(x[6]))+ (0.08506071 * float(x[7]))+ (0.67204595 * float(x[8]))+ (0.55715233 * float(x[9]))) + -0.55866534), 0)
    h_5 = max((((1.145844 * float(x[0]))+ (-0.48070854 * float(x[1]))+ (0.50188404 * float(x[2]))+ (-0.60216194 * float(x[3]))+ (0.17465173 * float(x[4]))+ (0.40089098 * float(x[5]))+ (0.013891139 * float(x[6]))+ (-0.1595371 * float(x[7]))+ (0.010740448 * float(x[8]))+ (0.6704547 * float(x[9]))) + -0.3672924), 0)
    o[0] = (0.5938375 * h_0)+ (-6.438414e-05 * h_1)+ (0.1081704 * h_2)+ (-6.0438876 * h_3)+ (-7.2460833 * h_4)+ (-4.002878 * h_5) + -0.12097075



    #Output Decision Rule
    if num_output_logits==1:
        if return_probabilities:
            exp_o = 1./(1. + np.exp(-o[0]))
            return np.array([1.-exp_o, exp_o])
        else:
            return o[0]>=0
    else:
        if return_probabilities:
            exps = np.exp(o)
            Z = sum(exps).reshape(-1, 1)
            return exps/Z
        else:
            return argmax(o)


def classify(arr, transform_true=False, return_probabilities=False):
    #apply transformation if necessary
    if transform_true:
        arr = transform(arr)
    #init
    w_h = np.array([[-89.89453125, -29.043170928955078, -7.678798198699951, -63.41958999633789, -35.050655364990234, -2.0299758911132812, 8.046565055847168, -1.8384134769439697, 0.2355940043926239, 2.5691123008728027], [16.951128005981445, -2.9375150203704834, -52.737892150878906, 5.274150371551514, 23.306041717529297, -13.137918472290039, 4.220363616943359, 6.516196250915527, -1.8815374374389648, 0.02729739062488079], [42.4175910949707, -17.89822769165039, 18.45210838317871, -22.403249740600586, 6.573563575744629, 14.892492294311523, 0.22269298136234283, -0.5853332877159119, 1.4742696285247803, 1.2474260330200195], [-3.879549741744995, -1.340309500694275, -0.45095598697662354, -2.6206493377685547, -1.6340916156768799, -0.14680840075016022, 0.10927291214466095, -0.3918296694755554, -0.7640243172645569, -0.27824708819389343], [-4.132079601287842, -1.2618626356124878, -0.2521098554134369, -3.0090394020080566, -1.5102628469467163, -0.04270358756184578, 0.5654733777046204, 0.08506070822477341, 0.6720459461212158, 0.5571523308753967], [1.1458439826965332, -0.4807085394859314, 0.5018840432167053, -0.6021619439125061, 0.17465172708034515, 0.400890976190567, 0.013891139067709446, -0.15953710675239563, 0.010740447789430618, 0.6704546809196472]])
    b_h = np.array([-1.0805529356002808, -6.615161895751953, 0.08622091263532639, -0.048939142376184464, -0.5586653351783752, -0.3672924041748047])
    w_o = np.array([[0.5938374996185303, -6.438414129661396e-05, 0.10817039757966995, -6.043887615203857, -7.2460832595825195, -4.002878189086914]])
    b_o = np.array(-0.12097074836492538)

    #Hidden Layer
    h = np.dot(arr, w_h.T) + b_h
    
    relu = np.maximum(h, np.zeros_like(h))


    #Output
    out = np.dot(relu, w_o.T) + b_o
    if num_output_logits == 1:
        if return_probabilities:
            exp_o = 1./(1. + np.exp(-out))
            return np.concatenate((1.-exp_o, exp_o), axis=1)
        else:
            return (out >= 0).astype('int').reshape(-1)
    else:
        if return_probabilities:
            exps = np.exp(out)
            Z = np.sum(exps, axis=1).reshape(-1, 1)
            return exps/Z
        else:
            return (np.argmax(out, axis=1)).reshape(-1)



def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r', encoding='utf-8') as csvinput:
        #readers and writers
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(reader, None) + ["Prediction"]))
        
        outputs = classify(arr)
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(outputs[i]), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            print(','.join(['"' + field + '"' if ',' in field else field for field in row]))


def Validate(cleanarr):
    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    parser.add_argument('-trim', action="store_true", help="If true, the prediction will not output ignored columns.")
    args = parser.parse_args()
    faulthandler.enable()

    if args.validate:
        args.trim = True


    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        output = preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate), trim=args.trim)
        get_key, classmapping = clean(preprocessedfile if output!=-1 else args.csvfile, cleanfile, -1, args.headerless, (not args.validate), trim=args.trim)
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}
        output = None


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')
    if not args.trim and ignorecolumns != []:
        cleanarr = cleanarr[:, important_idxs]


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile if output!=-1 else args.csvfile, get_key, classmapping)


    #Validate
    else:
        classifier_type = 'NN'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
        #Correct Labels
        true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap = 1
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            classbalance = [float(num_class_0)/count, float(num_class_1)/count]
            H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))

            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            json_dict = {'instance_count':                        count ,
                         'classifier_type':                        classifier_type,
                         'classes':                            2 ,
                         'false_negative_instances':    num_FN ,
                         'false_positive_instances':    num_FP ,
                         'true_positive_instances':    num_TP ,
                         'true_negative_instances':    num_TN,
                         'false_negatives':                        FN ,
                         'false_positives':                        FP ,
                         'true_negatives':                        TN ,
                         'true_positives':                        TP ,
                         'number_correct':                        num_correct ,
                         'accuracy': {
                             'best_guess': randguess,
                             'improvement': modelacc-randguess,
                             'model_accuracy': modelacc,
                         },
                         'model_capacity':                        model_cap ,
                         'generalization_ratio':                int(float(num_correct * 100) / model_cap) * H/ 100.0,
                         'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0,
                        'shannon_entropy_of_labels':           H,
                        'classbalance':                        classbalance}
            if args.json:
                pass
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                elif classifier_type == 'RF':
                    print("Classifier Type:                    Random Forest")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0 * H) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            classbalance = [float(numofcertainclass) / count for numofcertainclass in numeachclass.values()]
            H = float(-1.0 * sum([classbalance[i] * math.log(classbalance[i]) / math.log(2) for i in range(len(classbalance))]))

            if args.json:
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'classes':                            n_classes,
                             'number_correct': num_correct,
                             'accuracy': {
                                 'best_guess': randguess,
                                 'improvement': modelacc - randguess,
                                 'model_accuracy': modelacc,
                             },
                             'model_capacity': model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0 * H,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0,
                        'shannon_entropy_of_labels':           H,
                        'classbalance':                        classbalance}
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                elif classifier_type == 'RF':
                    print("Classifier Type:                    Random Forest")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0 * H) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, json, labels=None, sample_weight=None, normalize=None):
            stats = {}
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")
            n_labels = labels.size

            for class_i in range(n_labels):
                stats[class_i] = {'TP':{},'FP':{},'FN':{},'TN':{}}
                class_i_indices = np.argwhere(y_true==class_i)
                not_class_i_indices = np.argwhere(y_true!=class_i)
                stats[int(class_i)]['TP'] = int(np.sum(y_pred[class_i_indices]==y_true[class_i_indices]))
                stats[int(class_i)]['FP'] = int(np.sum(y_pred[class_i_indices]!=y_true[class_i_indices]))
                stats[int(class_i)]['TN'] = int(np.sum(y_pred[not_class_i_indices]==y_true[not_class_i_indices]))
                stats[int(class_i)]['FN'] = int(np.sum(y_pred[not_class_i_indices]!=y_true[not_class_i_indices]))
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                if not json:
                    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                    sys.exit()
                else:
                    return np.array([]), stats
                

            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------



            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]

            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm, stats
        mtrx, stats = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1), args.json)
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            json_dict['multiclass_stats'] = stats
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])

    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        if output!=-1:
            os.remove(preprocessedfile)
